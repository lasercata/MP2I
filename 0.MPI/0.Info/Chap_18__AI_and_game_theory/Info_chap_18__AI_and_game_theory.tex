\documentclass[a4paper, 12pt, twoside]{article}


%------------------------------------------------------------------------
%
% Author                :   Lasercata
% Last modification     :   2023.02.09
%
%------------------------------------------------------------------------


%------ini
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
%\usepackage[english]{babel}


%------geometry
\usepackage[textheight=700pt, textwidth=500pt]{geometry}


%------color
\usepackage{xcolor}
\definecolor{ff4500}{HTML}{ff4500}
\definecolor{00f}{HTML}{0000ff}
\definecolor{0ff}{HTML}{00ffff}
\definecolor{656565}{HTML}{656565}

%\renewcommand{\emph}{\textcolor{ff4500}}
%\renewcommand{\em}{\color{ff4500}}

\newcommand{\Emph}{\textcolor{ff4500}}

\newcommand{\strong}[1]{\textcolor{ff4500}{\bf #1}}
\newcommand{\st}{\color{ff4500}\bf}


%------Code highlighting
%---listings
\usepackage{listings}

\definecolor{cbg}{HTML}{272822}
\definecolor{cfg}{HTML}{ececec}
\definecolor{ccomment}{HTML}{686c58}
\definecolor{ckw}{HTML}{f92672}
\definecolor{cstring}{HTML}{e6db72}
\definecolor{cstringlight}{HTML}{98980f}
\definecolor{lightwhite}{HTML}{fafafa}

\lstdefinestyle{DarkCodeStyle}{
    backgroundcolor=\color{cbg},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstring},
    basicstyle=\ttfamily\footnotesize\color{cfg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    xleftmargin=\leftskip
}

\lstdefinestyle{LightCodeStyle}{
    backgroundcolor=\color{lightwhite},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstringlight},
    basicstyle=\ttfamily\footnotesize\color{cbg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=L,
    xleftmargin=\leftskip
}

%\lstset{style=DarkCodeStyle}
\lstset{style=LightCodeStyle}
%Usage : \begin{lstlisting}[language=Caml, xleftmargin=xpt] ... \end{lstlisting}


%---Algorithm
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\SetKwProg{Fn}{Function}{:}{}
\SetKw{KwPrint}{Print}

\newcommand\commfont[1]{\textit{\texttt{\textcolor{656565}{#1}}}}
\SetCommentSty{commfont}
\SetProgSty{texttt}
\SetArgSty{textnormal}
\SetFuncArgSty{textnormal}
%\SetProgArgSty{texttt}

\newenvironment{indalgo}[2][H]{
    \begin{algoBox}
        \begin{algorithm}[#1]
            \caption{#2}
}
{
        \end{algorithm}
    \end{algoBox}
}


%---tcolorbox
\usepackage[many]{tcolorbox}
\DeclareTColorBox{emphBox}{O{black}O{lightwhite}}{
    breakable,
    outer arc=0pt,
    arc=0pt,
    top=0pt,
    toprule=-.5pt,
    right=0pt,
    rightrule=-.5pt,
    bottom=0pt,
    bottomrule=-.5pt,
    colframe=#1,
    colback=#2,
    enlarge left by=10pt,
    width=\linewidth-\leftskip-10pt,
}

\DeclareTColorBox{algoBox}{O{black}O{lightwhite}}{
    breakable,
    arc=0pt,
    top=0pt,
    toprule=-.5pt,
    right=0pt,
    rightrule=-.5pt,
    bottom=0pt,
    bottomrule=-.5pt,
    left=0pt,
    leftrule=-.5pt,
    colframe=#1,
    colback=#2,
    width=\linewidth-\leftskip-10pt,
}


%-------make the table of content clickable
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}


%------pictures
\usepackage{graphicx}
%\usepackage{wrapfig}

\usepackage{tikz}
%\usetikzlibrary{babel}             %Uncomment this to use circuitikz
%\usetikzlibrary{shapes.geometric}  % To draw triangles in trees
%\usepackage{circuitikz}            %Electrical circuits drawing
\usetikzlibrary{patterns}


%------tabular
%\usepackage{color}
%\usepackage{colortbl}
%\usepackage{multirow}


%------Physics
%---Packages
%\usepackage[version=4]{mhchem} %$\ce{NO4^2-}$

%---Commands
\newcommand{\link}[2]{\mathrm{#1} \! - \! \mathrm{#2}}
\newcommand{\pt}[1]{\cdot 10^{#1}} % Power of ten
\newcommand{\dt}[2][t]{\dfrac{\mathrm d #2}{\mathrm d #1}} % Derivative


%------math
%---Packages
%\usepackage{textcomp}
%\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools} % For abs
\usepackage{stmaryrd} %for \llbracket and \rrbracket
\usepackage{mathrsfs} %for \mathscr{x} (different from \mathcal{x})

%---Commands
%-Sets
\newcommand{\N}{\mathbb{N}} %set N
\newcommand{\Z}{\mathbb{Z}} %set Z
\newcommand{\Q}{\mathbb{Q}} %set Q
\newcommand{\R}{\mathbb{R}} %set R
\newcommand{\C}{\mathbb{C}} %set C
\newcommand{\U}{\mathbb{U}} %set U
\newcommand{\seg}[2]{\left[ #1\ ;\ #2 \right]}
\newcommand{\nset}[2]{\left\llbracket #1\ ;\ #2 \right\rrbracket}

%-Exponantial / complexs
\newcommand{\e}{\mathrm{e}}
\newcommand{\cj}[1]{\overline{#1}} %overline for the conjugate.

%-Vectors
\newcommand{\vect}{\overrightarrow}
\newcommand{\veco}[3]{\displaystyle \vect{#1}\binom{#2}{#3}} %vector + coord

%-Limits
\newcommand{\lm}[2][{}]{\lim\limits_{\substack{#2 \\ #1}}} %$\lm{x \to a} f$ or $\lm[x < a]{x \to a} f$
\newcommand{\Lm}[3][{}]{\lm[#1]{#2} \left( #3 \right)} %$\Lm{x \to a}{f}$ or $\Lm[x < a]{x \to a}{f}$
\newcommand{\tendsto}[1]{\xrightarrow[#1]{}}

%-Integral
\newcommand{\dint}[4][x]{\displaystyle \int_{#2}^{#3} #4 \mathrm{d} #1} %$\dint{a}{b}{f(x)}$ or $\dint[t]{a}{b}{f(t)}$

%-left right
\newcommand{\lr}[1]{\left( #1 \right)}
\newcommand{\lrb}[1]{\left[ #1 \right]}
\newcommand{\lrbb}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\lrangle}[1]{\left\langle #1 \right\rangle}

%-Others
\newcommand{\para}{\ /\!/\ } %//
\newcommand{\ssi}{\ \Leftrightarrow \ }
\newcommand{\eqsys}[2]{\begin{cases} #1 \\ #2 \end{cases}}

\newcommand{\med}[2]{\mathrm{med} \left[ #1\ ;\ #2 \right]}  %$\med{A}{B} -> med[A ; B]$
\newcommand{\Circ}[2]{\mathscr{C}_{#1, #2}}

\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}

\newcommand{\oboxed}[1]{\textcolor{ff4500}{\boxed{\textcolor{black}{#1}}}} %orange boxed

\newcommand{\rboxed}[1]{\begin{array}{|c} \hline #1 \\ \hline \end{array}} %boxed with right opened
\newcommand{\lboxed}[1]{\begin{array}{c|} \hline #1 \\ \hline \end{array}} %boxed with left opened

\newcommand{\orboxed}[1]{\textcolor{ff4500}{\rboxed{\textcolor{black}{#1}}}} %orange right boxed
\newcommand{\olboxed}[1]{\textcolor{ff4500}{\lboxed{\textcolor{black}{#1}}}} %orange left boxed


%------commands
%---to quote
\newcommand{\simplecit}[1]{\guillemotleft$\;$#1$\;$\guillemotright}
\newcommand{\cit}[1]{\simplecit{\textcolor{656565}{#1}}}
\newcommand{\quo}[1]{\cit{\it #1}}

%---to indent
\newcommand{\ind}[1][20pt]{\advance\leftskip + #1}
\newcommand{\deind}[1][20pt]{\advance\leftskip - #1}

%---to indent a text
\newcommand{\indented}[2][20pt]{\par \ind[#1] #2 \par \deind[#1]}
\newenvironment{indt}[2][20pt]{#2 \par \ind[#1]}{\par \deind} %Titled indented env

%---title
\newcommand{\thetitle}[2]{\begin{center}\textbf{{\LARGE \underline{\Emph{#1} :}} {\Large #2}}\end{center}}

%---Maths environments
%-Proofs
\newenvironment{proof}[1][{}]{\begin{indt}{$\square$ #1}}{$\blacksquare$ \end{indt}}

%-Maths parts (proposition, definition, ...)
\newenvironment{mathpart}[1]{\begin{indt}{\boxed{\text{\textbf{#1}}}}}{\end{indt}}
\newenvironment{mathbox}[1]{\boxed{\text{\textbf{#1}}}\begin{emphBox}}{\end{emphBox}}
\newenvironment{mathul}[1]{\begin{indt}{\underline{\textbf{#1}}}}{\end{indt}}

\newenvironment{theo}{\begin{mathpart}{Théorème}}{\end{mathpart}}
\newenvironment{Theo}{\begin{mathbox}{Théorème}}{\end{mathbox}}

\newenvironment{prop}{\begin{mathpart}{Proposition}}{\end{mathpart}}
\newenvironment{Prop}{\begin{mathbox}{Proposition}}{\end{mathbox}}
\newenvironment{props}{\begin{mathpart}{Propriétés}}{\end{mathpart}}

\newenvironment{defi}{\begin{mathpart}{Définition}}{\end{mathpart}}
\newenvironment{meth}{\begin{mathpart}{Méthode}}{\end{mathpart}}

\newenvironment{Rq}{\begin{mathul}{Remarque :}}{\end{mathul}}
\newenvironment{Rqs}{\begin{mathul}{Remarques :}}{\end{mathul}}

\newenvironment{Ex}{\begin{mathul}{Exemple :}}{\end{mathul}}
\newenvironment{Exs}{\begin{mathul}{Exemples :}}{\end{mathul}}


%------Sections
% To change section numbering :
% \renewcommand\thesection{\Roman{section}}
% \renewcommand\thesubsection{\arabic{subsection})}
% \renewcommand\thesubsubsection{\textit \alph{subsubsection})}

% To start numbering from 0
% \setcounter{section}{-1}


%------page style
\usepackage{fancyhdr}
\usepackage{lastpage}

\setlength{\headheight}{18pt}
\setlength{\footskip}{50pt}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE, RO]{\textit{\textcolor{black}{\today}}}
\fancyhead[RE, LO]{\large{\textsl{\Emph{\texttt{\jobname}}}}}

\fancyfoot[RO, LE]{\textit{\texttt{\textcolor{black}{Page \thepage /}\pageref{LastPage}}}}
\fancyfoot[LO, RE]{\includegraphics[scale=0.12]{/home/lasercata/Pictures/1.images_profil/logo/mieux/lasercata_logo_fly_fond_blanc.png}}


%------init lengths
\setlength{\parindent}{0pt} %To avoid using \noindent everywhere.
\setlength{\parskip}{3pt}


%---------------------------------Begin Document
\begin{document}
    
    \thetitle{Chapitre 18}{Intelligence artificielle et théorie des jeux}
    
    \tableofcontents
    \newpage
    
    \begin{indt}{\section{Introduction}}
        L'expression \emph{intelligence artificielle} est une manière floue de désigner des algorithmes chargés comme tous les autres de résoudre des problèmes.
        Certains d'entre eux doivent jouer à des jeux, d'autres doivent répartir des données en plusieurs catégories (on parle de \emph{classification}) ou déterminer des valeurs numériques associées à des paramètres d'entrée (on parle de \emph{problème de régression}).
        Quelques traits communs à ces algorithmes sont l'usage d'heuristiques afin d'essayer d'obtenir des réponses les meilleures possibles en temps raisonnable et l'exploitation d'une grande quantité de données afin d'en construire une représentation (on parle de l'\emph{apprentissage d'un modèle de données}) qui sera exploité pour construire la réponse de l'algorithme.
        Dans ce chapitre, on se limite à l'étude des problèmes de classification et de la théorie des jeux.
    \end{indt}

    \vspace{12pt}
    
    \begin{indt}{\section{Apprentissage supervisé}}
        \begin{indt}{\subsection{Algorithme des $k$ plus proches voisins}}
            \begin{indt}{\subsubsection{Introduction}}
                \label{2.1.1}

                Pour résoudre un problème de classification, la méthode de l'apprentissage supervisé consiste à exploiter des données dont on connait déjà la classe afin de construire un algorithme de classification prenant les caractéristiques d'une donnée en entrée et renvoyant la classe à laquelle cette donnée appartient probablement.

                Les données manipulées sont en général représentées par des points de $\R^d$ ou $d$ est souvent grand.
                Par exemple, si on veut reconnaître des caractères manuscrits, l'algorithme peut prendre en entrée une image de $28 \times 28$ pixels en 256 niveaux de gris contenant un scan du caractère manuscrit à reconnaître, donc la donnée est représentée par un point de $\nset{0}{255}^d$, où $d = 28^2 = 784$.

                Si on veut répartir dans $C$ classes des données de $\R^d$, le problème consiste à construire un algorithme réalisant une fonction $\R^d \longrightarrow \nset 0 {C - 1}$ en exploitant une heuristique s'appuyant sur un ensemble de couples $(x, y) \in \R^d \times \nset 0 {C - 1}$, où $y$ est la classe de $x$, appelées données d'entrainement.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Algorithme des $k$ plus proches voisins}}
                Idée : il est possible que des données proches appartiennent à la même classe, donc on pourrait pour un point donné, renvoyer la classe du point déjà étiqueté le plus proche.

                Problème : la donnée d'entrainement la plus proche peut appartenir à une autre classe que celle du point, par example si les données d'entrainement sont mal réparties, ou bruitées, ou si le point considéré est proche de la frontière entre deux classes.

                \begin{center}
                    \begin{tikzpicture}[scale=2]
                        \draw (0, 0) rectangle (2, -1);
                        \draw[dashed] (1, .2) -- (1, -1.2);

                        \node (C0) at (.5, .2) {$C_0$};
                        \node (C1) at (1.5, .2) {$C_1$};

                        \node at (.3, -.3) [color=ff4500] {$\times$};
                        \node at (.6, -.8) [color=ff4500] {$\times$};

                        \node at (.8, -.5) [color=blue] {$\times$};

                        \node at (1.2, -.6) [color=ff4500] {$\times$};
                        \node at (1.6, -.3) [color=ff4500] {$\times$};
                        \node at (1.6, -.8) [color=ff4500] {$\times$};

                        \matrix [draw, below left, rounded corners=5pt] at (5, -.1) {
                            \node [color=ff4500, label=right:entraînement] {$\times$}; \\
                            \node [color=blue, label=right:point à classer] {$\times$}; \\
                        };
                    \end{tikzpicture}
                \end{center}

                \vspace{6pt}
                
                Pour éviter cet écueil, on considère plutôt la classe majoritaire parmi les classes des $k$ données d'entrainement les plus proches du point d'entrée, pour un $k$ fixé.

                On parle alors de l'algorithme des $k$--plus proches voisins ($k$NN pour $k$--\textit{nearest neighbors}).

                \begin{indt}{Variables d'ajustement :}
                    $-$ En cas d'égalité, il faut choisir une classe, par exemple au hasard parmi les classes majoritaires.

                    $-$ Le nombre $k$ de voisins : si $k$ est trop faible, l'algorithme sera trop sensible au bruit sur les données et si $k$ est trop grand, l'algorithme renverra surtout la classe majoritaire parmi les données d'entraînement, donc effectue une mauvaise généralisation.

                    $-$ La notion de distance : on utilise souvent la distance de \textsc{Minkowski}
                    \[
                        d(x, x') = \lr{\sum_{i = 1}^d \abs{x_i - x_i'}^p}^{\tfrac 1 p}
                    \]

                    qui donne la distance de \textsc{Manhattan} pour $p = 1$, et la distance euclidienne pour $p = 2$.
                    Le programme se limite à la distance euclidienne.
                \end{indt}

                Pour déterminer les $k$ plus proches voisins d'un point donné, on peut exploiter une file de priorité :

                \begin{indalgo}{Algorithme des $k$ plus proches voisins}
                    \KwInput{données d'entraînement $\lr{x_i, y_i}_{i \in \nset 1 N}$}
                    \KwInput{point à classer $x$}

                    \BlankLine

                    $F \gets$ file de priorité max vide\;

                    \For{$i$ de 1 à $k$}{
                        Insérer $i$ dans $F$ avec la priorité $d(x, x_i)$\;
                    }

                    \For{$i$ de $k + 1$ à $N$}{
                        \If{$d(x, x_i) < d(x, x_{\max F})$}{
                            Extraire le max de $F$\;
                            Insérer $i$ dans $F$ avec la priorité $d(x, x_i)$\;
                        }
                    }

                    $C \gets \set{C_i\ |\ i \in F}$\;

                    \Return un élément le plus fréquent de $C$\;
                \end{indalgo}

                Complexité : $\mathcal O(N\log k)$ en temps, et $\mathcal O(k)$ en espace.

                \vspace{12pt}
                
                Remarque : si on a beaucoup de point à classer, cet algorithme est peu efficace car il nécessite de parcourir l'intégralité des données pour chaque point à classer. On pourrait plutôt effectuer un prétraitement des données pour rendre plus efficace le calcul des $k$ plus proches voisins d'un point donné.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Arbres $k$--dimensionnels}}
                Attention : on ne parle pas du $k$ de $k$NN, mais plutôt de la dimension de l'espace des données ($d$ en \ref{2.1.1}, page \pageref{2.1.1}), mais c'est la lettre $k$ qui est utilisée dans la littérature.

                \vspace{12pt}
                
                $\bullet$ Définition : la structure d'arbre $k$--dimensionnel, ou arbre $k$-d, est une généralisation des la notion d'arbre binaire de recherche : un arbre binaire étiqueté par des éléments de $\R^k$ est un arbre $k$-d si et seulement si pour tout n\oe ud d'étiquette $x = (x_0, \ldots, x_{k - 1})$ de profondeur $i$,
                \[
                    \begin{array}{ll}
                        \forall x' = (x_0', \ldots, x_{k - 1}')\ \text{étiquette du sous-arbre gauche}, & x_j' \le x_j
                        \\
                        \forall x' = (x_0', \ldots, x_{k - 1}')\ \text{étiquette du sous-arbre droit}, & x_j' > x_j
                    \end{array}
                \]

                où $j = i \mod k$.

                \vspace{12pt}
                
                $\bullet$ Exemple en dimension 2 :

                \begin{center}
                    \begin{tikzpicture}[scale=1]
                        \draw[->] (0, 0) -- (9, 0);
                        \draw[->] (0, 0) -- (0, 9);

                        \node (x) at (9.3, -.5) {$x$};
                        \node (x) at (-.5, 9.3) {$y$};

                        \node (1) at (1, 2) [label=right:{$1$}] {$\times$};
                        \node (2) at (1, 5) [label=right:{$2$}] {$\times$};
                        \node (3) at (3, 3) [label=below:{$3$}] {$\times$};
                        \node (4) at (4, 7) [label=above left:{$4$}] {$\times$};
                        \node (5) at (5, 5) [label=left:{$5$}] {$\times$};
                        \node (6) at (6, 3) [label=above:{$6$}] {$\times$};
                        \node (7) at (7, 1.5) [label=left:{$7$}] {$\times$};
                        \node (8) at (8, 6.5) [label=left:{$8$}] {$\times$};

                        \draw[dashed] (0, 3) -- (8.5, 3);
                        \draw[dashed] (1, 0) -- (1, 8.5);
                        \draw[dashed] (5, 0) -- (5, 8.5);
                        \draw[dashed] (1, 7) -- (5, 7);
                        \draw[dashed] (7, 0) -- (7, 3);
                        \draw[dashed] (8, 3) -- (8, 8.5);
                %    \end{tikzpicture}
                %\end{center}

                %\begin{center}
                %    \begin{tikzpicture}
                        \node (5) at (12, 7) [circle, draw] {$5$}
                            child [xshift=-20pt] {node [circle, draw] {$3$}
                                child {node [circle, draw] {$1$}}
                                child {node [circle, draw] {$2$}
                                    child [xshift=20pt] {node [circle, draw] {$4$}}
                                }
                            }
                            child [xshift=20pt] {node [circle, draw] {$6$}
                                child {node [circle, draw] {$7$}}
                                child {node [circle, draw] {$8$}}
                            }
                        ;
                    \end{tikzpicture}
                \end{center}

                \vspace{12pt}
                
                $\bullet$ Remarque : dans cet exemple, on a fait en sorte de construire un arbre $k$-d équilibré en choisissant l'élément médian pour la coordonnée associée à la profondeur du n\oe ud comme étiquette.

                On écrit l'algorithme \texttt{créer\_arbre} suivant :

                \begin{indalgo}{\texttt{créer\_arbre}}
                    \SetKwFunction{creerarbre}{créer\_arbre}

                    \Fn{\creerarbre{$k$, $i$, $l$}}{
                        \KwInput{dimension $k$}
                        \KwInput{profondeur $i$}
                        \KwInput{liste de données $l$}

                        \BlankLine

                        \If{$l =$ \texttt{[]}}{
                            \Return l'arbre vide\;
                        }
                        \Else{
                            Extraire l'élément $x$ de $l$ médian pour la coordonnée $i \mod k$\;

                            $l_<,\ l_> \gets$ partition de $l$ suivant le pivot $x$\;

                            \Return \texttt{Noeud}($x$, \creerarbre{$k$, $i + 1$, $l_<$}, \creerarbre{$k$, $(i + 1)$, $l_>$})\;
                        }
                    }
                \end{indalgo}

                Complexité : à l'aide de l'algorithme de calcul de la médiane en temps linéaire (\textit{cf} chapitre 7, 2.2), la complexité est celle du tri rapide dans le meilleur cas, \textit{i.e} $\mathcal O(N\log N)$.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Recherche des $k$ plus proches voisins dans un arbre $k$-d}}
                Idée : pour trouver les $k$ plus proches voisins d'un point $x \in \R^d$ dans un arbre $k$-d $T$ de dimension $d$, on procède initialement comme pour la recherche de $x$ dans un ABR (en comparant la bonne coordonnée à chaque profondeur) puis on remonte dans l'arbre en sélectionnant les $k$ voisins, parfois en redescendant des un sous-arbre que l'on avait ignoré.

                Exemple : on suppose être au niveau d'un n\oe ud d'étiquette $x'$ de profondeur $i$ et tel que $x_i > x_i'$.

                On cherche donc récursivement les $k$ voisins dans le sous-arbre droit.

                S'il y a moins de $k$ n\oe uds dans ce sous-arbre, il faudra considérer $x'$ comme voisin et peut-être aussi les n\oe uds du sous-arbre gauche.

                Même si l'appel récursif sélectionne $k$ voisins, on peut devoir considérer $x'$ ou les n\oe uds du sous-arbre gauche si $\abs{x_i - x_i'}$ est inférieure à la distance maximale entre $x$ et les voisins sélectionnés.

                Par exemple, si $k = 3, d = 2$, et $i$ correspond aux abscisses,

                \begin{center}
                    \begin{tikzpicture}[scale=.95]
                        \draw[->] (0, 0) -- (6.3, 0);
                        \draw[->] (0, 0) -- (0, 6.3);

                        \draw[dashed] (2, 0) -- (2, 6);

                        \node (xp) at (2, 6) [label=left:{$x'$}] {$\times$};

                        \node (x) at (3.6, 3.4) [label=right:{$x$}] {$\times$};
                        \draw (x) circle (2.5);

                        \node (vp) at (1.6, 3.2) [label=above:{$v'$}] {$\times$};
                        \node (v1) at (4.2, 4.7) [label=left:{$v_1$}] {$\times$};
                        \node (v2) at (3.1, 2) [label=right:{$v_2$}] {$\times$};
                        \node (v3) at (5.1, 1.4) [label=below right:{$v_3$}] {$+$};

                        \draw[<->] (x) to node [above right] {$d_{\max}$} (v3);
                        \draw[<->] (2.1, 3.4) to node [above] {\footnotesize{$\abs{x_i - x_i'}$}} (x);
                %    \end{tikzpicture}
                %\end{center}

                %\begin{center}
                %    \begin{tikzpicture}
                        \node (0) at (9, 5) [circle, draw] {$x'$}
                            child {node [circle, draw] {$v'$}}
                            child {node [circle, draw] {$v_2$}
                                child {node [circle, draw] {$v_3$}}
                                child {node [circle, draw] {$v_1$}}
                            }
                        ;
                    \end{tikzpicture}
                \end{center}

                D'où l'algorithme :
                \begin{indalgo}{\texttt{recherche\_voisins}, \texttt{visite}}
                    \SetKwFunction{recherchevoisins}{recherche\_voisins}
                    \SetKwFunction{visite}{visite}

                    \Fn{\recherchevoisins{$x$, $T$, $k$}}{
                        $F \gets$ file de priorité max vide\;
                        \visite{$F, x, T, 0, k$}\;

                        \Return les éléments de $F$\;
                    }

                    \BlankLine

                    \Fn{\visite{$F, x, T, i, k$}}{
                        \If{$T$ = \texttt{Noeud}($x', k, r$)}{
                            %$t_1, t_2 \gets$ si $x_i \le x_i'$, alors $(l, r)$, sinon $(r, l)$\;
                            \If{$x_i \le x_i'$}{
                                $t_1, t_2 \gets l, r$\;
                            }
                            \Else{
                                $t_1, t_2 \gets r, l$\;
                            }

                            \BlankLine

                            \visite{$F, x, t_1, i + 1 \mod d, k$}\;

                            \BlankLine

                            \If{$\abs F < k$ ou priorité $\max F \ge \abs{x_i - x_j}$}{
                                \If{$d(x, x') <$ priorité $\max F$}{
                                    Extraire $\max F$\;
                                    Insérer $x'$ dans $F$ avec la priorité $d(x, x')$\;
                                }

                                \visite{$F, x, t_2, i + 1 \mod d, k$}\;
                            }
                        }
                    }
                \end{indalgo}

                Dans le pire cas, on visite les $N$ n\oe uds de l'arbre (donc on n'a rien gagné par rapport au premier algorithme) mais le plus souvent on ne visite que de l'ordre de $\log N$ n\oe uds.
            \end{indt}
        \end{indt}

        \vspace{12pt}
        
        \begin{indt}{\subsection{Arbres de décision}}
            \begin{indt}{\subsubsection{Introduction}}
                Un arbre de décision est un outil permettant d'implémenter un algorithme de classification dont le fonctionnement est le suivant :
                étant donné un point à classer, on descend récursivement dans l'arbre en effectuant à chaque n\oe ud un test dur une coordonnée dont le résultat détermine la branche à parcourir.
                La feuille atteinte donne la classe du point.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Définition (\textit{arbre de décision})}}
                Un \emph{arbre décision} est un arbre étiqueté tel que les étiquettes des n\oe uds internes sont des coordonnées et celles des feuilles sont des classes, de telle sorte que les fils d'un n\oe ud donné correspond aux différentes valeurs possibles pour la coordonnée associée au n\oe ud.

                Si la coordonnée est \emph{catégorique}, \textit{i.e} ne peut prendre qu'un nombre fini de valeur, alors il y a autant de fils que de valeurs.
                Si la coordonnée est \emph{numérique}, les fils correspondent à des intervalles de valeurs disjoints.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                On veut savoir comment passer la soirée, sachant qu'il y a trois activités possibles : aller en soirée, étudier, ou se tourner les pouces.
                \begin{indt}{On représente les étudiants par des triplets dont les coordonnées sont :}
                    $-$ un booléen indiquant s'il y a une soirée organisée par son cercle d'amis ;

                    $-$ Un réel $> 0$ indiquant le nombre de jour avant la prochaine deadline (exemple : devoir) ;

                    $-$ Un booléen indiquant si l'étudiant est procrastinateur.
                \end{indt}

                On pourrait construire l'arbre suivant :
                \begin{center}
                    \begin{tikzpicture}
                        \node (0) at (0, 0) [rectangle, draw] {soirée}
                            child [xshift=-40pt] {node [rectangle, draw] {aller en soirée} edge from parent [above left] node {\textit{oui}}}
                            child [xshift=40pt, yshift=-20pt] {node [rectangle, draw] {deadline}
                                child [xshift=-60pt] {node [rectangle, draw] {étudier} edge from parent [above left] node {$> 1$}}
                                child [yshift=-20pt] {node [rectangle, draw] {procratistinateur}
                                    child [xshift=-30pt, yshift=-15pt] {node [rectangle, draw] {se tourner les pouces} edge from parent [above left] node {\textit{oui}}}
                                    child [xshift=30pt, yshift=-15pt] {node [rectangle, draw] {étudier} edge from parent [above right] node {\textit{non}}}
                                    edge from parent [right] node {$\in ]1 ; 7[$}
                                }
                                child [xshift=80pt] {node [rectangle, draw] {se tourner les pouces} edge from parent [above right] node {$> 7$}}
                                edge from parent [above right] node {\textit{non}}
                            }
                        ;
                    \end{tikzpicture}
                \end{center}

                L'étudiant $(V, 0.2, 5, F)$ va en soirée

                L'étudiant $(F, 4, V)$ va se tourner les pouces.

                \vspace{12pt}
                
                Remarque : dans le cadre de l'apprentissage supervisé, la question est de savoir comment construire un arbre de décision à partir de données étiquetées.
                Il faut donc  choisir la structure de l'arbre  et les tests effectués à chaque n\oe ud. Il est possible de construire un arbre qui classe parfaitement toutes les données d'entraînement ou alors de s'autoriser des erreurs pour tenir compte d'un éventuel bruit sur les données d'entraînement.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Entropie de \textsc{Shannon}}}
                $\bullet$ Idée : pour construire l'arbre de décision, une idée serait de placer en racine la coordonnée qui discrimine au mieux entre les différentes classes pour le jeu de données et de construire les sous-arbres récursivement.
                On utiliser pour cela la notion d'\emph{entropie de \textsc{Shannon}}, qui est une mesure de l'information contenue dans un jeu de données.

                \vspace{12pt}
                
                $\bullet$ Définition (\textit{entropie de \textsc{Shannon}}) :
                on considère un ensemble $S$ de $N$ données réparties dans $C$ classes $C_0, \ldots, C_{C - 1}$.

                L'entropie de \textsc{Shannon} de $S$ est la quantité
                \[
                    H(S) = -\sum_{i = 0}^{C - 1} \dfrac{\abs{C_i}}{N} \log_2 \dfrac{\abs{C_i}}{N}
                \]

                \vspace{12pt}
                
                $\bullet$ Remarque : L'entropie de \textsc{Shannon} est l'espérance du nombre de bits d'informations que l'on obtient en tirant aléatoirement uniformément un point de $S$.

                En effet, $\dfrac{\abs{C_i}} N$ est la probabilité d'obtenir un élément de la classe $C_i$, et $-\log_2 \dfrac{\abs{C_i}}N = \log_2 N - \log_2 \abs{C_i}$ est la différence entre le nombre de bits nécessaires pour représenter l'intégralité des données et le nombre de bits nécessaires pour distinguer entre elles les données de la classe $C_i$.
                C'est donc le nombre de bits qui restent afin de donner de l'information sur l'appartenance à la classe $C_i$.

                \begin{indt}{En particulier,}
                    $-$ Si $C = 1$, alors $H(S) = 0$ : la représentation des données sert uniquement à les distinguer entre elles et n'apporte aucune information sur la classe.
                    
                    $-$ L'entropie est maximale lorsque les données sont uniformément réparties $\lr{\abs{C_i} = \dfrac N C}$.
                    $H(S) = -\log_2 C$, et on obtient autant de bits que nécessaire pour distinguer les $C$ classes.
                \end{indt}

                \vspace{12pt}
                
                $\bullet$ Définition (\textit{gain d'un attribut}) :
                On considère un ensemble $S$ de $N$ données en dimension $d$, et une coordonnée (ou un attribut) $i \in \nset 1 d$ dont on note $m$ le nombre de valeurs possibles.

                Le \emph{gain} de l'attribut $i$ est la quantité
                \[
                    G(S, i) = H(S) - \sum_{j = 1}^m \dfrac{\abs{S_j}}{N} H(S_j)
                \]

                où $S_1, \ldots, S_m$ est la partition de $S$ suivant les valeurs de la coordonnée $i$.

                \vspace{12pt}
                
                $\bullet$ Remarque : le gain de l'attribut correspond à l'espérance de la perte d'entropie lorsque l'on fixe la valeur de l'attribut.
                \[
                    \begin{array}{rcl}
                        G(S, i)
                        &=& \displaystyle
                        H(S) - \sum_{j = 1}^m \dfrac{\abs{S_j}}{N} H(S_j)
                        \\
                        &=& \displaystyle
                        \sum_{j = 1}^m \dfrac{\abs{S_j}}{N} H(S)
                        - \sum_{j = 1}^m \dfrac{\abs{S_j}}{N} H(S_j)
                        \\
                        &=& \displaystyle
                        \sum_{j = 1}^m \dfrac{\abs{S_j}}{N} (H(S) - H(S_j))
                    \end{array}
                \]

                où $\dfrac{\abs{S_j}} N$ est la probabilité de tirer un point sont la coordonnée $i$ prend la $j$\textsuperscript{ème} valeur, et $H(S) - H(S_j)$ correspond à la différence entre l'entropie du jeu de données initial et l'entropie des données restantes après le choix de la $j$\textsuperscript{ème} valeur.

                \vspace{6pt}
                
                Plus un attribut est discriminant pour les classes, plus l'entropie des données restantes après le choix d'un valeur pour l'attribut doit être faible. On cherche donc à maximiser le gain de l'attribut.

                \vspace{12pt}
                
                $\bullet$ Proposition (Inégalité de \textsc{Gibls}) :
                \begin{emphBox}
                    Si $\lr{p_i}_{i \in \nset 0 {k - 1}}$, et $\lr{q_i}_{i \in \nset 0 {k - 1}}$ sont deux distributions de probabilité sur un ensemble de cardinal $k$, alors
                    \[
                        -\sum_{i = 0}^{k - 1} p_i \log_2(p_i)
                        \le
                        -\sum_{i = 0}^{k - 1} p_i \log_2(q_i)
                    \]
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    $\forall x \in \R^*_+,\ \ln(x) \le x - 1$ (concavité de $\ln$).

                    Donc
                    \[
                        \begin{array}{rcl}
                            \displaystyle
                            \sum_{i = 0}^{k - 1} p_i \ln\!\lr{\dfrac{q_i}{p_i}}
                            &\le& \displaystyle
                            \sum_{i = 0}^{k - 1} p_i \lr{\dfrac{q_i}{p_i} - 1}
                            \\
                            &=& \displaystyle
                            \sum_{i = 0}^{k - 1} (q_i - p_i)
                            \\
                            &=& 1 - 1 = 0
                        \end{array}
                    \]

                    Donc
                    \[
                        \sum_{i = 0}^{k - 1} p_i \ln q_i
                        \le
                        \sum_{i = 0}^{k - 1} p_i \ln p_i
                    \]

                    ce qui conclut car $\dfrac{-1}{\ln 2} < 0$.
                \end{proof}

                \vspace{12pt}
                
                $\bullet$ Corollaire :
                \begin{emphBox}
                    Si $S$ est un ensemble de $N$ données en dimension $d$ réparties dans les clauses $C_0, \ldots, C_{C - 1}$, et si $i \in \nset 1 d$ est un attribut pouvant prendre une valeur, alors $G(S, i) \ge 0$.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    \[
                        \begin{array}{rcl}
                            G(S, i)
                            &=& \displaystyle
                            H(S) - \sum_{j = 1}^m \dfrac{\abs{S_j}}{N} H(S_j)
                            \\
                            &=& \displaystyle
                            - \sum_{k = 0}^{C - 1} \dfrac{\abs{C_k}}{N} \log_2 \dfrac{\abs{C_k}}{N}
                            - \sum_{j = 1}^m \dfrac{\abs{S_j}}{N} \lr{
                                -\sum_{k = 0}^{C - 1} \dfrac{\abs{C_k \cap S_j}}{\abs{S_j}} \log_2 \dfrac{\abs{C_k \cap S_j}}{\abs{S_j}}
                            }
                            \\
                            &=& \displaystyle
                            \sum_{k = 0}^{C - 1} \sum_{j = 1}^m \dfrac{\abs{C_k \cap S_j}}{N}\log_2 \dfrac{\abs{C_k}}{N}
                            - \lr{
                                -\sum_{j = 1}^m \sum_{k = 0}^{C - 1} \dfrac{\abs{C_k \cap S_j}}{N} \log_2 \dfrac{\abs{C_k \cap S_j}}{\abs{S_j}}
                            }
                        \end{array}
                    \]

                    Or
                    \[
                        \log_2 \dfrac{\abs{C_k \cap S_j}}{\abs{S_j}}
                        = \log_2 \dfrac{\abs{C_k \cap S_j}}{N}
                        = - \log_2 \dfrac{\abs{S_j}}{N}
                    \]

                    Donc
                    \[
                        \begin{array}{rcl}
                            G(S, i)
                            &=&\displaystyle
                            - \sum_{k = 0}^{C - 1} \sum_{j = 1}^m \dfrac{\abs{C_i \cap S_j}}{N} \lr{\log_2 \dfrac{\abs{C_k}}{N} + \log_2 \dfrac{\abs{S_j}}{N}}
                            - \lr{
                                -\sum_{j = 1}^m \sum_{k = 0}^{C - 1} \dfrac{\abs{C_k \cap S_j}}{N} \log_2 \dfrac{\abs{C_k \cap S_j}}{N}
                            }
                        \end{array}
                    \]

                    On note $\displaystyle \lr{p_{j, k}}_{(j, k) \in \nset 1 n \times \nset 0 {C - 1}} = \lr{\dfrac{\abs{C_k \cap S_j}}{N}}_{j, k}$
                    et $\displaystyle \lr{q_{j, k}}_{j, k} = \lr{\dfrac{\abs{C_k} \abs{S_j}}{N^2}}_{j, k}$
                    et on remarque que $\lr{p_{j, k}}_{j, k}$ et $\lr{q_{j, k}}_{j, k}$ sont des distributions de probabilité, donc l'inégalité de \textsc{Gibls} donne :
                    \[
                        - \sum_{j, k} p_{j, k} \log_2 p_{j, k}
                        \le
                        - \sum_{j, k} p_{j, k}\log_2 q_{j, k}
                    \]

                    d'où $G(S, i) \ge 0$.
                \end{proof}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Algorithme ID3}}
                $\bullet$ Principe : l'algorithme ID3 (pour \textit{Iterative Dichotomiser 3}) consiste à construire un arbre de décision en choisissant pour racine l'attribut de gain maximal et en construisant récursivement les sous-arbres en considérant comme jeu de données la partition des données selon la valeur de l'attribut choisi.

                \vspace{12pt}
                
                $\bullet$ Cas binaire : on énonce l'algorithme ID3 dans le cas où les attributs sont binaires.

                \begin{indalgo}{ID3, cas binaire}
                    \KwInput{L'ensemble $S$ des données}
                    \KwInput{L'ensemble $A$ des attributs que l'on s'autorise à utiliser dans les n\oe uds}

                    \BlankLine

                    \If{$S = \varnothing$}{
                        Construire une feuille contenant la classe la plus représentée parmi les données qui ont servi à construire son père\;
                    }
                    \ElseIf{tous les éléments de $S$ appartient à la même classe $C$}{
                        Construire une feuille contenant $C$\;
                    }
                    \ElseIf{$A = \varnothing$}{
                        Construire une feuille contenant la classe la plus représentée parmi les données de $S$\;
                    }
                    \Else{
                        $i \gets$ attribut de $A$ qui maximise $G(S, i)$\;

                        $S_0 \gets$ données de $S$ dont l'attribut $i$ vaut $0$\;
                        $S_1 \gets$ données de $S$ dont l'attribut $i$ vaut $1$\;

                        Construire le n\oe ud d'étiquette $i$, de sous-arbre gauche \texttt{ID3}($S_0, A \setminus \set i$), de sous-arbre droit \texttt{ID3}($S_1, A \setminus \set i$)\;
                    }
                \end{indalgo}

                \vspace{12pt}
                
                $\bullet$ Exemple :

                \begin{center}
                    \begin{tikzpicture}
                        \node (0) at (0, 0) [circle, draw] {$L$}
                            child [xshift=-20pt] {node [circle, draw] {$O$}}
                            child [xshift=20pt] {node [circle, draw] {$C$}
                                child [xshift=-20pt] {node [circle, draw] {$E$}
                                    child [xshift=-20pt] {node [circle, draw] {$F$}
                                        child {node [circle, draw] {$1$}}
                                        child {node [circle, draw] {$O$}}
                                    }
                                    child [xshift=20pt] {node [circle, draw] {$F$}
                                        child {node [circle, draw] {$O$}}
                                        child {node [circle, draw] {$1$}}
                                    }
                                }
                                child [xshift=20pt] {node [circle, draw] {$1$}}
                            }
                        ;
                        
                        \node (arr) at (-6, -3) {
                            \begin{tabular}{cccc|c}
                                \multicolumn{4}{c}{Attributs}
                                & Classe
                                \\
                                $E$ & $C$ & $F$ & $L$ & $R$
                                \\
                                \hline
                                1 & 1 & 1 & 1 & 1
                                \\
                                1 & 1 & 1 & 0 & 0
                                \\
                                1 & 1 & 0 & 1 & 1
                                \\
                                1 & 1 & 0 & 0 & 0
                                \\
                                1 & 0 & 1 & 1 & 1
                                \\
                                1 & 0 & 1 & 0 & 0
                                \\
                                1 & 0 & 0 & 1 & 0
                                \\
                                1 & 0 & 0 & 0 & 0
                                \\
                                0 & 0 & 1 & 1 & 0
                                \\
                                0 & 0 & 1 & 0 & 0
                                \\
                                0 & 0 & 0 & 1 & 1
                                \\
                                0 & 0 & 0 & 0 & 0
                            \end{tabular}
                        };
                    \end{tikzpicture}
                \end{center}
            \end{indt}
        \end{indt}

        \vspace{12pt}
        
        \begin{indt}{\subsection{Analyse des résultats}}
            \begin{indt}{\subsubsection{Introduction}}
                Les algorithmes d'apprentissage dépendent de paramètres (par exemple le nombre $k$ de voisins dans $k$NN ou une heuristique d'apprentissage d'arbres de décision telle qu'ID3) qu'il faut ajuster selon le cas d'application afin d'obtenir le meilleur classificateur possible.

                Choisir les meilleurs paramètres pour un algorithme d'apprentissage passe par l'essai de plusieurs valeurs de ces paramètres et par une évaluation du classificateur obtenu.

                On a donc besoin d'un moyen de mesurer els performances d'un classificateur.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Jeu d'entraînement, jeu de test}}
                Dans le cadre de l'apprentissage supervisé, on dispose de données étiquetées par le résultat attendu, donc il est possible d'évaluer les performances d'un classificateur en comparant le résultat qu'il renvoie et le résultat attendu sur chacune des données étiquettées.

                Cependant, il faut éviter d'utiliser les mêmes données pour l'apprentissage et pour les tests car l'algorithme est censé renvoyer de bons résultats sur les données d'entraînement. On n'aurait donc pas une bonne mesure de la capacité de l'algorithme à généraliser le modèle qu'il apprend.
                Il est alors d'usage de répartir les données en deux jeux : un jeu d'entraînement qui contient les données qui serviront à l'apprentissage, et un jeu de test qui contient les données qui serviront aux mesures de performance.

                Construire ces jeux de données est un enjeux majeur car de mauvais choix peuvent donner des résultats grossièrement faux. Par exemple, si on applique l'algorithme ID3 à un jeu d'entraînement dont toutes les données appartiennent à la même classe, l'arbre de décision obtenu est une feuille réduite à cette classe donc le classificateur est une fonction constante qui se trompera sur toutes les données d'autres classes.
                Disposer d'une grande quantité de données d'entraînement est aussi important.

                Les données sont en général réparties entre les deux jeux avec de l'ordre de 80\% de données d'entraînement et 20\% de données de test. Une manière de respecter la répartition des classes dans le jeu d'entraînement consiste à tirer les données d'entraînement avec un tirage aléatoire uniforme.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Sur-apprentissage / sous-apprentissage}}
                L'objectif de l'apprentissage est de deviner des règles implicites qui régissent la distribution des classes, à partir d'un jeu de données, et de généraliser ces règles pour construire un modèle permettant de déterminer la classe de données pour lesquelles on ne dispose pas d'information.
                Le sur-apprentissage consiste en une mauvaise généralisation liée à une trop grande précision sur le jeu d'entraînement.

                Par exemple, pour un algorithme de régression, si les données sont bruitées, il y a sur-apprentissage si la fonction produite par l'algorithme "colle" trop aux données d'entraînement et reproduit le bruit des données au lieu d'apprendre uniquement la tendance générale.

                Par exemple, il est toujours possible d'obtenir une erreur nulle sur les données d'entraînement par interpolation, mais cela ne garantit pas la qualité de la régression en dehors de ces données.

                %............................................................
                %\newline
                %\quo{Courbe quadratique avec des point approximant la courbe dont certains sont aberrants, et une courbe qui passe exactement par les point d'entraînement, mais qui ne représente pas bien la courbe.}
                %\newline
                %............................................................

                \begin{center}
                    \begin{tikzpicture}[domain=0:5]
                        \draw[->] (0, 0) to (6, 0);
                        \draw[->] (0, 0) to (0, 6);
                        
                        \draw [color=ff4500] plot (\x, {-.8 * (\x - 1) * (\x - 4) + 3.2});

                        %\node (1) at (0, 0) {$\times$};
                        %\node (2) at (.5, 1.8) {$\times$};
                        %\node (3) at (1, 3.2) {$\times$};
                        %\node (4) at (1.5, 4.2) {$\times$};
                        %\node (5) at (2.5, 5) {$\times$};
                        %\node (6) at (3.5, 4.2) {$\times$};
                        %\node (7) at (4, 3.2) {$\times$};
                        %\node (8) at (4.5, 1.8) {$\times$};
                        %\node (9) at (5, 0) {$\times$};

                        \node (1) at (0, 0) {$\times$};
                        \node (2) at (.4, 2.1) {$\times$};
                        \node (3) at (1, 2.8) {$\times$};
                        \node (4) at (1.5, 4.2) {$\times$};
                        \node (5) at (2.5, 5.5) {$\times$};
                        \node (6) at (3.4, 4.0) {$\times$};
                        \node (7) at (4.2, 3.3) {$\times$};
                        \node (8) at (4.4, 1.8) {$\times$};
                        \node (9) at (5, 0) {$\times$};

                        %\draw [color=blue] plot [smooth] coordinates {(1) (2) (.6, 1.8) (3) (1.20, 4.1) (4) (2, 4.3) (5) (2.9, 5.5) (6) (7) (4.25, 2.2) (8) (9)};
                        \draw [color=blue] plot [smooth] coordinates {(1) (2) (3) (4) (5) (6) (7) (8) (9)};
                    \end{tikzpicture}
                \end{center}

                Il y a sous-apprentissage lorsque l'algorithme ne tient pas assez compte des données d'entraînement et n'apprend donc pas assez de règles pour obtenir un bon résultat.

                Par exemple : régression linéaire sur les même données.

                %............................................................
                %\newline
                %\quo{Même courbe quadratique, mais approximation aberrante avec une droite.}
                %\newline
                %............................................................

                \begin{center}
                    \begin{tikzpicture}[domain=0:5]
                        \draw[->] (0, 0) to (6, 0);
                        \draw[->] (0, 0) to (0, 6);
                        
                        \draw [color=ff4500] plot (\x, {-.8 * (\x - 1) * (\x - 4) + 3.2});
                        \node (1) at (0, 0) {$\times$};

                        \node (2) at (.4, 2.1) {$\times$};
                        \node (3) at (1, 2.8) {$\times$};
                        \node (4) at (1.5, 4.2) {$\times$};
                        \node (5) at (2.5, 5.5) {$\times$};
                        \node (6) at (3.4, 4.0) {$\times$};
                        \node (7) at (4.2, 3.3) {$\times$};
                        \node (8) at (4.4, 1.8) {$\times$};
                        \node (9) at (5, 0) {$\times$};

                        \draw[color=blue] (.3, 2.5) to (5, 4);
                    \end{tikzpicture}
                \end{center}

                Pour des algorithmes d'apprentissage qui ajustent leurs paramètres en cours d'apprentissage, il est alors nécessaire d'avoir un troisième jeu de données, appelé jeu de validation, qui sert à détecter le sur-apprentissage afin d'interrompre l'algorithme.
                Une répartition fréquente des données entre jeu d'entraînement, jeu de validation, et jeu de test est de l'ordre de 60\% / 20\% / 20\%.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Matrice de confusion}}
                $\bullet$ Remarque : les résultats obtenus par exécution du classificateur résultant de l'apprentissage sur les données du jeu de test permettent de calculer le taux d'erreur, \textit{i.e} le rapport entre le nombre de tests pour lesquels le résultat est faux et le nombre total de tests.

                Cependant, ce taux ne donne qu'une information partielle sur les performances de classification car il n'indique rien sur la nature des erreurs.
                Dans le cadre des problèmes de classification, la notion de \emph{matrice de confusion} permet d'effectuer une analyse plus poussée.

                \vspace{12pt}
                
                $\bullet$ Définition (\textit{matrice de confusion}) : on considère un problème de classification à $C$ classes $C_0, \ldots, C_{C - 1}$.

                La \emph{matrice de confusion} d'un classificateur pour un jeu de test donné est la matrice de taille $C \times C$ telle que $\forall i, j \in \nset 0 {C - 1}$, le coefficient $(i, j)$ de la matrice est le nombre de données de test appartenant à la classe $C_i$ pour lesquelles le classificateur a renvoyé la classe $C_j$.

                \vspace{12pt}
                
                $\bullet$ La matrice suivante pour un problème à trois classes :
                \[
                    \begin{pmatrix}
                        6 & 0 & 2
                        \\
                        0 & 5 & 0
                        \\
                        1 & 1 & 5
                    \end{pmatrix}
                \]

                Les réponses correctes sont situées la diagonale donc le taux d'erreur est la somme des valeurs qui ne sont pas sur la diagonale divisée par la somme de toutes les valeurs.
                Ici : $\dfrac{4}{20} = \dfrac 1 5$, soit $20\%$.

                \begin{indt}{La matrice indique également :}
                    $-$ que le classificateur ne se trompe jamais sur les données de la classe $C_1$ ;

                    $-$ que lorsque le classificateur se trompe sur une donnée de la classe $C_0$, c'est qu'il y a confusion uniquement avec la classe $C_2$ et qu'il y a $\dfrac 2 8 = \dfrac 1 4$ soit $25\%$ des données sur lesquelles il y a confusion ;

                    $-$ le taux d'erreur pour les données de chaque classe ;

                    $-$ que lorsque le classificateur répond $C_0$, il donne une bonne réponse dans $\dfrac 6 7$, soit environ $85.7\%$ des cas ;

                    $-$ etc.
                \end{indt}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Mesures de précision dans le cas d'un classificateur binaire (H.P)}}
                $\bullet$ Dans le cas d'un problème de classification binaire, le classificateur n'a que deux réponses possibles, que l'on peut assimiler à "vrai" et "faux".
                C'est donc un algorithme qui essaie de résoudre un problème de décision.

                \begin{indt}{La matrice de confusion pour un tel algorithme est de taille $2 \times 2$, et si on assimile la classe $C_0$ à "vrai" et la classe $C_1$ à "faux", on dit que le coefficient}
                    $-$ $(0, 0)$ représente les vrai positifs ;

                    $-$ $(0, 1)$ représente les faux négatifs ;

                    $-$ $(1, 0)$ représente les faux positifs ;

                    $-$ $(1, 1)$ représente les vrai négatifs.
                \end{indt}

                Dans ce cas, il est d'usage de définir certaines quantités, appelées \emph{mesures de précision}, qui permettent d'évaluer la qualité du classificateur.

                \vspace{12pt}
                
                \begin{indt}{$\bullet$ Définition : étant donné une matrice de confusion $2 \times 2$, on définit :}
                    $-$ la \emph{sensibilité} : c'est le taux de vrais positifs, \textit{i.e} le rapport entre le nombre de vrais positifs et le nombre de données positives (vrais positifs et faux négatifs)

                    Cela représente la capacité du classificateur à reconnaître des données de la classe $C_0$.

                    \vspace{6pt}
                    
                    $-$ La \emph{spécificité} : c'est le taux de vrais négatifs, donc cela représente la capacité du classificateur à reconnaître les données de la classe $C_1$.

                    \vspace{6pt}
                    
                    $-$ La \emph{précision} : c'est le rapport entre le nombre de vrais positifs et le nombre de réponses positives de classificateur.
                    Cela représente la fiabilité du classificateur lorsqu'il répond "vrai".
                \end{indt}

                \[
                    \begin{pmatrix}
                        a & b
                        \\
                        c & d
                    \end{pmatrix}
                \]

                Sensibilité : $\dfrac{a}{a + b}$

                Spécificité : $\dfrac{d}{c + d}$

                Précision : $\dfrac{a}{a + c}$

                \vspace{12pt}
                
                $\bullet$ Remarque : au-delà de ces mesures, on peut également tracer la courbe ROC (\textit{Receiver Operator Characteristic}) qui représente en abscisse le pourcentage de faux positifs et en ordonnées le pourcentage de vrais positifs. Un point correspond à une exécution d'un classificateur sur un je de test. On fait en général varier les paramètres d'un algorithme d'apprentissage et on place des point pour chacun des classificateurs obtenus, ce qui permet d'obtenir une courbe, appelée courbe ROC, représentant les performances de l'algorithme d'apprentissage.

                \vspace{12pt}
                
                Exemple :

                \begin{center}
                    \begin{tikzpicture}
                        \draw[->] (0, 0) to (5, 0);
                        \draw[->] (0, 0) to (0, 5);

                        \draw (0, 0) to (4.5, 4.5);

                        \node at (3, 2) [color=red] {$\times$};

                        \node (1) at (.25, 1.5) {$\times$};
                        \node (2) at (.6, 2.8) {$\times$};
                        \node (3) at (1.2, 3.9) {$\times$};
                        \node (4) at (2.3, 4.3) {$\times$};
                        \node (5) at (3.5, 4.45) {$\times$};

                        \draw[color=ff4500] plot [smooth, tension=.6] coordinates {(0, 0) (1) (2) (3) (4) (5) (4.5, 4.5)};

                        \fill [pattern = dots] % needs \usetikzlibrary{patterns}
                            (0, 0)
                            -- plot [smooth, tension=.6] coordinates {(0, 0) (1) (2) (3) (4) (5) (4.5, 4.5)}
                            -- (4.5, 4.5);
                    \end{tikzpicture}
                \end{center}

                La diagonale représente les cas où on renvoie autant de bonnes réponses que de mauvaises.
                Cela représente donc un algorithme qui ferait un tirage aléatoire pour répondre.

                Un point situé sous cette courbe représente un algorithme qui ferait moins bien que le hasard.

                On mesure en général la qualité d'un algorithme d'apprentissage par la valeur de l'aire située entre sa courbe ROC et la  diagonale.

                L'objectif est de trouver un algorithme pour lequel cette aire est maximale puis d'ajuster ses paramètres pour obtenir un classificateur dont le point est proche de celui du coin supérieur gauche.
            \end{indt}
        \end{indt}
    \end{indt}
    
\end{document}
%--------------------------------------------End
