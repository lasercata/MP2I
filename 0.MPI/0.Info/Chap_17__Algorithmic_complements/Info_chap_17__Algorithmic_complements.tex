\documentclass[a4paper, 12pt, twoside]{article}


%------------------------------------------------------------------------
%
% Author                :   Lasercata
% Last modification     :   2023.01.17
%
%------------------------------------------------------------------------


%------ini
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
%\usepackage[english]{babel}


%------geometry
\usepackage[textheight=700pt, textwidth=500pt]{geometry}


%------color
\usepackage{xcolor}
\definecolor{ff4500}{HTML}{ff4500}
\definecolor{00f}{HTML}{0000ff}
\definecolor{0ff}{HTML}{00ffff}
\definecolor{656565}{HTML}{656565}

%\renewcommand{\emph}{\textcolor{ff4500}}
%\renewcommand{\em}{\color{ff4500}}

\newcommand{\Emph}{\textcolor{ff4500}}

\newcommand{\strong}[1]{\textcolor{ff4500}{\bf #1}}
\newcommand{\st}{\color{ff4500}\bf}


%------Code highlighting
%---listings
\usepackage{listings}

\definecolor{cbg}{HTML}{272822}
\definecolor{cfg}{HTML}{ececec}
\definecolor{ccomment}{HTML}{686c58}
\definecolor{ckw}{HTML}{f92672}
\definecolor{cstring}{HTML}{e6db72}
\definecolor{cstringlight}{HTML}{98980f}
\definecolor{lightwhite}{HTML}{fafafa}

\lstdefinestyle{DarkCodeStyle}{
    backgroundcolor=\color{cbg},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstring},
    basicstyle=\ttfamily\footnotesize\color{cfg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    xleftmargin=\leftskip
}

\lstdefinestyle{LightCodeStyle}{
    backgroundcolor=\color{lightwhite},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstringlight},
    basicstyle=\ttfamily\footnotesize\color{cbg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=L,
    xleftmargin=\leftskip
}

%\lstset{style=DarkCodeStyle}
\lstset{style=LightCodeStyle}
%Usage : \begin{lstlisting}[language=Caml, xleftmargin=xpt] ... \end{lstlisting}


%---Algorithm
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\SetKwProg{Fn}{Function}{:}{}
\SetKw{KwPrint}{Print}

\newcommand\commfont[1]{\textit{\texttt{\textcolor{656565}{#1}}}}
\SetCommentSty{commfont}
\SetProgSty{texttt}
\SetArgSty{textnormal}
\SetFuncArgSty{textnormal}
%\SetProgArgSty{texttt}

\newenvironment{indalgo}[2][H]{
    \begin{algoBox}
        \begin{algorithm}[#1]
            \caption{#2}
}
{
        \end{algorithm}
    \end{algoBox}
}


%---tcolorbox
\usepackage[many]{tcolorbox}
\DeclareTColorBox{emphBox}{O{black}O{lightwhite}}{
    breakable,
    outer arc=0pt,
    arc=0pt,
    top=0pt,
    toprule=-.5pt,
    right=0pt,
    rightrule=-.5pt,
    bottom=0pt,
    bottomrule=-.5pt,
    colframe=#1,
    colback=#2,
    enlarge left by=10pt,
    width=\linewidth-\leftskip-10pt,
}

\DeclareTColorBox{algoBox}{O{black}O{lightwhite}}{
    breakable,
    arc=0pt,
    top=0pt,
    toprule=-.5pt,
    right=0pt,
    rightrule=-.5pt,
    bottom=0pt,
    bottomrule=-.5pt,
    left=0pt,
    leftrule=-.5pt,
    colframe=#1,
    colback=#2,
    width=\linewidth-\leftskip-10pt,
}


%-------make the table of content clickable
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}


%------pictures
\usepackage{graphicx}
%\usepackage{wrapfig}

\usepackage{tikz}
%\usetikzlibrary{babel}             %Uncomment this to use circuitikz
%\usetikzlibrary{shapes.geometric}  % To draw triangles in trees
%\usepackage{circuitikz}            %Electrical circuits drawing


%------tabular
%\usepackage{color}
%\usepackage{colortbl}
%\usepackage{multirow}


%------Physics
%---Packages
%\usepackage[version=4]{mhchem} %$\ce{NO4^2-}$

%---Commands
\newcommand{\link}[2]{\mathrm{#1} \! - \! \mathrm{#2}}
\newcommand{\pt}[1]{\cdot 10^{#1}} % Power of ten
\newcommand{\dt}[2][t]{\dfrac{\mathrm d #2}{\mathrm d #1}} % Derivative


%------math
%---Packages
%\usepackage{textcomp}
%\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools} % For abs
\usepackage{stmaryrd} %for \llbracket and \rrbracket
\usepackage{mathrsfs} %for \mathscr{x} (different from \mathcal{x})

%---Commands
%-Sets
\newcommand{\N}{\mathbb{N}} %set N
\newcommand{\Z}{\mathbb{Z}} %set Z
\newcommand{\Q}{\mathbb{Q}} %set Q
\newcommand{\R}{\mathbb{R}} %set R
\newcommand{\C}{\mathbb{C}} %set C
\newcommand{\U}{\mathbb{U}} %set U
\newcommand{\seg}[2]{\left[ #1\ ;\ #2 \right]}
\newcommand{\nset}[2]{\left\llbracket #1\ ;\ #2 \right\rrbracket}

%-Exponantial / complexs
\newcommand{\e}{\mathrm{e}}
\newcommand{\cj}[1]{\overline{#1}} %overline for the conjugate.

%-Vectors
\newcommand{\vect}{\overrightarrow}
\newcommand{\veco}[3]{\displaystyle \vect{#1}\binom{#2}{#3}} %vector + coord

%-Limits
\newcommand{\lm}[2][{}]{\lim\limits_{\substack{#2 \\ #1}}} %$\lm{x \to a} f$ or $\lm[x < a]{x \to a} f$
\newcommand{\Lm}[3][{}]{\lm[#1]{#2} \left( #3 \right)} %$\Lm{x \to a}{f}$ or $\Lm[x < a]{x \to a}{f}$
\newcommand{\tendsto}[1]{\xrightarrow[#1]{}}

%-Integral
\newcommand{\dint}[4][x]{\displaystyle \int_{#2}^{#3} #4 \mathrm{d} #1} %$\dint{a}{b}{f(x)}$ or $\dint[t]{a}{b}{f(t)}$

%-left right
\newcommand{\lr}[1]{\left( #1 \right)}
\newcommand{\lrb}[1]{\left[ #1 \right]}
\newcommand{\lrbb}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\lrangle}[1]{\left\langle #1 \right\rangle}

%-Others
\newcommand{\para}{\ /\!/\ } %//
\newcommand{\ssi}{\ \Leftrightarrow \ }
\newcommand{\eqsys}[2]{\begin{cases} #1 \\ #2 \end{cases}}

\newcommand{\med}[2]{\mathrm{med} \left[ #1\ ;\ #2 \right]}  %$\med{A}{B} -> med[A ; B]$
\newcommand{\Circ}[2]{\mathscr{C}_{#1, #2}}

\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}

\newcommand{\oboxed}[1]{\textcolor{ff4500}{\boxed{\textcolor{black}{#1}}}} %orange boxed

\newcommand{\rboxed}[1]{\begin{array}{|c} \hline #1 \\ \hline \end{array}} %boxed with right opened
\newcommand{\lboxed}[1]{\begin{array}{c|} \hline #1 \\ \hline \end{array}} %boxed with left opened

\newcommand{\orboxed}[1]{\textcolor{ff4500}{\rboxed{\textcolor{black}{#1}}}} %orange right boxed
\newcommand{\olboxed}[1]{\textcolor{ff4500}{\lboxed{\textcolor{black}{#1}}}} %orange left boxed


%------commands
%---to quote
\newcommand{\simplecit}[1]{\guillemotleft$\;$#1$\;$\guillemotright}
\newcommand{\cit}[1]{\simplecit{\textcolor{656565}{#1}}}
\newcommand{\quo}[1]{\cit{\it #1}}

%---to indent
\newcommand{\ind}[1][20pt]{\advance\leftskip + #1}
\newcommand{\deind}[1][20pt]{\advance\leftskip - #1}

%---to indent a text
\newcommand{\indented}[2][20pt]{\par \ind[#1] #2 \par \deind[#1]}
\newenvironment{indt}[2][20pt]{#2 \par \ind[#1]}{\par \deind} %Titled indented env

%---title
\newcommand{\thetitle}[2]{\begin{center}\textbf{{\LARGE \underline{\Emph{#1} :}} {\Large #2}}\end{center}}

%---Maths environments
%-Proofs
\newenvironment{proof}[1][{}]{\begin{indt}{$\square$ #1}}{$\blacksquare$ \end{indt}}

%-Maths parts (proposition, definition, ...)
\newenvironment{mathpart}[1]{\begin{indt}{\boxed{\text{\textbf{#1}}}}}{\end{indt}}
\newenvironment{mathbox}[1]{\boxed{\text{\textbf{#1}}}\begin{emphBox}}{\end{emphBox}}
\newenvironment{mathul}[1]{\begin{indt}{\underline{\textbf{#1}}}}{\end{indt}}

\newenvironment{theo}{\begin{mathpart}{Théorème}}{\end{mathpart}}
\newenvironment{Theo}{\begin{mathbox}{Théorème}}{\end{mathbox}}

\newenvironment{prop}{\begin{mathpart}{Proposition}}{\end{mathpart}}
\newenvironment{Prop}{\begin{mathbox}{Proposition}}{\end{mathbox}}
\newenvironment{props}{\begin{mathpart}{Propriétés}}{\end{mathpart}}

\newenvironment{defi}{\begin{mathpart}{Définition}}{\end{mathpart}}
\newenvironment{meth}{\begin{mathpart}{Méthode}}{\end{mathpart}}

\newenvironment{Rq}{\begin{mathul}{Remarque :}}{\end{mathul}}
\newenvironment{Rqs}{\begin{mathul}{Remarques :}}{\end{mathul}}

\newenvironment{Ex}{\begin{mathul}{Exemple :}}{\end{mathul}}
\newenvironment{Exs}{\begin{mathul}{Exemples :}}{\end{mathul}}


%------Sections
% To change section numbering :
% \renewcommand\thesection{\Roman{section}}
% \renewcommand\thesubsection{\arabic{subsection})}
% \renewcommand\thesubsubsection{\textit \alph{subsubsection})}

% To start numbering from 0
% \setcounter{section}{-1}


%------page style
\usepackage{fancyhdr}
\usepackage{lastpage}

\setlength{\headheight}{18pt}
\setlength{\footskip}{50pt}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE, RO]{\textit{\textcolor{black}{\today}}}
\fancyhead[RE, LO]{\large{\textsl{\Emph{\texttt{\jobname}}}}}

\fancyfoot[RO, LE]{\textit{\texttt{\textcolor{black}{Page \thepage /}\pageref{LastPage}}}}
\fancyfoot[LO, RE]{\includegraphics[scale=0.12]{/home/lasercata/Pictures/1.images_profil/logo/mieux/lasercata_logo_fly_fond_blanc.png}}


%------init lengths
\setlength{\parindent}{0pt} %To avoid using \noindent everywhere.
\setlength{\parskip}{3pt}

\usepackage{bbm}
\newcommand{\1}{\mathbbm 1}


%---------------------------------Begin Document
\begin{document}
    
    \thetitle{Chapitre 17}{Complément d'algorithmique}
    
    \tableofcontents
    \newpage
    
    \begin{indt}{\section{Optimisation}}
        \begin{indt}{\subsection{Optimisation exacte}}
            \begin{indt}{\subsubsection{Introduction}}
                On s'intéresse ici à la résolution de problèmes d'optimisation au sens de la définition du chapitre 16, en 1.2.2 : on cherche un algorithme calculant une solution optimale \textbf{pour toute instance}.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple : le problème du sac à dos}}
                \label{1.1.2}

                Le problème est le suivant : on dispose d'objets de poids respectifs $w_0, \ldots, w_{n - 1}$ et de valeurs respectives $p_0, \ldots, p_{n - 1}$ et d'un sac à dos capable de supporter un poids $W$. On souhaite sélectionner des objets de sorte à maximiser la valeur totale sans dépasser la capacité du sac à dos.

                Dans la version en variables réelles, on suppose que l'on peut prendre des fractions des objets. Le problème d'optimisation se formule ainsi :

                Maximiser $\displaystyle \sum_{i = 0}^{n - 1} x_i p_i$ sous les contraintes :
                \[
                    \begin{cases}
                        \displaystyle
                        \sum_{i = 0}^{n - 1} x_i w_i \le W
                        \\
                        \forall i \in \nset 0 {n - 1},\ x_i \in \seg 0 1
                    \end{cases}
                \]

                Ce problème est résolu par un algorithme glouton :

                \begin{indalgo}{Solution du problème du sac à dos, version en variables réelles}
                    \label{alg:1}
                    Trier les objets par $\dfrac{p_i}{w_i}$ décroissant\;

                    \While{que possible en considérant les objets dans cet ordre}{
                        Fixer $x_i$ à 1\;
                    }

                    Lorsque cela n'est plus possible, prendre la fraction de l'objet courant permettant de remplir le sac\;
                \end{indalgo}

                Cet algorithme calcule bien une solution optimale : si on note $i$ l'objet de $\dfrac{p_i}{w_i}$ maximal non encore sélectionné et si une solution optimale coïncidant avec l'algorithme sur les objets avant $i$, et ne sélectionne pas cet objet dans son intégralité, alors $\exists j$ tel que la solution optimale sélectionne une fraction de l'objet $j$ qui est $> x_j$.

                Dans ce cas, il existe $\delta_j > 0$ tel que l'on peut ajouter une quantité $\dfrac{\delta_j}{w_i}$ de l'objet $i$ et retirer une quantité $\dfrac{\delta_j}{w_j}$ de l'objet $j$ à la solution optimale.

                La variation de poids est $\dfrac{\delta_j}{w_i}w_i - \dfrac{\delta_j}{w_j}w_j = 0$, donc on a toujours une solution.

                La variation de valeur est
                \[
                    \dfrac{\delta_j}{w_i} p_i - \dfrac{\delta_j}{w_j} p_j
                    = \underbrace{\delta_j}_{> 0}\underbrace{\lr{\dfrac{p_i}{w_i} - \dfrac{p_j}{w_j}}}_{\ge 0}
                \]

                Donc la solution reste optimale.

                On peut donc modifier la solution optimale jusqu'à l'obtention d'une solution optimale ayant choisi l'objet $i$ dans son intégralité.

                L'invariant \simplecit{il existe une solution optimale ayant fait les mêmes choix que l'algorithme glouton} est vrai.

                \vspace{12pt}
                
                Remarque : le problème du sac à dos, dans sa version entière (les $x_i \in \set{0, 1}$) ne peut pas être résolu par l'algorithme glouton (algorithme n°\ref{alg:1}) auquel on retire la dernière étape ne prenant qu'une fraction du dernier objet.

                \begin{center}
                    \begin{tabular}{l|ccc}
                        Poids & 5 & 5 & 7
                        \\
                        \hline
                        Valeur & 5 & 5 & 8
                    \end{tabular}
                    $\qquad W = 10$
                \end{center}

                \textit{Cf} l'exemple ci-dessus : l'algorithme glouton donne une solution de valeur $8$ en prenant l'objet de poids 7 alors qu'une solution optimale est de valeur 10 : on prend les deux objets de poids 5.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Séparation et évaluation (\textit{branch and bound})}}
                $\bullet$ Une technique de résolution des problèmes d'optimisation consiste à effectuer une exploration exhaustive de l'ensemble des solutions et à conserver la meilleure solution.

                Cependant, on se heurte à des problèmes de complexité (exemple : pour le sac à dos en variables entières, il y a $2^n$ solutions potentielles à tester).

                On peut parfois accélérer la recherche grâce à l'heuristique du retour sur trace (\textit{cf} chapitre 8, 4.3).

                Par exemple, pour le problème du sac à dos, il y a surement de nombreuses combinaisons d'objets qui dépassent la capacité du sac à dos. On peut donc sélectionner les objets un à un et lorsque l'on s'aperçoit que la capacité du sac à dos est dépassée, on revient sur le dernier choix.

                En pratique, cela revient à construire un arbre binaire dans lequel tous les n\oe uds de même profondeur correspondent à un même objet et pour ces n\oe uds, le fils gauche correspond au cas où l'on a sélectionné l'objet et le fils droit au cas où l'objet n'est pas sélectionné.

                On élague les branches correspondant à des sélections dépassant la capacité du sac à dos.

                Si $w_1 + w_2 > W$,

                \begin{center}
                    \begin{tikzpicture}
                        \node (0) at (0, 0) {$\bullet$}
                            child [xshift=-90pt] {node {$\bullet$}
                                child [xshift=-30pt] {node {$\bullet$}
                                    child {node {} edge from parent [dashed]}
                                    edge from parent [above left] node {$x_2 = 1$}
                                }
                                child [xshift=30pt] {node {$\bullet$}
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above left] node {$x_3 = 1$}
                                    }
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above right] node {$x_3 = 0$}
                                    }
                                    edge from parent [above right] node {$x_2 = 0$}
                                }
                                edge from parent [above left] node {$x_1 = 1$}
                            }
                            child [xshift=90pt] {node {$\bullet$}
                                child [xshift=-30pt] {node {$\bullet$}
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above left] node {$x_3 = 1$}
                                    }
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above right] node {$x_3 = 0$}
                                    }
                                    edge from parent [above left] node {$x_2 = 1$}
                                }
                                child [xshift=30pt] {node {$\bullet$}
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above left] node {$x_3 = 1$}
                                    }
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above right] node {$x_3 = 0$}
                                    }
                                    edge from parent [above right] node {$x_2 = 0$}
                                }
                                edge from parent [above right] node {$x_1 = 0$}
                            }
                        ;

                        \node (1) at (-6, -6) {
                            \begin{tabular}{c}
                                Sous-arbre non parcouru
                                \\
                                car dépasse la capacité
                            \end{tabular}
                        };
                        \draw[->] (1) to (-5.7, -4.5);
                    \end{tikzpicture}
                \end{center}

                Dans le cadre de la résolution d'un problème d'optimisation, on peut parfois élaguer encore plus l'arbre de recherche en considérant le coût des solutions construites : si on sait évaluer une borne du meilleur possible pour une série de choix sans parcourir l'intégralité du sous-arbre correspondant, on peut parfois élaguer ce sous-arbre si on connaît déjà une solution de coût meilleur que cette borne.

                \begin{indt}{Cette méthode consiste à concevoir un algorithme par séparation et évaluation :}
                    $-$ La séparation consiste à diviser le problème en sous-problèmes, donc à créer un branchement dans l'arbre de reverche.

                    Exemple : pour le problème du sac à dos, on a deux sous-problèmes, selon que l'objet $i$ est sélectionné ou non.

                    \vspace{6pt}
                    
                    $-$ L'évaluation consiste à déterminer une borne sur le coût d'une solution optimale \textbf{réalisable avec les choix déjà faits} et à le comparer avec une borne connue pour savoir s'il est nécessaire de poursuivre l'exploration du sous-arbre.
                \end{indt}

                \vspace{12pt}
                
                \begin{indt}{Pour que cette méthode soit efficace, on a besoin de bonnes heuristiques pour :}
                    $-$ La séparation : si les choix initiaux convergent rapidement vers une \simplecit{bonne solution}, on élaguera plus de branches dans la suite de l'exploration.

                    \vspace{6pt}
                    
                    $-$ L'évaluation : on doit pouvoir calculer \emph{efficacement} une borne \emph{la plus juste possible} pour avoir de bonnes chances d'élaguer des branches.
                \end{indt}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple : le problème du sac à dos}}
                $\bullet$ Pour la séparation, on sélectionne ou pas un objet, avec l'heuristique suivante : on considère les objets par $\dfrac{p_i}{w_i}$ décroissant.

                \vspace{6pt}
                
                $\bullet$ Pour l'évaluation, on utilise l'heuristique de relaxation : on relâche certaines contraintes, ce qui élargit le domaine des solutions donc permet potentiellement d'atteindre un meilleur coût.
                Si le problème relâché est plus simple à résoudre, le coût d'une solution optimale est donc la borne recherchée.

                Ici, on effectue une relaxation continue : on n'impose plus aux $x_i$ d'être des entiers, ce qui nous ramène au problème vu en \ref{1.1.2} (page \pageref{1.1.2}), que l'on sait résoudre efficacement (en $\mathcal O(n)$ car les objets seront triés une seule fois au début de l'algorithme pour l'heuristique de séparation).

                Remarque : si l'algorithme nous donne une solution entière : on a trouvé la solution optimale (selon les choix qui sont déjà fait).

                Si l'algorithme nous donne une solution avec un terme dans $]0, 1[$, la partie entière de la valeur de cette solution est une borne supérieure sur la solution optimale du problème entier.

                \vspace{12pt}
                
                Exemple : on considère l'instance suivante :

                \begin{center}
                    \begin{tabular}{r|cccc}
                        $i$ & 1 & 2 & 3 & 4
                        \\
                        \hline
                        $p_i$ & 1 & 1 & 5 & 3
                        \\
                        \hline
                        $w_i$ & 3 & 2 & 4 & 2
                    \end{tabular}
                    $\qquad W = 5$
                \end{center}

                Le tri des objets indique qu'on doit les traiter dans l'ordre suivant : 4, 3, 2, 1.

                On note au cours de l'exécution, $W$ la capacité courante du sac à dos, et $V$ la valeur totale courante des objets sélectionnés.

                \begin{center}
                    \begin{tikzpicture}[scale=1.6]
                        \node (0) at (0, 0) [rectangle, draw] {$W = 5, V = 0, \sup = \floor{3 + \dfrac 3 4 5} = 6$}
                            child [xshift=-60pt] {node [rectangle, draw] {$W = 3, V = 3, \sup = 6$}
                                child [xshift=-20pt] {node [rectangle, draw] {$W = -1$}
                                edge from parent [above left] node {$x_3 = 1$}}
                                child [xshift=20pt] {node [rectangle, draw] {$
                                        \begin{array}{c}
                                            W = 3, V = 3
                                            \\
                                            \sup = 3 + \floor{1 + \frac 1 3} = 4
                                        \end{array}
                                $}
                                    child [xshift=-20pt] {node [rectangle, draw] {$
                                            \begin{array}{c}
                                                W = 1, V = 4
                                                \\
                                                \sup = 4 + \floor{\frac 1 3} = 4 = V
                                            \end{array}
                                    $} edge from parent [left] node {$x_2 = 1$}}
                                    child [xshift=20pt] {node [rectangle, draw] {$
                                            \begin{array}{c}
                                                W = 3, V = 3
                                                \\
                                                \sup = 4 = \mathrm{inf}
                                            \end{array}
                                    $} edge from parent [right] node {$x_2 = 0$}}
                                    edge from parent [above right] node {$x_3 = 0$}
                                }
                                edge from parent [above left] node {$x_4 = 1$}
                            }
                            child [xshift=60pt] {node [rectangle, draw] {$
                                    \begin{array}{c}
                                        W = 5, V = 0
                                        \\
                                        \sup = \floor{5 + \frac 1 2} = 5
                                    \end{array}
                            $}
                                child [xshift=-20pt] {node [rectangle, draw] {$
                                        \begin{array}{c}
                                            W = 1, V = 5
                                            \\
                                            \sup = 5 = V > \mathrm{inf}
                                        \end{array}
                                $} edge from parent [left] node {$x_3 = 1$}}
                                edge from parent [above right] node {$x_4 = 0$}
                            }
                        ;
                        
                        \node (1) at (-4.4, -3.5) {\huge{$\times$}};
                        \node (1) at (.3, -5.2) {\huge{$\times$}};
                    \end{tikzpicture}
                \end{center}

                Solution $(0, 1, 0, 1)$, et $\inf = 4$

                Solution $(0, 0, 1, 0)$, et $\inf = 5$

                \vspace{12pt}
                
                Conclusion : sélectionner uniquement l'objet $3$ est une solution optimale.

                \vspace{6pt}
                
                \boxed{\rm Exo} écrire l'exécution de l'algorithme si l'heuristique de séparation consiste à prendre les objets par ordre d'indice ou par ordre de poids décroissant ou de valeur décroissante.

                Tester aussi l'heuristique d'évaluation qui consiste à prendre la borne des valeurs des objets comme borne supérieure.
            \end{indt}
        \end{indt}

        \vspace{12pt}
        
        \begin{indt}{\subsection{Optimisation et \textbf{NP}-complétude}}
            \begin{indt}{\subsubsection{Introduction}}
                On considère un problème d'optimisation caractérisé par une relation $\mathcal R \subseteq A \times B$ et une fonction de coût $c : B \longrightarrow \R^+$.
                On rappelle que le problème de décision associé à ce problème d'optimisation s'énonce ainsi : étant donné une instance $a \in A$ et un entier $k \in \N$, existe-t-il une solution $b \in B$ telle que
                \[
                    \begin{cases}
                        a \mathcal R b
                        \\
                        c(b) \le k
                    \end{cases}
                \]

                Remarque : on peut aussi considérer des problèmes de maximisation et la condition à satisfaire est alors $c(k) \ge k$.

                \vspace{6pt}
                
                Fait : s'il existe un algorithme de complexité polynomiale qui résout le problème d'optimisation, alors le problème de décision associé appartient à la classe $\mathbf{P}$.
                En effet, il suffit pour une instance $a$ d'exécuter l'algorithme qui résout le problème d'optimisation sur $a$ (complexité polynomiale en la taille de $a$) puis de comparer le coût de la solution optimale obtenue et $k$ (complexité $\mathcal O(\log k)$).
                Cela donne un algorithme polynomial qui résout le problème de décision.

                \vspace{12pt}
                
                Conséquence : si le problème de décision associé à un problème d'optimisation est \textbf{NP}-complet, alors on a peu d'espoir de trouver un algorithme polynomial qui résout le problème d'optimisation.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Retour au problème du sac à dos}}
                On sait que le problème en variables réelles peut être résolu en temps $\mathcal O(n\log n)$ par un algorithme glouton et que cet algorithme ne fonctionne pas pour le problème en variables entières.

                \vspace{12pt}
                
                $\bullet$ Résolution par programmation dynamique : on considère une instance
                \[
                    (w_1, \ldots, w_n, p_1, \ldots, p_n, W)
                \]
                du problème du sac à dos en variables entières.

                $\forall i \in \nset 0 n,\ \forall w \in \nset 0 W$, on note $V(i, w)$ la valeur maximale que l'on peut atteindre en sélectionnant des objets parmi ceux d'indices 1 à $i$ dans un sac à dos de capacité $w$.

                On cherche à obtenir $V(n, W)$.

                $
                    \forall i \in \nset 0 {n - 1},\
                    \forall w \in \nset 0 W,\
                $
                \[
                    V(i + 1, w) =
                    \begin{cases}
                        \vspace{-32pt}
                        \\
                        V(i, w)
                        & \text{si}\ \overbrace{w_{i + 1} > w}^{\substack{\text{on ne peut pas} \\ \text{sélectionner l'objet $i + 1$}}}
                        \vspace{6pt}
                        \\
                        \max(\!\!\!\!\!\! \underbrace{V(i, w)}_{\substack{\text{on ne sélectionne} \\ \text{pas l'objet $i + 1$}}}\!\!\!\!\!\!\!,\ \, \underbrace{p_{i + 1} + V(i, w - w_{i + 1})}_{\substack{\text{on sélectionne} \\ \text{l'objet $i + 1$}}})
                        & \text{sinon}
                        \vspace{-21pt}
                    \end{cases}
                \]

                \vspace{12pt}

                $\forall w \in \nset 0 W,\ V(0, W) = 0$

                On peut donc remplir la matrice $V$ ligne par ligne (par $i$ croissant) et retrouver une solution réalisant $V(n, W)$ en temps $\mathcal O(W_n)$.

                Conclusion ? Aucune car c'est un algorithme de complexité exponentielle en la taille de l'instance ($W$ est de taille $\mathcal O(\log w)$).

                \vspace{12pt}
                
                $\bullet$ Proposition

                \begin{emphBox}
                    On considère le problème de décision suivant :

                    \textsc{Sac\_a\_dos} : étant donné $n$ poids $w_1, \ldots, w_n \in \N$, $n$ valeurs $x_1, \ldots, x_n \in \set{0, 1}$ telles que
                    \[
                        \begin{cases}
                            \displaystyle
                            \sum_{i = 1}^n x_i p_i \ge k
                            \\
                            \displaystyle
                            \sum_{i = 1}^n x_i w_i \le W
                        \end{cases}
                        \qquad
                        \text{?}
                    \]

                    \vspace{6pt}
                    
                    \textsc{Sac\_a\_dos} est \textbf{NP}-complet.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    $-$ \textsc{Sac\_a\_dos} $\in \mathbf{NP}$ : $x_1, \ldots, x_n$ est un certificat vérifiable en temps polynomial.

                    \vspace{6pt}
                    
                    $-$ \textsc{Sac\_a\_dos} est \textbf{NP}-difficile : on procède par réduction de $2$-\textsc{Partition} (\textit{cf} TD$_{46}$) : étant donné $S = \set{a_1, \ldots, a_n} \subseteq \N$, existe-t-il $I \subseteq \nset 1 n$ tel que
                    \[
                        \sum_{i \in I} a_i = \sum_{i \in \nset 1 n \setminus I} a_i
                        \qquad \text{?}
                    \]

                    Soit $S = \set{a_1, \ldots, a_n}$ une instance de 2-\textsc{Partition}.

                    On construit l'instance suivante de \textsc{Sac\_a\_dos} :
                    \[
                        \begin{cases}
                            \forall i \in \nset 1 n,\ w_i = p_i = a_i
                            \\
                            \displaystyle
                            W = k = \dfrac 1 2 \sum_{i = 1}^n a_i
                        \end{cases}
                    \]

                    \begin{indt}{Cette instance est calculable en temps polynomial en la taille de $S$ et cela constitue une réduction :}
                        $+$ Si $\exists I \subseteq \nset 1 n\ |\ \displaystyle \sum_{i \in I} a_i = \sum_{i \in \nset 1 n \setminus I} a_i$, alors
                        \[
                            \sum_{i = 1}^n a_i
                            = \sum_{i \in I} a_i + \sum_{i \in \nset 1 n \setminus I} a_i
                            = 2\sum_{i \in I} a_i
                        \]

                        donc en notant $\forall i \in \nset 1 n,\ x_i = \1_I(i)$, on obtient
                        \[
                            \sum_{i = 1}^n x_i p_i
                            = \sum_{i = 1}^n x_i w_i
                            = \sum_{i = 1}^n x_i a_i
                            = \sum_{i = 1}^n \1_I(i) a_i
                            = \sum_{i \in I} a_i
                            = \dfrac 1 2 \sum_{i = 1}^n a_i
                            = k
                            = W
                        \]

                        Donc les $x_i$ sont une solution de l'instance de \textsc{Sac\_a\_dos} associée.

                        \vspace{6pt}
                        
                        $+$ Réciproquement, si
                        \[
                            \exists (x_1, \ldots, x_n) \in \set{0, 1}^n\ |\
                            \begin{cases}
                                \displaystyle
                                \sum_{i = 1}^n x_i w_i \le W
                                \\
                                \displaystyle
                                \sum_{i = 1}^n x_i p_i \ge k
                            \end{cases}
                        \]

                        Alors
                        \[
                            \dfrac 1 2 \sum_{i = 1}^n a_i
                            = k
                            \le \sum_{i = 1}^n x_i p_i
                            = \sum_{i = 1}^n x_i a_i
                            = \sum_{i = 1}^n x_i w_i
                            \le W
                            = \dfrac 1 2 \sum_{i = 1}^n a_i
                        \]

                        Donc
                        \[
                            \begin{array}{rcl}
                                \displaystyle
                                \sum_{i = 1}^n x_i a_i
                                &=& \displaystyle
                                \dfrac 1 2 \sum_{i = 1}^n a_i
                                \vspace{3pt}
                                \\
                                &=& \displaystyle
                                \dfrac 1 2 \lr{\sum_{\substack{i = 1 \\ x_i = 1}}^n a_i + \sum_{\substack{i = 1 \\ x_i = 0}}^n a_i}
                                \vspace{3pt}
                                \\
                                &=& \displaystyle
                                \dfrac 1 2 \lr{\sum_{\substack{i = 1 \\ x_i = 1}}^n x_i a_i + \sum_{\substack{i = 1 \\ x_i = 0}}^n a_i}
                                \vspace{3pt}
                                \\
                                &=& \displaystyle
                                \dfrac 1 2 \lr{\sum_{\substack{i = 1}}^n x_i a_i + \sum_{\substack{i = 1 \\ x_i = 0}}^n a_i}
                            \end{array}
                        \]

                        Donc $\displaystyle \sum_{\substack{i = 1 \\ x_i = 1}}^n a_i = \sum_{i = 1}^n x_i a_i = \sum_{\substack{i = 0 \\ x_i = 0}}^n a_i$
                    \end{indt}

                    Donc $I = \set{i \in \nset 1 n\ |\ x_i = 1}$ est une solution à l'instance de 2-\textsc{Partition}
                \end{proof}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarque}}
                \label{1.2.3}

                Comme on a peu d'espoir de trouver efficacement une solution optimale à un problème d'optimisation dont le problème de décision associé est $\mathbf{NP}$-complet, on va plutôt chercher efficacement une solution \simplecit{pas trop mauvaise}.
                On cherche donc à concevoir un algorithme de complexité polynomiale qui fournit des solutions pour lesquelles on peut estimer la \simplecit{distance} à l'optimum.

                \vspace{12pt}
                
                $\bullet$ Exemple : l'algorithme glouton vu en \ref{1.1.2} (page \pageref{1.1.2}) pour le problème du sac à dos est très mauvais vis à vis du problème en variables entières : si $k \in \N^*$ et $W \in \N \setminus \set{0, 1}$, on considère deux objets tels que
                \[
                    \begin{cases}
                        p_1 = 1
                        \\
                        w_1 = \dfrac{W - 1}{k}
                    \end{cases}
                    \quad
                    \text{et}
                    \quad
                    \begin{cases}
                        p_2 = k
                        \\
                        w_2 = W
                    \end{cases}
                \]

                L'algorithme glouton sélectionne l'objet $1$ car $\dfrac{1}{\dfrac{W - 1}{k}}  = \dfrac{k}{W - 1} > \dfrac k W$, ce qui donne une solution de valeur 1 alors que la solution optimale consiste à prendre l'objet 2 pour une valeur $k$.

                Ainsi $\forall k \in \N^*$, il existe une instance telle que la solution optimale est $k$ fois meilleure que celle calculée par l'algorithme glouton.

                \vspace{12pt}
                
                $\bullet$ On peut faire mieux en modifiant légèrement l'algorithme : on garde la meilleure solution entre celle de l'algorithme glouton et celle qui consiste à ne prendre que l'objet de valeur maximale.

                \vspace{6pt}
                
                Proposition :
                \begin{emphBox}
                    On note pour toute instance $e$, $V^*(e)$ la valeur d'une solution optimale, et $V(e)$ la valeur de la solution calculée par l'algorithme glouton modifié.

                    Alors
                    \[
                        \forall e,\ V^*(e) \le 2V(e)
                    \]
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On note $e = (w_1, \ldots, w_n, p_1, \ldots, p_n, W)$.

                    Quitte à renuméroter, on peut supposer que les objets sont tirés par $\dfrac{p_i}{w_i}$ décroissant.

                    On sait que toute solution entière donne une valeur inférieure à celle d'une solution optimale réelle.
                    De plus, une telle solution est calculée \textit{via} l'algorithme glouton.

                    Alors, en notant $j \in \nset 1 n$ l'indice du premier objet que l'on ne peut pas placer intégralement dans le sac à dos, on a
                    \[
                        \begin{array}{rcl}
                            V^*(e)
                            &\le& \displaystyle
                            \sum_{i = 1}^{j - 1} p_i
                            + \underbrace{\dfrac{\displaystyle W - \sum_{i = 1}^{j - 1} w_i}{w_j}}_{< 1} p_j
                            \\
                            &\le& \displaystyle
                            \sum_{i = 1}^{j - 1} p_i + p_i
                            \vspace{3pt}
                            \\
                            &\le& \displaystyle
                            \sum_{i = 1}^{j - 1} p_i + \max_{i \in \nset 1 n} p_i
                            \vspace{3pt}
                            \\
                            &\le& \displaystyle
                            2 \max\!\lr{\sum_{i = 1}^{j - 1} a_i,\ \max_{i \in \nset 1 n} p_i}
                        \end{array}
                    \]
                    (L'algorithme glouton prend tous les objets de 1 à $j - 1$ puis la fraction de l'objet $j$ qui permet de remplir le sac à dos)
                \end{proof}

                \vspace{12pt}
                
                Remarque : on a donc un algorithme de même complexité que l'algorithme glouton et tel que la solution calculée est toujours de valeur supérieure à la moitié de la valeur optimale.

                \vspace{12pt}
                
                \boxed{\rm H.P} $\forall \varepsilon \in \R^*_+$, il existe un algorithme de complexité $\mathcal O\!\lr{\dfrac{1 + \varepsilon}{\varepsilon} n^3}$ déterminant une solution entière au problème du sac à dos et tel que $\forall e$ instance, la valeur $V(e)$ de la solution calculée vérifie $V^*(e) \le (1 + \varepsilon)V(e)$.

                Idée : par programmation dynamique : $\forall i \in \nset 0 n,\ \forall p \in \nset{0}{\displaystyle \sum_{i = 1}^n p_i}$, on calcule le poids minimal $W(i, p)$ réalisable en sélectionnant des objets parmi ceux d'indices de 1 à $i$ de sorte que la valeur obtenue vaille $p$ (et le poids $\le W$).

                On le fait avec des objets de valeur modifiée en fonction de $n$, $\varepsilon$, $p_{\max} = \max_{i \in \nset 1 n} p_i$

                $\displaystyle \forall i \in \nset 1 n$, on note $\floor{\dfrac{p_i}{2^t}}$, où
                \[
                    t = \floor{\log\!\lr{\dfrac{\varepsilon}{1 + \varepsilon} \dfrac{p_{\max}}{n}}}
                \]
            \end{indt}
        \end{indt}

        \vspace{12pt}
        
        \begin{indt}{\subsection{Algorithmes d'approximation}}
            \begin{indt}{\subsubsection{Définition (\textit{algorithme d'approximation})}}
                On considère une problème d'optimisation caractérisé par une relation $\mathcal R \subseteq A \times B$ et une fonction de coût $c : B \longrightarrow \R^+$.

                Soit $\alpha \in \R_+^*$, et $M$ un algorithme tel que $\forall a \in A$, $M$ appliqué à $a$ termine et revoie une solution associée à l'instance $a$, \textit{i.e} $a \mathcal R M(a)$.

                On dit que $M$ est un \emph{algorithme d'approximation} de facteur $\alpha$, ou \emph{$\alpha$-approximation}, pour le problème d'optimisation si, en notant $\forall a \in A$, $c^*(a)$ le coût d'une solution optimale associée à $a$ :
                \begin{indt}{}
                    $-$ si on a un problème de minimisation, $\forall a \in A,\ c(M(a)) < \alpha c^*(a)$ (et $\alpha > 1$)
                    
                    $-$ si on a un problème de maximisation, $\forall a \in A,\ c(M(a)) \ge \alpha c^*(a)$ (et $\alpha < 1$).
                \end{indt}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                L'algorithme glouton pour le problème du sac à dos n'est pas un algorithme d'approximation car on a vu en \ref{1.2.3} (page \pageref{1.2.3}) que l'on peut construire des instances telles que le rapport entre la valeur de la solution calculée est valeur optimale est arbitrairement petit.

                En revanche, on a vu que l'algorithme modifié constitue une $\dfrac 1 2$--approximation.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarques}}
                $\bullet$ On parle parfois d'$\alpha(n)$--approximation, où le facteur $\alpha$ est variable et dépend de la taille de l'instance.

                $\bullet$ On utilise souvent des algorithmes gloutons pour construire des algorithmes d'approximation.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                On considère le problème d'optimisation suivant, associé à \textsc{Vertex\_Cover} : étant donné un graphe $G = (S, A)$, déterminer une couverture par les sommets de $G$ de taille minimale.

                On a peu d'espoir de résoudre en temps polynomial ce problème d'optimisation car \textsc{Vertex\_Cover} est \textbf{NP}--complet (\textit{cf} chapitre 16, 2.3.5) : si c'était possible, étant donné une instance $(G, k)$ de \textsc{Vertex\_Cover}, on pourrait déterminer une couverture optimale de $G$ et comparer sa taille à $k$ (si sa taille est inférieure à $k$, on peut toujours ajouter des sommets pour obtenir une couverture de taille $k$).

                \vspace{12pt}
                
                $\bullet$ Algorithme d'approximation :

                Idée : on pourrait sélectionner les extrémités d'un ensemble d'arêtes qui rencontrent toutes les autres arêtes du graphe.
                Pour minimiser le nombre de sommets sélectionnés, on cherchera des arêtes sans extrémité commune, donc sur un couplage maximal.
                On ne cherchera pas un couplage maximum car on veut minimiser le nombre de sommets, donc d'arêtes.

                D'où l'algorithme glouton :
                \begin{indalgo}{}
                    \label{alg:2}

                    $C \gets \varnothing$\;
                    \While{$A \neq \varnothing$}{
                        Extraire une arête $\set{s, t}$ de $A$\;
                        $C \gets \set{s, t} \cup C$\;
                        Supprimer les arêtes adjacentes à $s$ ou à $t$\;
                    }

                    \Return $C$\;
                \end{indalgo}

                \vspace{12pt}
                
                $\bullet$ Proposition :
                \begin{emphBox}
                    Cet algorithme est une 2--approximation pour le problème de la couverture par les sommets minimale.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On démontre d'abord que l'ensemble $C$ calculé est une couverture par les sommets de $G$.

                    Pour cela, on démontre l'invariant : \simplecit{toutes les arêtes supprimées sont couvertes par les sommets de $C$}.

                    En fin d'exécution, toutes les arêtes ont été supprimées, donc sont couvertes.

                    \vspace{12pt}
                    
                    On note $M$ l'ensemble des arêtes extraites sur la ligne 3 de l'algorithme \ref{alg:2}.

                    Par construction, $\abs C = 2 \abs M$.

                    De plus, $M$ est un couplage de $G$ : on démontre l'invariant : \simplecit{$M$ est un couplage, et toutes les arêtes de $A$ sont non adjacentes à celles de $M$}.

                    $-$ Initialement, $M = \varnothing$, donc l'invariant est vrai.

                    $-$ Invariance : on suppose l'invariant vrai et on note $\set{s, t}$ la prochaine arête extraite de $A$.

                    Par l'invariant, $\set{s, t}$ est non adjacente aux arêtes de $M$ et $M$ est un couplage.

                    Après suppression des arêtes incidentes à $s$ ou à $t$, il ne reste dans $A$ que des arêtes non adjacentes à $\set{s, t}$ et non adjacentes aux arêtes de $M$ d'après l'invariant.

                    L'invariant reste donc vrai.

                    \vspace{12pt}
                    
                    On note maintenant $C^*$ une couverture optimale.

                    Comme les sommets de $C^*$ doivent couvrir les arêtes de $M$, et comme $M$ est un couplage, il y a nécessairement au moins un sommet dans $C^*$ pour chaque arête de $M$.

                    Donc
                    \[
                        \abs{C^*} \ge \abs M = \dfrac{\abs C}{2}
                    \]
                    \textit{i.e}
                    \[
                        \abs C \le 2 \abs{C^*}
                    \]
                \end{proof}

                \vspace{12pt}
                
                $\bullet$ Remarque :
                si on suppose que $G$ est un arbre, alors il existe un algorithme glouton optimal pour ce problème.

                Comme il faut couvrir l'unique arête incidente à une feuille, on peut sélectionner l'autre extrémité de cette arête en espérant couvrir ainsi plusieurs arêtes.
                Supprimer les arêtes incidentes à ce sommet peut créer plusieurs arbres, donc il faudra tenir compte du fait que le graphe peut devenir une forêt.

                D'où l'algorithme suivant :
                \begin{indalgo}{}
                    $F \gets$ file de priorité min dont les éléments sont les sommets, dont la priorité est leur degré\;
                    $C \gets \varnothing$\;

                    \While{$F \neq \varnothing$}{
                        Extraire l'élément $s$ de priorité min de $F$\;
                        \If{$s$ admet un voisin $t$}{
                            $C \gets C \cup \set{A}$\;
                            $A \gets A \setminus \set{\set{s, t}}$\;

                            \For{chaque voisin $u$ de $t$}{
                                $A \gets A \setminus \set{\set{t, u}}$\;
                                Décrémenter la priorité de $u$ dans $F$\;
                            }

                            Mettre la priorité de $t$ dans $F$ à 0\;
                        }

                        \Return $C$\;
                    }
                \end{indalgo}

                Complexité :
                \[
                    \mathcal O\!\lr{
                        \underbrace{\abs S + \abs A}_{\text{création de}\ F}
                        + \sum_{s \in S} \underbrace{\log \abs S}_{\substack{\text{extraction} \\ \text{du min}}}
                        + \sum_{t \in S} \underbrace{(d(t) + 1)\log \abs S}_{\substack{\text{mises à jour} \\ \text{de priorité}}}
                    }
                    =
                    \mathcal O\!\lr{(\abs S + \abs A)\log \abs S}
                \]
                
                Ce qui est polynomial en $\abs G$.

                \vspace{12pt}
                
                $\bullet$ Proposition :
                \begin{emphBox}
                    Chaque sommet extrait est de degré 0 ou 1 au moment de l'extraction.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On a l'invariant \simplecit{les priorité des sommets sont leurs degrés}.

                    Il suffit alors de démontrer l'invariant \simplecit{le sous-graphe $G_F$ induit par $F$ est une forêt} car une sommet de degré minimal dans une forêt est soi isolé, soit une feuille d'un arbre.

                    $-$ Initialement, $G_F = G$ qui est un arbre, donc une forêt.

                    $-$ Invariance : si $G_F$ est une forêt, et si on extrait $s$ de $F$,
                    alors $d(s) \in \set{0, 1}$.

                    Si $d(s) = 0$, on a juste retiré un sommet isolé, donc $G_F$ reste une forêt.

                    Si $d(s) = 1$ et $t$ est l'unique voisin de $s$, on décompose l'arbre de $G_F$ contenant $s$ et $t$ en un sommet isolé $(A)$ et autant de sous-arbres que $t$ admet de voisins différents de $s$.

                    Donc $G_F$ reste une forêt.
                \end{proof}

                \vspace{12pt}
                
                $\bullet$ Proposition
                \begin{emphBox}
                    L'algorithme glouton calcule une couverture par les sommets optimale de $G$.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    Tout d'abord, $C$ est bien une couverture car chaque arête est supprimée parce qu'elle est couverte par un élément ajouté à $C$.
                    Toutes les arêtes sont bien supprimées car on retire teous les sommets de la file et pour chaque sommet extrait, il est de degré 0 ou 1 lors de son extraction, \textit{i.e} toutes els arêtes incidentes à ce sommet ont été supprimées sauf peut-être une qui sera supprimé au moment de l'exécution.

                    \vspace{12pt}
                    
                    On démontre l'optimalité grâce à l'invariant \simplecit{il existe une couverture optimale $C^*$ telle que $C \subseteq C^*$}.

                    $-$ Initialement, $C = \varnothing$, et il existe une couverture optimale donc l'invariant est vrai.

                    $-$ Invariance : on suppose qu'il existe une couverture optimale $C^*\ |\ C \subseteq C^*$, et on note $s$ le sommet extrait.

                    Si $d(s) = 0$, $C$ ne change pas et $C^*$ convient encore.

                    \begin{indt}{Sinon, $d(s) = 1$, et on note $t$ l'unique voisin de $s$.}
                        $+$ si $t \in C^*$, $C^*$ convient encore.

                        $+$ sinon, $s \in C^*$ car $\set{s, t}$ doit être couverte.
                    \end{indt}

                    On montre alors que $\lr{C^* \setminus \set s} \cup \set t$ est une couverture optimale contenant $C \cup \set t$ :

                    $s \notin C$, car sinon, au moment de son extraction, $d(s) = 0$, et on a supposé $d(s) = 1$.

                    Donc $C \subseteq C^* \setminus \set s$, et $C \cup \set t \subseteq \lr{C^* \setminus s€t s} \cup \set t$.

                    Le cardinal ne change pas, donc il reste à démontrer que $\lr{C^* \setminus \set s} \cup \set t$ est une couverture.

                    C'est vrai car la seule arête incidente à $s$ qui n'est pas couverte par $C$, donc par $C^* \setminus \set s$ est l'arête $\set{s, t}$, qui est couverte par $\set t$. Les autres arêtes sont bien couvertes par $C^* \setminus \set s$.

                    Finalement, en sortie d'algorithme, $C$ est une couverture incluse dnas une couverture optimale, donc c'est une couverture optimale.
                \end{proof}
            \end{indt}
        \end{indt}
    \end{indt}
    
\end{document}
%--------------------------------------------End
