\documentclass[a4paper, 12pt, twoside]{article}


%------------------------------------------------------------------------
%
% Author                :   Lasercata
% Last modification     :   2023.01.24
%
%------------------------------------------------------------------------


%------ini
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
%\usepackage[english]{babel}


%------geometry
\usepackage[textheight=700pt, textwidth=500pt]{geometry}


%------color
\usepackage{xcolor}
\definecolor{ff4500}{HTML}{ff4500}
\definecolor{00f}{HTML}{0000ff}
\definecolor{0ff}{HTML}{00ffff}
\definecolor{656565}{HTML}{656565}

%\renewcommand{\emph}{\textcolor{ff4500}}
%\renewcommand{\em}{\color{ff4500}}

\newcommand{\Emph}{\textcolor{ff4500}}

\newcommand{\strong}[1]{\textcolor{ff4500}{\bf #1}}
\newcommand{\st}{\color{ff4500}\bf}


%------Code highlighting
%---listings
\usepackage{listings}

\definecolor{cbg}{HTML}{272822}
\definecolor{cfg}{HTML}{ececec}
\definecolor{ccomment}{HTML}{686c58}
\definecolor{ckw}{HTML}{f92672}
\definecolor{cstring}{HTML}{e6db72}
\definecolor{cstringlight}{HTML}{98980f}
\definecolor{lightwhite}{HTML}{fafafa}

\lstdefinestyle{DarkCodeStyle}{
    backgroundcolor=\color{cbg},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstring},
    basicstyle=\ttfamily\footnotesize\color{cfg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    xleftmargin=\leftskip
}

\lstdefinestyle{LightCodeStyle}{
    backgroundcolor=\color{lightwhite},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstringlight},
    basicstyle=\ttfamily\footnotesize\color{cbg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=L,
    xleftmargin=\leftskip
}

%\lstset{style=DarkCodeStyle}
\lstset{style=LightCodeStyle}
%Usage : \begin{lstlisting}[language=Caml, xleftmargin=xpt] ... \end{lstlisting}


%---Algorithm
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\SetKwProg{Fn}{Function}{:}{}
\SetKw{KwPrint}{Print}

\newcommand\commfont[1]{\textit{\texttt{\textcolor{656565}{#1}}}}
\SetCommentSty{commfont}
\SetProgSty{texttt}
\SetArgSty{textnormal}
\SetFuncArgSty{textnormal}
%\SetProgArgSty{texttt}

\newenvironment{indalgo}[2][H]{
    \begin{algoBox}
        \begin{algorithm}[#1]
            \caption{#2}
}
{
        \end{algorithm}
    \end{algoBox}
}


%---tcolorbox
\usepackage[many]{tcolorbox}
\DeclareTColorBox{emphBox}{O{black}O{lightwhite}}{
    breakable,
    outer arc=0pt,
    arc=0pt,
    top=0pt,
    toprule=-.5pt,
    right=0pt,
    rightrule=-.5pt,
    bottom=0pt,
    bottomrule=-.5pt,
    colframe=#1,
    colback=#2,
    enlarge left by=10pt,
    width=\linewidth-\leftskip-10pt,
}

\DeclareTColorBox{algoBox}{O{black}O{lightwhite}}{
    breakable,
    arc=0pt,
    top=0pt,
    toprule=-.5pt,
    right=0pt,
    rightrule=-.5pt,
    bottom=0pt,
    bottomrule=-.5pt,
    left=0pt,
    leftrule=-.5pt,
    colframe=#1,
    colback=#2,
    width=\linewidth-\leftskip-10pt,
}


%-------make the table of content clickable
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}


%------pictures
\usepackage{graphicx}
%\usepackage{wrapfig}

\usepackage{tikz}
%\usetikzlibrary{babel}             %Uncomment this to use circuitikz
%\usetikzlibrary{shapes.geometric}  % To draw triangles in trees
%\usepackage{circuitikz}            %Electrical circuits drawing


%------tabular
%\usepackage{color}
%\usepackage{colortbl}
%\usepackage{multirow}


%------Physics
%---Packages
%\usepackage[version=4]{mhchem} %$\ce{NO4^2-}$

%---Commands
\newcommand{\link}[2]{\mathrm{#1} \! - \! \mathrm{#2}}
\newcommand{\pt}[1]{\cdot 10^{#1}} % Power of ten
\newcommand{\dt}[2][t]{\dfrac{\mathrm d #2}{\mathrm d #1}} % Derivative


%------math
%---Packages
%\usepackage{textcomp}
%\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools} % For abs
\usepackage{stmaryrd} %for \llbracket and \rrbracket
\usepackage{mathrsfs} %for \mathscr{x} (different from \mathcal{x})

%---Commands
%-Sets
\newcommand{\N}{\mathbb{N}} %set N
\newcommand{\Z}{\mathbb{Z}} %set Z
\newcommand{\Q}{\mathbb{Q}} %set Q
\newcommand{\R}{\mathbb{R}} %set R
\newcommand{\C}{\mathbb{C}} %set C
\newcommand{\U}{\mathbb{U}} %set U
\newcommand{\seg}[2]{\left[ #1\ ;\ #2 \right]}
\newcommand{\nset}[2]{\left\llbracket #1\ ;\ #2 \right\rrbracket}

%-Exponantial / complexs
\newcommand{\e}{\mathrm{e}}
\newcommand{\cj}[1]{\overline{#1}} %overline for the conjugate.

%-Vectors
\newcommand{\vect}{\overrightarrow}
\newcommand{\veco}[3]{\displaystyle \vect{#1}\binom{#2}{#3}} %vector + coord

%-Limits
\newcommand{\lm}[2][{}]{\lim\limits_{\substack{#2 \\ #1}}} %$\lm{x \to a} f$ or $\lm[x < a]{x \to a} f$
\newcommand{\Lm}[3][{}]{\lm[#1]{#2} \left( #3 \right)} %$\Lm{x \to a}{f}$ or $\Lm[x < a]{x \to a}{f}$
\newcommand{\tendsto}[1]{\xrightarrow[#1]{}}

%-Integral
\newcommand{\dint}[4][x]{\displaystyle \int_{#2}^{#3} #4 \mathrm{d} #1} %$\dint{a}{b}{f(x)}$ or $\dint[t]{a}{b}{f(t)}$

%-left right
\newcommand{\lr}[1]{\left( #1 \right)}
\newcommand{\lrb}[1]{\left[ #1 \right]}
\newcommand{\lrbb}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\lrangle}[1]{\left\langle #1 \right\rangle}

%-Others
\newcommand{\para}{\ /\!/\ } %//
\newcommand{\ssi}{\ \Leftrightarrow \ }
\newcommand{\eqsys}[2]{\begin{cases} #1 \\ #2 \end{cases}}

\newcommand{\med}[2]{\mathrm{med} \left[ #1\ ;\ #2 \right]}  %$\med{A}{B} -> med[A ; B]$
\newcommand{\Circ}[2]{\mathscr{C}_{#1, #2}}

\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}

\newcommand{\oboxed}[1]{\textcolor{ff4500}{\boxed{\textcolor{black}{#1}}}} %orange boxed

\newcommand{\rboxed}[1]{\begin{array}{|c} \hline #1 \\ \hline \end{array}} %boxed with right opened
\newcommand{\lboxed}[1]{\begin{array}{c|} \hline #1 \\ \hline \end{array}} %boxed with left opened

\newcommand{\orboxed}[1]{\textcolor{ff4500}{\rboxed{\textcolor{black}{#1}}}} %orange right boxed
\newcommand{\olboxed}[1]{\textcolor{ff4500}{\lboxed{\textcolor{black}{#1}}}} %orange left boxed


%------commands
%---to quote
\newcommand{\simplecit}[1]{\guillemotleft$\;$#1$\;$\guillemotright}
\newcommand{\cit}[1]{\simplecit{\textcolor{656565}{#1}}}
\newcommand{\quo}[1]{\cit{\it #1}}

%---to indent
\newcommand{\ind}[1][20pt]{\advance\leftskip + #1}
\newcommand{\deind}[1][20pt]{\advance\leftskip - #1}

%---to indent a text
\newcommand{\indented}[2][20pt]{\par \ind[#1] #2 \par \deind[#1]}
\newenvironment{indt}[2][20pt]{#2 \par \ind[#1]}{\par \deind} %Titled indented env

%---title
\newcommand{\thetitle}[2]{\begin{center}\textbf{{\LARGE \underline{\Emph{#1} :}} {\Large #2}}\end{center}}

%---Maths environments
%-Proofs
\newenvironment{proof}[1][{}]{\begin{indt}{$\square$ #1}}{$\blacksquare$ \end{indt}}

%-Maths parts (proposition, definition, ...)
\newenvironment{mathpart}[1]{\begin{indt}{\boxed{\text{\textbf{#1}}}}}{\end{indt}}
\newenvironment{mathbox}[1]{\boxed{\text{\textbf{#1}}}\begin{emphBox}}{\end{emphBox}}
\newenvironment{mathul}[1]{\begin{indt}{\underline{\textbf{#1}}}}{\end{indt}}

\newenvironment{theo}{\begin{mathpart}{Théorème}}{\end{mathpart}}
\newenvironment{Theo}{\begin{mathbox}{Théorème}}{\end{mathbox}}

\newenvironment{prop}{\begin{mathpart}{Proposition}}{\end{mathpart}}
\newenvironment{Prop}{\begin{mathbox}{Proposition}}{\end{mathbox}}
\newenvironment{props}{\begin{mathpart}{Propriétés}}{\end{mathpart}}

\newenvironment{defi}{\begin{mathpart}{Définition}}{\end{mathpart}}
\newenvironment{meth}{\begin{mathpart}{Méthode}}{\end{mathpart}}

\newenvironment{Rq}{\begin{mathul}{Remarque :}}{\end{mathul}}
\newenvironment{Rqs}{\begin{mathul}{Remarques :}}{\end{mathul}}

\newenvironment{Ex}{\begin{mathul}{Exemple :}}{\end{mathul}}
\newenvironment{Exs}{\begin{mathul}{Exemples :}}{\end{mathul}}


%------Sections
% To change section numbering :
% \renewcommand\thesection{\Roman{section}}
% \renewcommand\thesubsection{\arabic{subsection})}
% \renewcommand\thesubsubsection{\textit \alph{subsubsection})}

% To start numbering from 0
% \setcounter{section}{-1}


%------page style
\usepackage{fancyhdr}
\usepackage{lastpage}

\setlength{\headheight}{18pt}
\setlength{\footskip}{50pt}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE, RO]{\textit{\textcolor{black}{\today}}}
\fancyhead[RE, LO]{\large{\textsl{\Emph{\texttt{\jobname}}}}}

\fancyfoot[RO, LE]{\textit{\texttt{\textcolor{black}{Page \thepage /}\pageref{LastPage}}}}
\fancyfoot[LO, RE]{\includegraphics[scale=0.12]{/home/lasercata/Pictures/1.images_profil/logo/mieux/lasercata_logo_fly_fond_blanc.png}}


%------init lengths
\setlength{\parindent}{0pt} %To avoid using \noindent everywhere.
\setlength{\parskip}{3pt}

\usepackage{bbm}
\newcommand{\1}{\mathbbm 1}


%---------------------------------Begin Document
\begin{document}
    
    \thetitle{Chapitre 17}{Complément d'algorithmique}
    
    \tableofcontents
    \listofalgorithms
    \newpage
    
    \begin{indt}{\section{Optimisation}}
        \begin{indt}{\subsection{Optimisation exacte}}
            \begin{indt}{\subsubsection{Introduction}}
                On s'intéresse ici à la résolution de problèmes d'optimisation au sens de la définition du chapitre 16, en 1.2.2 : on cherche un algorithme calculant une solution optimale \textbf{pour toute instance}.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple : le problème du sac à dos}}
                \label{1.1.2}

                Le problème est le suivant : on dispose d'objets de poids respectifs $w_0, \ldots, w_{n - 1}$ et de valeurs respectives $p_0, \ldots, p_{n - 1}$ et d'un sac à dos capable de supporter un poids $W$. On souhaite sélectionner des objets de sorte à maximiser la valeur totale sans dépasser la capacité du sac à dos.

                Dans la version en variables réelles, on suppose que l'on peut prendre des fractions des objets. Le problème d'optimisation se formule ainsi :

                Maximiser $\displaystyle \sum_{i = 0}^{n - 1} x_i p_i$ sous les contraintes :
                \[
                    \begin{cases}
                        \displaystyle
                        \sum_{i = 0}^{n - 1} x_i w_i \le W
                        \\
                        \forall i \in \nset 0 {n - 1},\ x_i \in \seg 0 1
                    \end{cases}
                \]

                Ce problème est résolu par un algorithme glouton :

                \begin{indalgo}{Solution du problème du sac à dos, version en variables réelles}
                    \label{alg:1}
                    Trier les objets par $\dfrac{p_i}{w_i}$ décroissant\;

                    \While{que possible en considérant les objets dans cet ordre}{
                        Fixer $x_i$ à 1\;
                    }

                    Lorsque cela n'est plus possible, prendre la fraction de l'objet courant permettant de remplir le sac\;
                \end{indalgo}

                Cet algorithme calcule bien une solution optimale : si on note $i$ l'objet de $\dfrac{p_i}{w_i}$ maximal non encore sélectionné et si une solution optimale coïncidant avec l'algorithme sur les objets avant $i$, et ne sélectionne pas cet objet dans son intégralité, alors $\exists j$ tel que la solution optimale sélectionne une fraction de l'objet $j$ qui est $> x_j$.

                Dans ce cas, il existe $\delta_j > 0$ tel que l'on peut ajouter une quantité $\dfrac{\delta_j}{w_i}$ de l'objet $i$ et retirer une quantité $\dfrac{\delta_j}{w_j}$ de l'objet $j$ à la solution optimale.

                La variation de poids est $\dfrac{\delta_j}{w_i}w_i - \dfrac{\delta_j}{w_j}w_j = 0$, donc on a toujours une solution.

                La variation de valeur est
                \[
                    \dfrac{\delta_j}{w_i} p_i - \dfrac{\delta_j}{w_j} p_j
                    = \underbrace{\delta_j}_{> 0}\underbrace{\lr{\dfrac{p_i}{w_i} - \dfrac{p_j}{w_j}}}_{\ge 0}
                \]

                Donc la solution reste optimale.

                On peut donc modifier la solution optimale jusqu'à l'obtention d'une solution optimale ayant choisi l'objet $i$ dans son intégralité.

                L'invariant \simplecit{il existe une solution optimale ayant fait les mêmes choix que l'algorithme glouton} est vrai.

                \vspace{12pt}
                
                Remarque : le problème du sac à dos, dans sa version entière (les $x_i \in \set{0, 1}$) ne peut pas être résolu par l'algorithme glouton (algorithme n°\ref{alg:1}) auquel on retire la dernière étape ne prenant qu'une fraction du dernier objet.

                \begin{center}
                    \begin{tabular}{l|ccc}
                        Poids & 5 & 5 & 7
                        \\
                        \hline
                        Valeur & 5 & 5 & 8
                    \end{tabular}
                    $\qquad W = 10$
                \end{center}

                \textit{Cf} l'exemple ci-dessus : l'algorithme glouton donne une solution de valeur $8$ en prenant l'objet de poids 7 alors qu'une solution optimale est de valeur 10 : on prend les deux objets de poids 5.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Séparation et évaluation (\textit{branch and bound})}}
                $\bullet$ Une technique de résolution des problèmes d'optimisation consiste à effectuer une exploration exhaustive de l'ensemble des solutions et à conserver la meilleure solution.

                Cependant, on se heurte à des problèmes de complexité (exemple : pour le sac à dos en variables entières, il y a $2^n$ solutions potentielles à tester).

                On peut parfois accélérer la recherche grâce à l'heuristique du retour sur trace (\textit{cf} chapitre 8, 4.3).

                Par exemple, pour le problème du sac à dos, il y a surement de nombreuses combinaisons d'objets qui dépassent la capacité du sac à dos. On peut donc sélectionner les objets un à un et lorsque l'on s'aperçoit que la capacité du sac à dos est dépassée, on revient sur le dernier choix.

                En pratique, cela revient à construire un arbre binaire dans lequel tous les n\oe uds de même profondeur correspondent à un même objet et pour ces n\oe uds, le fils gauche correspond au cas où l'on a sélectionné l'objet et le fils droit au cas où l'objet n'est pas sélectionné.

                On élague les branches correspondant à des sélections dépassant la capacité du sac à dos.

                Si $w_1 + w_2 > W$,

                \begin{center}
                    \begin{tikzpicture}
                        \node (0) at (0, 0) {$\bullet$}
                            child [xshift=-90pt] {node {$\bullet$}
                                child [xshift=-30pt] {node {$\bullet$}
                                    child {node {} edge from parent [dashed]}
                                    edge from parent [above left] node {$x_2 = 1$}
                                }
                                child [xshift=30pt] {node {$\bullet$}
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above left] node {$x_3 = 1$}
                                    }
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above right] node {$x_3 = 0$}
                                    }
                                    edge from parent [above right] node {$x_2 = 0$}
                                }
                                edge from parent [above left] node {$x_1 = 1$}
                            }
                            child [xshift=90pt] {node {$\bullet$}
                                child [xshift=-30pt] {node {$\bullet$}
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above left] node {$x_3 = 1$}
                                    }
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above right] node {$x_3 = 0$}
                                    }
                                    edge from parent [above left] node {$x_2 = 1$}
                                }
                                child [xshift=30pt] {node {$\bullet$}
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above left] node {$x_3 = 1$}
                                    }
                                    child {node {$\bullet$}
                                        child {node {} edge from parent [dashed]}
                                        edge from parent [above right] node {$x_3 = 0$}
                                    }
                                    edge from parent [above right] node {$x_2 = 0$}
                                }
                                edge from parent [above right] node {$x_1 = 0$}
                            }
                        ;

                        \node (1) at (-6, -6) {
                            \begin{tabular}{c}
                                Sous-arbre non parcouru
                                \\
                                car dépasse la capacité
                            \end{tabular}
                        };
                        \draw[->] (1) to (-5.7, -4.5);
                    \end{tikzpicture}
                \end{center}

                Dans le cadre de la résolution d'un problème d'optimisation, on peut parfois élaguer encore plus l'arbre de recherche en considérant le coût des solutions construites : si on sait évaluer une borne du meilleur possible pour une série de choix sans parcourir l'intégralité du sous-arbre correspondant, on peut parfois élaguer ce sous-arbre si on connaît déjà une solution de coût meilleur que cette borne.

                \begin{indt}{Cette méthode consiste à concevoir un algorithme par séparation et évaluation :}
                    $-$ La séparation consiste à diviser le problème en sous-problèmes, donc à créer un branchement dans l'arbre de reverche.

                    Exemple : pour le problème du sac à dos, on a deux sous-problèmes, selon que l'objet $i$ est sélectionné ou non.

                    \vspace{6pt}
                    
                    $-$ L'évaluation consiste à déterminer une borne sur le coût d'une solution optimale \textbf{réalisable avec les choix déjà faits} et à le comparer avec une borne connue pour savoir s'il est nécessaire de poursuivre l'exploration du sous-arbre.
                \end{indt}

                \vspace{12pt}
                
                \begin{indt}{Pour que cette méthode soit efficace, on a besoin de bonnes heuristiques pour :}
                    $-$ La séparation : si les choix initiaux convergent rapidement vers une \simplecit{bonne solution}, on élaguera plus de branches dans la suite de l'exploration.

                    \vspace{6pt}
                    
                    $-$ L'évaluation : on doit pouvoir calculer \emph{efficacement} une borne \emph{la plus juste possible} pour avoir de bonnes chances d'élaguer des branches.
                \end{indt}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple : le problème du sac à dos}}
                $\bullet$ Pour la séparation, on sélectionne ou pas un objet, avec l'heuristique suivante : on considère les objets par $\dfrac{p_i}{w_i}$ décroissant.

                \vspace{6pt}
                
                $\bullet$ Pour l'évaluation, on utilise l'heuristique de relaxation : on relâche certaines contraintes, ce qui élargit le domaine des solutions donc permet potentiellement d'atteindre un meilleur coût.
                Si le problème relâché est plus simple à résoudre, le coût d'une solution optimale est donc la borne recherchée.

                Ici, on effectue une relaxation continue : on n'impose plus aux $x_i$ d'être des entiers, ce qui nous ramène au problème vu en \ref{1.1.2} (page \pageref{1.1.2}), que l'on sait résoudre efficacement (en $\mathcal O(n)$ car les objets seront triés une seule fois au début de l'algorithme pour l'heuristique de séparation).

                Remarque : si l'algorithme nous donne une solution entière : on a trouvé la solution optimale (selon les choix qui sont déjà fait).

                Si l'algorithme nous donne une solution avec un terme dans $]0, 1[$, la partie entière de la valeur de cette solution est une borne supérieure sur la solution optimale du problème entier.

                \vspace{12pt}
                
                Exemple : on considère l'instance suivante :

                \begin{center}
                    \begin{tabular}{r|cccc}
                        $i$ & 1 & 2 & 3 & 4
                        \\
                        \hline
                        $p_i$ & 1 & 1 & 5 & 3
                        \\
                        \hline
                        $w_i$ & 3 & 2 & 4 & 2
                    \end{tabular}
                    $\qquad W = 5$
                \end{center}

                Le tri des objets indique qu'on doit les traiter dans l'ordre suivant : 4, 3, 2, 1.

                On note au cours de l'exécution, $W$ la capacité courante du sac à dos, et $V$ la valeur totale courante des objets sélectionnés.

                \begin{center}
                    \begin{tikzpicture}[scale=1.6]
                        \node (0) at (0, 0) [rectangle, draw] {$W = 5, V = 0, \sup = \floor{3 + \dfrac 3 4 5} = 6$}
                            child [xshift=-60pt] {node [rectangle, draw] {$W = 3, V = 3, \sup = 6$}
                                child [xshift=-20pt] {node [rectangle, draw] {$W = -1$}
                                edge from parent [above left] node {$x_3 = 1$}}
                                child [xshift=20pt] {node [rectangle, draw] {$
                                        \begin{array}{c}
                                            W = 3, V = 3
                                            \\
                                            \sup = 3 + \floor{1 + \frac 1 3} = 4
                                        \end{array}
                                $}
                                    child [xshift=-20pt] {node [rectangle, draw] {$
                                            \begin{array}{c}
                                                W = 1, V = 4
                                                \\
                                                \sup = 4 + \floor{\frac 1 3} = 4 = V
                                            \end{array}
                                    $} edge from parent [left] node {$x_2 = 1$}}
                                    child [xshift=20pt] {node [rectangle, draw] {$
                                            \begin{array}{c}
                                                W = 3, V = 3
                                                \\
                                                \sup = 4 = \mathrm{inf}
                                            \end{array}
                                    $} edge from parent [right] node {$x_2 = 0$}}
                                    edge from parent [above right] node {$x_3 = 0$}
                                }
                                edge from parent [above left] node {$x_4 = 1$}
                            }
                            child [xshift=60pt] {node [rectangle, draw] {$
                                    \begin{array}{c}
                                        W = 5, V = 0
                                        \\
                                        \sup = \floor{5 + \frac 1 2} = 5
                                    \end{array}
                            $}
                                child [xshift=-20pt] {node [rectangle, draw] {$
                                        \begin{array}{c}
                                            W = 1, V = 5
                                            \\
                                            \sup = 5 = V > \mathrm{inf}
                                        \end{array}
                                $} edge from parent [left] node {$x_3 = 1$}}
                                edge from parent [above right] node {$x_4 = 0$}
                            }
                        ;
                        
                        \node (1) at (-4.4, -3.5) {\huge{$\times$}};
                        \node (1) at (.3, -5.2) {\huge{$\times$}};
                    \end{tikzpicture}
                \end{center}

                Solution $(0, 1, 0, 1)$, et $\inf = 4$

                Solution $(0, 0, 1, 0)$, et $\inf = 5$

                \vspace{12pt}
                
                Conclusion : sélectionner uniquement l'objet $3$ est une solution optimale.

                \vspace{6pt}
                
                \boxed{\rm Exo} écrire l'exécution de l'algorithme si l'heuristique de séparation consiste à prendre les objets par ordre d'indice ou par ordre de poids décroissant ou de valeur décroissante.

                Tester aussi l'heuristique d'évaluation qui consiste à prendre la borne des valeurs des objets comme borne supérieure.
            \end{indt}
        \end{indt}

        \vspace{12pt}
        
        \begin{indt}{\subsection{Optimisation et \textbf{NP}-complétude}}
            \begin{indt}{\subsubsection{Introduction}}
                On considère un problème d'optimisation caractérisé par une relation $\mathcal R \subseteq A \times B$ et une fonction de coût $c : B \longrightarrow \R^+$.
                On rappelle que le problème de décision associé à ce problème d'optimisation s'énonce ainsi : étant donné une instance $a \in A$ et un entier $k \in \N$, existe-t-il une solution $b \in B$ telle que
                \[
                    \begin{cases}
                        a \mathcal R b
                        \\
                        c(b) \le k
                    \end{cases}
                \]

                Remarque : on peut aussi considérer des problèmes de maximisation et la condition à satisfaire est alors $c(k) \ge k$.

                \vspace{6pt}
                
                Fait : s'il existe un algorithme de complexité polynomiale qui résout le problème d'optimisation, alors le problème de décision associé appartient à la classe $\mathbf{P}$.
                En effet, il suffit pour une instance $a$ d'exécuter l'algorithme qui résout le problème d'optimisation sur $a$ (complexité polynomiale en la taille de $a$) puis de comparer le coût de la solution optimale obtenue et $k$ (complexité $\mathcal O(\log k)$).
                Cela donne un algorithme polynomial qui résout le problème de décision.

                \vspace{12pt}
                
                Conséquence : si le problème de décision associé à un problème d'optimisation est \textbf{NP}-complet, alors on a peu d'espoir de trouver un algorithme polynomial qui résout le problème d'optimisation.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Retour au problème du sac à dos}}
                On sait que le problème en variables réelles peut être résolu en temps $\mathcal O(n\log n)$ par un algorithme glouton et que cet algorithme ne fonctionne pas pour le problème en variables entières.

                \vspace{12pt}
                
                $\bullet$ Résolution par programmation dynamique : on considère une instance
                \[
                    (w_1, \ldots, w_n, p_1, \ldots, p_n, W)
                \]
                du problème du sac à dos en variables entières.

                $\forall i \in \nset 0 n,\ \forall w \in \nset 0 W$, on note $V(i, w)$ la valeur maximale que l'on peut atteindre en sélectionnant des objets parmi ceux d'indices 1 à $i$ dans un sac à dos de capacité $w$.

                On cherche à obtenir $V(n, W)$.

                $
                    \forall i \in \nset 0 {n - 1},\
                    \forall w \in \nset 0 W,\
                $
                \[
                    V(i + 1, w) =
                    \begin{cases}
                        \vspace{-32pt}
                        \\
                        V(i, w)
                        & \text{si}\ \overbrace{w_{i + 1} > w}^{\substack{\text{on ne peut pas} \\ \text{sélectionner l'objet $i + 1$}}}
                        \vspace{6pt}
                        \\
                        \max(\!\!\!\!\!\! \underbrace{V(i, w)}_{\substack{\text{on ne sélectionne} \\ \text{pas l'objet $i + 1$}}}\!\!\!\!\!\!\!,\ \, \underbrace{p_{i + 1} + V(i, w - w_{i + 1})}_{\substack{\text{on sélectionne} \\ \text{l'objet $i + 1$}}})
                        & \text{sinon}
                        \vspace{-21pt}
                    \end{cases}
                \]

                \vspace{12pt}

                $\forall w \in \nset 0 W,\ V(0, W) = 0$

                On peut donc remplir la matrice $V$ ligne par ligne (par $i$ croissant) et retrouver une solution réalisant $V(n, W)$ en temps $\mathcal O(W_n)$.

                Conclusion ? Aucune car c'est un algorithme de complexité exponentielle en la taille de l'instance ($W$ est de taille $\mathcal O(\log w)$).

                \vspace{12pt}
                
                $\bullet$ Proposition

                \begin{emphBox}
                    On considère le problème de décision suivant :

                    \textsc{Sac\_a\_dos} : étant donné $n$ poids $w_1, \ldots, w_n \in \N$, $n$ valeurs $x_1, \ldots, x_n \in \set{0, 1}$ telles que
                    \[
                        \begin{cases}
                            \displaystyle
                            \sum_{i = 1}^n x_i p_i \ge k
                            \\
                            \displaystyle
                            \sum_{i = 1}^n x_i w_i \le W
                        \end{cases}
                        \qquad
                        \text{?}
                    \]

                    \vspace{6pt}
                    
                    \textsc{Sac\_a\_dos} est \textbf{NP}-complet.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    $-$ \textsc{Sac\_a\_dos} $\in \mathbf{NP}$ : $x_1, \ldots, x_n$ est un certificat vérifiable en temps polynomial.

                    \vspace{6pt}
                    
                    $-$ \textsc{Sac\_a\_dos} est \textbf{NP}-difficile : on procède par réduction de $2$-\textsc{Partition} (\textit{cf} TD$_{46}$) : étant donné $S = \set{a_1, \ldots, a_n} \subseteq \N$, existe-t-il $I \subseteq \nset 1 n$ tel que
                    \[
                        \sum_{i \in I} a_i = \sum_{i \in \nset 1 n \setminus I} a_i
                        \qquad \text{?}
                    \]

                    Soit $S = \set{a_1, \ldots, a_n}$ une instance de 2-\textsc{Partition}.

                    On construit l'instance suivante de \textsc{Sac\_a\_dos} :
                    \[
                        \begin{cases}
                            \forall i \in \nset 1 n,\ w_i = p_i = a_i
                            \\
                            \displaystyle
                            W = k = \dfrac 1 2 \sum_{i = 1}^n a_i
                        \end{cases}
                    \]

                    \begin{indt}{Cette instance est calculable en temps polynomial en la taille de $S$ et cela constitue une réduction :}
                        $+$ Si $\exists I \subseteq \nset 1 n\ |\ \displaystyle \sum_{i \in I} a_i = \sum_{i \in \nset 1 n \setminus I} a_i$, alors
                        \[
                            \sum_{i = 1}^n a_i
                            = \sum_{i \in I} a_i + \sum_{i \in \nset 1 n \setminus I} a_i
                            = 2\sum_{i \in I} a_i
                        \]

                        donc en notant $\forall i \in \nset 1 n,\ x_i = \1_I(i)$, on obtient
                        \[
                            \sum_{i = 1}^n x_i p_i
                            = \sum_{i = 1}^n x_i w_i
                            = \sum_{i = 1}^n x_i a_i
                            = \sum_{i = 1}^n \1_I(i) a_i
                            = \sum_{i \in I} a_i
                            = \dfrac 1 2 \sum_{i = 1}^n a_i
                            = k
                            = W
                        \]

                        Donc les $x_i$ sont une solution de l'instance de \textsc{Sac\_a\_dos} associée.

                        \vspace{6pt}
                        
                        $+$ Réciproquement, si
                        \[
                            \exists (x_1, \ldots, x_n) \in \set{0, 1}^n\ |\
                            \begin{cases}
                                \displaystyle
                                \sum_{i = 1}^n x_i w_i \le W
                                \\
                                \displaystyle
                                \sum_{i = 1}^n x_i p_i \ge k
                            \end{cases}
                        \]

                        Alors
                        \[
                            \dfrac 1 2 \sum_{i = 1}^n a_i
                            = k
                            \le \sum_{i = 1}^n x_i p_i
                            = \sum_{i = 1}^n x_i a_i
                            = \sum_{i = 1}^n x_i w_i
                            \le W
                            = \dfrac 1 2 \sum_{i = 1}^n a_i
                        \]

                        Donc
                        \[
                            \begin{array}{rcl}
                                \displaystyle
                                \sum_{i = 1}^n x_i a_i
                                &=& \displaystyle
                                \dfrac 1 2 \sum_{i = 1}^n a_i
                                \vspace{3pt}
                                \\
                                &=& \displaystyle
                                \dfrac 1 2 \lr{\sum_{\substack{i = 1 \\ x_i = 1}}^n a_i + \sum_{\substack{i = 1 \\ x_i = 0}}^n a_i}
                                \vspace{3pt}
                                \\
                                &=& \displaystyle
                                \dfrac 1 2 \lr{\sum_{\substack{i = 1 \\ x_i = 1}}^n x_i a_i + \sum_{\substack{i = 1 \\ x_i = 0}}^n a_i}
                                \vspace{3pt}
                                \\
                                &=& \displaystyle
                                \dfrac 1 2 \lr{\sum_{\substack{i = 1}}^n x_i a_i + \sum_{\substack{i = 1 \\ x_i = 0}}^n a_i}
                            \end{array}
                        \]

                        Donc $\displaystyle \sum_{\substack{i = 1 \\ x_i = 1}}^n a_i = \sum_{i = 1}^n x_i a_i = \sum_{\substack{i = 0 \\ x_i = 0}}^n a_i$
                    \end{indt}

                    Donc $I = \set{i \in \nset 1 n\ |\ x_i = 1}$ est une solution à l'instance de 2-\textsc{Partition}
                \end{proof}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarque}}
                \label{1.2.3}

                Comme on a peu d'espoir de trouver efficacement une solution optimale à un problème d'optimisation dont le problème de décision associé est $\mathbf{NP}$-complet, on va plutôt chercher efficacement une solution \simplecit{pas trop mauvaise}.
                On cherche donc à concevoir un algorithme de complexité polynomiale qui fournit des solutions pour lesquelles on peut estimer la \simplecit{distance} à l'optimum.

                \vspace{12pt}
                
                $\bullet$ Exemple : l'algorithme glouton vu en \ref{1.1.2} (page \pageref{1.1.2}) pour le problème du sac à dos est très mauvais vis à vis du problème en variables entières : si $k \in \N^*$ et $W \in \N \setminus \set{0, 1}$, on considère deux objets tels que
                \[
                    \begin{cases}
                        p_1 = 1
                        \\
                        w_1 = \dfrac{W - 1}{k}
                    \end{cases}
                    \quad
                    \text{et}
                    \quad
                    \begin{cases}
                        p_2 = k
                        \\
                        w_2 = W
                    \end{cases}
                \]

                L'algorithme glouton sélectionne l'objet $1$ car $\dfrac{1}{\dfrac{W - 1}{k}}  = \dfrac{k}{W - 1} > \dfrac k W$, ce qui donne une solution de valeur 1 alors que la solution optimale consiste à prendre l'objet 2 pour une valeur $k$.

                Ainsi $\forall k \in \N^*$, il existe une instance telle que la solution optimale est $k$ fois meilleure que celle calculée par l'algorithme glouton.

                \vspace{12pt}
                
                $\bullet$ On peut faire mieux en modifiant légèrement l'algorithme : on garde la meilleure solution entre celle de l'algorithme glouton et celle qui consiste à ne prendre que l'objet de valeur maximale.

                \vspace{6pt}
                
                Proposition :
                \begin{emphBox}
                    On note pour toute instance $e$, $V^*(e)$ la valeur d'une solution optimale, et $V(e)$ la valeur de la solution calculée par l'algorithme glouton modifié.

                    Alors
                    \[
                        \forall e,\ V^*(e) \le 2V(e)
                    \]
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On note $e = (w_1, \ldots, w_n, p_1, \ldots, p_n, W)$.

                    Quitte à renuméroter, on peut supposer que les objets sont tirés par $\dfrac{p_i}{w_i}$ décroissant.

                    On sait que toute solution entière donne une valeur inférieure à celle d'une solution optimale réelle.
                    De plus, une telle solution est calculée \textit{via} l'algorithme glouton.

                    Alors, en notant $j \in \nset 1 n$ l'indice du premier objet que l'on ne peut pas placer intégralement dans le sac à dos, on a
                    \[
                        \begin{array}{rcl}
                            V^*(e)
                            &\le& \displaystyle
                            \sum_{i = 1}^{j - 1} p_i
                            + \underbrace{\dfrac{\displaystyle W - \sum_{i = 1}^{j - 1} w_i}{w_j}}_{< 1} p_j
                            \\
                            &\le& \displaystyle
                            \sum_{i = 1}^{j - 1} p_i + p_i
                            \vspace{3pt}
                            \\
                            &\le& \displaystyle
                            \sum_{i = 1}^{j - 1} p_i + \max_{i \in \nset 1 n} p_i
                            \vspace{3pt}
                            \\
                            &\le& \displaystyle
                            2 \max\!\lr{\sum_{i = 1}^{j - 1} a_i,\ \max_{i \in \nset 1 n} p_i}
                        \end{array}
                    \]
                    (L'algorithme glouton prend tous les objets de 1 à $j - 1$ puis la fraction de l'objet $j$ qui permet de remplir le sac à dos)
                \end{proof}

                \vspace{12pt}
                
                Remarque : on a donc un algorithme de même complexité que l'algorithme glouton et tel que la solution calculée est toujours de valeur supérieure à la moitié de la valeur optimale.

                \vspace{12pt}
                
                \boxed{\rm H.P} $\forall \varepsilon \in \R^*_+$, il existe un algorithme de complexité $\mathcal O\!\lr{\dfrac{1 + \varepsilon}{\varepsilon} n^3}$ déterminant une solution entière au problème du sac à dos et tel que $\forall e$ instance, la valeur $V(e)$ de la solution calculée vérifie $V^*(e) \le (1 + \varepsilon)V(e)$.

                Idée : par programmation dynamique : $\forall i \in \nset 0 n,\ \forall p \in \nset{0}{\displaystyle \sum_{i = 1}^n p_i}$, on calcule le poids minimal $W(i, p)$ réalisable en sélectionnant des objets parmi ceux d'indices de 1 à $i$ de sorte que la valeur obtenue vaille $p$ (et le poids $\le W$).

                On le fait avec des objets de valeur modifiée en fonction de $n$, $\varepsilon$, $p_{\max} = \max_{i \in \nset 1 n} p_i$

                $\displaystyle \forall i \in \nset 1 n$, on note $\floor{\dfrac{p_i}{2^t}}$, où
                \[
                    t = \floor{\log\!\lr{\dfrac{\varepsilon}{1 + \varepsilon} \dfrac{p_{\max}}{n}}}
                \]
            \end{indt}
        \end{indt}

        \vspace{12pt}
        
        \begin{indt}{\subsection{Algorithmes d'approximation}}
            \begin{indt}{\subsubsection{Définition (\textit{algorithme d'approximation})}}
                On considère une problème d'optimisation caractérisé par une relation $\mathcal R \subseteq A \times B$ et une fonction de coût $c : B \longrightarrow \R^+$.

                Soit $\alpha \in \R_+^*$, et $M$ un algorithme tel que $\forall a \in A$, $M$ appliqué à $a$ termine et revoie une solution associée à l'instance $a$, \textit{i.e} $a \mathcal R M(a)$.

                On dit que $M$ est un \emph{algorithme d'approximation} de facteur $\alpha$, ou \emph{$\alpha$-approximation}, pour le problème d'optimisation si, en notant $\forall a \in A$, $c^*(a)$ le coût d'une solution optimale associée à $a$ :
                \begin{indt}{}
                    $-$ si on a un problème de minimisation, $\forall a \in A,\ c(M(a)) < \alpha c^*(a)$ (et $\alpha > 1$)
                    
                    $-$ si on a un problème de maximisation, $\forall a \in A,\ c(M(a)) \ge \alpha c^*(a)$ (et $\alpha < 1$).
                \end{indt}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                L'algorithme glouton pour le problème du sac à dos n'est pas un algorithme d'approximation car on a vu en \ref{1.2.3} (page \pageref{1.2.3}) que l'on peut construire des instances telles que le rapport entre la valeur de la solution calculée est valeur optimale est arbitrairement petit.

                En revanche, on a vu que l'algorithme modifié constitue une $\dfrac 1 2$--approximation.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarques}}
                $\bullet$ On parle parfois d'$\alpha(n)$--approximation, où le facteur $\alpha$ est variable et dépend de la taille de l'instance.

                $\bullet$ On utilise souvent des algorithmes gloutons pour construire des algorithmes d'approximation.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                On considère le problème d'optimisation suivant, associé à \textsc{Vertex\_Cover} : étant donné un graphe $G = (S, A)$, déterminer une couverture par les sommets de $G$ de taille minimale.

                On a peu d'espoir de résoudre en temps polynomial ce problème d'optimisation car \textsc{Vertex\_Cover} est \textbf{NP}--complet (\textit{cf} chapitre 16, 2.3.5) : si c'était possible, étant donné une instance $(G, k)$ de \textsc{Vertex\_Cover}, on pourrait déterminer une couverture optimale de $G$ et comparer sa taille à $k$ (si sa taille est inférieure à $k$, on peut toujours ajouter des sommets pour obtenir une couverture de taille $k$).

                \vspace{12pt}
                
                $\bullet$ Algorithme d'approximation :

                Idée : on pourrait sélectionner les extrémités d'un ensemble d'arêtes qui rencontrent toutes les autres arêtes du graphe.
                Pour minimiser le nombre de sommets sélectionnés, on cherchera des arêtes sans extrémité commune, donc sur un couplage maximal.
                On ne cherchera pas un couplage maximum car on veut minimiser le nombre de sommets, donc d'arêtes.

                D'où l'algorithme glouton :
                \begin{indalgo}{Approximation pour \textsc{Vertex\_Cover}}
                    \label{alg:2}

                    $C \gets \varnothing$\;
                    \While{$A \neq \varnothing$}{
                        Extraire une arête $\set{s, t}$ de $A$\;
                        $C \gets \set{s, t} \cup C$\;
                        Supprimer les arêtes adjacentes à $s$ ou à $t$\;
                    }

                    \Return $C$\;
                \end{indalgo}

                \vspace{12pt}
                
                $\bullet$ Proposition :
                \begin{emphBox}
                    Cet algorithme est une 2--approximation pour le problème de la couverture par les sommets minimale.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On démontre d'abord que l'ensemble $C$ calculé est une couverture par les sommets de $G$.

                    Pour cela, on démontre l'invariant : \simplecit{toutes les arêtes supprimées sont couvertes par les sommets de $C$}.

                    En fin d'exécution, toutes les arêtes ont été supprimées, donc sont couvertes.

                    \vspace{12pt}
                    
                    On note $M$ l'ensemble des arêtes extraites sur la ligne 3 de l'algorithme \ref{alg:2}.

                    Par construction, $\abs C = 2 \abs M$.

                    De plus, $M$ est un couplage de $G$ : on démontre l'invariant : \simplecit{$M$ est un couplage, et toutes les arêtes de $A$ sont non adjacentes à celles de $M$}.

                    $-$ Initialement, $M = \varnothing$, donc l'invariant est vrai.

                    $-$ Invariance : on suppose l'invariant vrai et on note $\set{s, t}$ la prochaine arête extraite de $A$.

                    Par l'invariant, $\set{s, t}$ est non adjacente aux arêtes de $M$ et $M$ est un couplage.

                    Après suppression des arêtes incidentes à $s$ ou à $t$, il ne reste dans $A$ que des arêtes non adjacentes à $\set{s, t}$ et non adjacentes aux arêtes de $M$ d'après l'invariant.

                    L'invariant reste donc vrai.

                    \vspace{12pt}
                    
                    On note maintenant $C^*$ une couverture optimale.

                    Comme les sommets de $C^*$ doivent couvrir les arêtes de $M$, et comme $M$ est un couplage, il y a nécessairement au moins un sommet dans $C^*$ pour chaque arête de $M$.

                    Donc
                    \[
                        \abs{C^*} \ge \abs M = \dfrac{\abs C}{2}
                    \]
                    \textit{i.e}
                    \[
                        \abs C \le 2 \abs{C^*}
                    \]
                \end{proof}

                \vspace{12pt}
                
                $\bullet$ Remarque :
                si on suppose que $G$ est un arbre, alors il existe un algorithme glouton optimal pour ce problème.

                Comme il faut couvrir l'unique arête incidente à une feuille, on peut sélectionner l'autre extrémité de cette arête en espérant couvrir ainsi plusieurs arêtes.
                Supprimer les arêtes incidentes à ce sommet peut créer plusieurs arbres, donc il faudra tenir compte du fait que le graphe peut devenir une forêt.

                D'où l'algorithme suivant :
                \begin{indalgo}{Algorithme optimal pour \textsc{Vertex\_Cover} sur les arbres}
                    $F \gets$ file de priorité min dont les éléments sont les sommets, dont la priorité est leur degré\;
                    $C \gets \varnothing$\;

                    \While{$F \neq \varnothing$}{
                        Extraire l'élément $s$ de priorité min de $F$\;
                        \If{$s$ admet un voisin $t$}{
                            $C \gets C \cup \set{A}$\;
                            $A \gets A \setminus \set{\set{s, t}}$\;

                            \For{chaque voisin $u$ de $t$}{
                                $A \gets A \setminus \set{\set{t, u}}$\;
                                Décrémenter la priorité de $u$ dans $F$\;
                            }

                            Mettre la priorité de $t$ dans $F$ à 0\;
                        }

                        \Return $C$\;
                    }
                \end{indalgo}

                Complexité :
                \[
                    \mathcal O\!\lr{
                        \underbrace{\abs S + \abs A}_{\text{création de}\ F}
                        + \sum_{s \in S} \underbrace{\log \abs S}_{\substack{\text{extraction} \\ \text{du min}}}
                        + \sum_{t \in S} \underbrace{(d(t) + 1)\log \abs S}_{\substack{\text{mises à jour} \\ \text{de priorité}}}
                    }
                    =
                    \mathcal O\!\lr{(\abs S + \abs A)\log \abs S}
                \]
                
                Ce qui est polynomial en $\abs G$.

                \vspace{12pt}
                
                $\bullet$ Proposition :
                \begin{emphBox}
                    Chaque sommet extrait est de degré 0 ou 1 au moment de l'extraction.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On a l'invariant \simplecit{les priorité des sommets sont leurs degrés}.

                    Il suffit alors de démontrer l'invariant \simplecit{le sous-graphe $G_F$ induit par $F$ est une forêt} car une sommet de degré minimal dans une forêt est soi isolé, soit une feuille d'un arbre.

                    $-$ Initialement, $G_F = G$ qui est un arbre, donc une forêt.

                    $-$ Invariance : si $G_F$ est une forêt, et si on extrait $s$ de $F$,
                    alors $d(s) \in \set{0, 1}$.

                    Si $d(s) = 0$, on a juste retiré un sommet isolé, donc $G_F$ reste une forêt.

                    Si $d(s) = 1$ et $t$ est l'unique voisin de $s$, on décompose l'arbre de $G_F$ contenant $s$ et $t$ en un sommet isolé $(A)$ et autant de sous-arbres que $t$ admet de voisins différents de $s$.

                    Donc $G_F$ reste une forêt.
                \end{proof}

                \vspace{12pt}
                
                $\bullet$ Proposition
                \begin{emphBox}
                    L'algorithme glouton calcule une couverture par les sommets optimale de $G$.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    Tout d'abord, $C$ est bien une couverture car chaque arête est supprimée parce qu'elle est couverte par un élément ajouté à $C$.
                    Toutes les arêtes sont bien supprimées car on retire teous les sommets de la file et pour chaque sommet extrait, il est de degré 0 ou 1 lors de son extraction, \textit{i.e} toutes els arêtes incidentes à ce sommet ont été supprimées sauf peut-être une qui sera supprimé au moment de l'exécution.

                    \vspace{12pt}
                    
                    On démontre l'optimalité grâce à l'invariant \simplecit{il existe une couverture optimale $C^*$ telle que $C \subseteq C^*$}.

                    $-$ Initialement, $C = \varnothing$, et il existe une couverture optimale donc l'invariant est vrai.

                    $-$ Invariance : on suppose qu'il existe une couverture optimale $C^*\ |\ C \subseteq C^*$, et on note $s$ le sommet extrait.

                    Si $d(s) = 0$, $C$ ne change pas et $C^*$ convient encore.

                    \begin{indt}{Sinon, $d(s) = 1$, et on note $t$ l'unique voisin de $s$.}
                        $+$ si $t \in C^*$, $C^*$ convient encore.

                        $+$ sinon, $s \in C^*$ car $\set{s, t}$ doit être couverte.
                    \end{indt}

                    On montre alors que $\lr{C^* \setminus \set s} \cup \set t$ est une couverture optimale contenant $C \cup \set t$ :

                    $s \notin C$, car sinon, au moment de son extraction, $d(s) = 0$, et on a supposé $d(s) = 1$.

                    Donc $C \subseteq C^* \setminus \set s$, et $C \cup \set t \subseteq \lr{C^* \setminus \set s} \cup \set t$.

                    Le cardinal ne change pas, donc il reste à démontrer que $\lr{C^* \setminus \set s} \cup \set t$ est une couverture.

                    C'est vrai car la seule arête incidente à $s$ qui n'est pas couverte par $C$, donc par $C^* \setminus \set s$ est l'arête $\set{s, t}$, qui est couverte par $\set t$. Les autres arêtes sont bien couvertes par $C^* \setminus \set s$.

                    Finalement, en sortie d'algorithme, $C$ est une couverture incluse dnas une couverture optimale, donc c'est une couverture optimale.
                \end{proof}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                On transforme le problème de décision \textsc{Sat} en le problème d'optimisation \textsc{Max\_Sat} : étant donné une formule $\varphi$ sous FNC, déterminer une valuation qui maximise le nombre de clauses de $\varphi$ satisfaites.

                \vspace{12pt}
                
                $\bullet$ Remarque : le problème de décision associé à \textsc{Max\_Sat} est \textbf{NP}--complet (réduction de \textsc{Sat} avec le nombre de clauses de la formule comme seuil).

                On peut de même considérer les problèmes \textsc{Max-$k$-Sat}, pour $k \ge 1$, où les formules considérées sont telles que toutes leurs clauses sont de taille $\le k$.

                \vspace{12pt}
                
                $\bullet$ Proposition
                \begin{emphBox}
                    \textsc{Max-$k$-Sat} est \textbf{NP}-complet $\forall k \ge 2$.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    Il suffit le de démontrer pour $k = 2$ car la fonction identité est une réduction polynomiale de \textsc{Max-$k$-Sat} $\forall k \ge 2$, en remarquant que tous ces problèmes appartiennent à la classe \textbf{NP} (certificat = valuation, on compte le nombre de clauses satisfaites).

                    On montre que $3$-\textsc{Sat} $\le_{\rm p}$ \textsc{Max-$2$-Sat}

                    Soit $\varphi$ une formule sous forme normale conjonctive dont les clauses sont de taille au plus trois.
                    On peut supposer que toutes les clauses de $\varphi$ sont de taille exactement trois, quitte à répéter une littéral dans les clauses trop petites.

                    On écrit donc
                    \[
                        \varphi = \bigwedge_{i = 1}^k (a_i \vee b_i \vee c_i)
                    \]

                    avec $a_i, b_i, c_i$ des littéraux.

                    On constuit alors l'instance suivante de \textsc{Max-2-Sat} :
                    \[
                        \begin{array}{rcl}
                            \text{Formule}
                            & :
                            & \displaystyle
                            \bigwedge_{i = 1}^k \psi_i
                            \\
                            \text{Seuil}
                            & :
                            & 7k
                        \end{array}
                    \]

                    où :
                    \[
                        \psi_i =
                        (a_i \wedge b_i \wedge c_i \wedge d_i \wedge (\neg a_i \vee \neg b_i) \wedge (\neg a_i \vee \neg c_i) \wedge (\neg b_i \vee \neg c_i) \wedge (a_i \vee \neg d_i) \wedge (b_i \vee \neg d_i) \wedge (c_i \vee \neg d_i))
                    \]

                    Si $v$ est une modèle de $\varphi$, alors $\forall i \in \nset 1 k$, il y a entre 1 et 3 littéraux de la clause $a_i \vee b_i \vee c_i$ qui sont satisfaits par $v$.
                    Dans tous les cas, on peut choisir la valeur de $d_i$ de sorte que 7 des 10 clauses construites à partir de cette clause soient satisfaites.

                    Au total, on aura bien au moins $7k$ clauses satisfaites.

                    \vspace{6pt}
                    
                    Réciproquement, si une valuation $v$ satisfait au moins $7k$ clauses de la formule construite :
                    $\forall i \in \nset 1 k$, si aucun des littéraux $a_i, b_i, c_i$ n'est satisfait par $v$, alors on ne peut satisfaire qu'au plus six clauses parmi des dix associés à ces littéraux.

                    Si un, deux, ou trois littéraux parmi $a_i, b_i, c_i$ sont satisfait par $v$, alors comme avant par disjonction de cas, on montre qu'au plus sept clauses parmi les dix associés à ces littéraux peuvent être satisfaites.
                    Donc pour chaque clause de $\varphi$ il y a au moins un littéral satisfait par $v$.
                \end{proof}

                \vspace{12pt}
                
                $\bullet$ Algorithme d'approximation pour \textsc{Max\_Sat} : on propose d'utiliser l'algorithme naïf suivant :

                \begin{indalgo}{Approximation pour \textsc{Max\_Sat}}
                    \label{alg:4}
                    \For{chaque variable $x$ de $\varphi$}{
                        Choisir la valeur de vérité de $x$ par un tirage aléatoire uniforme dans $\set{V, F}$\;
                    }
                    \Return la valuation obtenue\;
                \end{indalgo}

                On parle d'\emph{algorithme probabiliste} ou \emph{randomisé} car on effectue des tirages aléatoires.
                Le résultat de cet algorithme dépend des tirages effectués, donc pour évaluer la qualité de cet algorithme en terme d'approximation, on compare la valeur d'une solution optimale avec l'espérance de la valeur du résultat de cet algorithme.

                On note $S$ la variable aléatoire du nombre de clauses satisfaites par le résultat de l'algorithme, et $\forall c$ clause de $\varphi$, on note $S_c$ la variable aléatoire qui vaut 1 si $c$ est satisfaite par le résultat de l'algorithme, et 0 sinon.

                On sait que $\displaystyle S = \sum_{c \in \varphi} S_c$, donc par linéarité,
                \[
                    E(S) = \sum_{c \in \varphi} E(S_c)
                    = \sum_{c \in \varphi} \mathbb P(S_c = 1)
                \]

                \begin{indt}{Soit $c$ une clause de $\varphi$ :}
                    $-$ Si $c$ contient une variable et sa négation, $\mathbb P(S_c = 1) = 1$

                    $-$ Sinon, on note $k_c$ le nombre de littéraux indépendants de $c$, \textit{i.e} portant sur des variables différentes.
                    $c$ n'est satisfaite si et seulement si tous les littéraux prennent la valeur $F$, ce qui arrive avec la probabilité $\dfrac{1}{2^{k_0}}$ car les tirages aléatoires sont indépendants.

                    Donc
                    \[
                        \mathbb P(S_c = 1) = 1 - \dfrac{1}{2^{k_0}} \ge \dfrac 1 2
                    \]

                    Donc $E(S) \ge \dfrac k 2 \ge \dfrac{V^*(\varphi)}{2}$, où $V^*(\varphi)$ est le nombre maximal de clauses que l'on peut satisfaire dans $\varphi$ (majoré par le nombre de clauses de $\varphi$).

                    Cet algorithme est donc une $\dfrac 1 2$--approximation.

                    \vspace{12pt}
                    
                    Remarque : si toutes les clauses sont de taille $\ge 2$, on obtient une $\dfrac 3 4$--approximation.
                \end{indt}
            \end{indt}
        \end{indt}
    \end{indt}

    \vspace{12pt}
    
    \vspace{12pt}
    
    \begin{indt}{\section{Algorithmes probabilistes}}
        \begin{indt}{\subsection{Généralités}}
            \begin{indt}{\subsubsection{Définition (\textit{algorithme probabiliste})}}
                Un \emph{algorithme probabiliste} est un un algorithme au sens de chap.16, 2.1.2, qui donne les opérations élémentaires possibles, en ajoutant une opération de tirage aléatoire correspondant au tirage aléatoire uniforme d'un bit.

                On dit que l'algorithme est \emph{déterministe} s'il n'utilise pas l'opération de tirage aléatoire.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarque}}
                Deux exécutions d'un algorithme probabiliste peuvent donner des résultats différents sur le même paramètre (exemple : algorithme \ref{alg:4}, page \pageref{alg:4}) ou bien effectuer un nombre différent d'opérations élémentaires (exemple : \ref{2.1.3}, page \pageref{2.1.3}).

                La complexité temporelle pour une entrée fixée est donc une variable aléatoire dont on étudie en général l'espérance.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple : le tri rapide randomisé}}
                \label{2.1.3}

                Idée : pour éviter de tomber dans le pire cas du tri rapide, on tire aléatoirement uniformément le pivot parmi les éléments à trier.

                Remarque : dans le pire cas, on tire par exemple systématiquement le plus petit élément, ce qui donne $\mathcal O(n^2)$ comparaisons, mais cela arrive avec une probabilité de $\dfrac 1 {n!}$.

                On utilise l'algorithme suivant :

                \begin{indalgo}{Tri rapide randomisé}
                    \SetKwFunction{FastSort}{Tri\_rapide}

                    \Fn{\FastSort{$l$}}{
                        \If{$\abs l \le 1$}{
                            \Return $l$\;
                        }
                        \Else{
                            $p \gets$ élément aléatoire uniforme de $l$\;
                            $l_<, l_> \gets$ partition de $l$ suivant $p$\;

                            \Return \FastSort{$l_<$} @ $[p]$ @ \FastSort{$l_>$}\;
                        }
                    }
                \end{indalgo}

                On note $l = [l_1, \ldots, l_n]$, et $C$ la variable aléatoire du nombre de comparaisons effectuées par \textsf{Tri\_rapide}.

                On remarque que lorsque deux éléments sont comparés, c'est que l'un des deux est le pivot courant, donc ils ne seront plus comparés par la suite.

                Ainsi,
                \[
                    C = \sum_{i = 1}^n \sum_{j = i + 1}^n C_{i, j}
                \]
                
                où $C_{i, j}$ est la variable aléatoire indicatrice qui vaut 1 si $l_i$ et $l_j$ sont comparés, et 0 sinon.

                Donc
                \[
                    E(C) =
                    \sum{i = 1}^n \sum_{j = i + 1}^n E(C_{i, j})
                    = \sum_{i = 1}^n \sum_{j = i + 1}^n p_{i, j}
                \]

                en notant $p_{i, j}$ la probabilité de l'événement \simplecit{$l_i$ et $l_j$ sont comparés}.

                On souhaite donc déterminer les $p_{i, j}$.

                On peut représenter une exécution de cet algorithme par un arbre binaire : la racine est le pivot tiré et les sous-arbres gauche et droit sont les arbres représentant les exécutions des appels récursifs sur $l_<$ et $l_>$.

                \vspace{12pt}
                
                Exemple : $[1, 12, 3, 7, 5]$

                Une exécution possible :

                $-$ on tire 7, $l_< = [1, 3, 5]$, $l_> = [12]$

                \begin{indt}{$-$ On trie $l_<$ :}
                    on tire $5$. $l_< = [1, 3]$, $l_> = []$.

                    \begin{indt}{On trie $l_<$ :}
                        on tire 3. $l_< = [1]$, $l_> = []$
                    \end{indt}
                \end{indt}

                L'arbre associé :
                \begin{center}
                    \begin{tikzpicture}
                        \node (7) at (0, 0) [circle, draw] {$7$}
                            child {node [circle, draw] {$5$}
                                child {node [circle, draw, xshift=-20pt] {$3$}
                                    child {node [circle, draw, xshift=-20pt] {$1$}}
                                }
                            }
                            child {node [circle, draw] {$12$}}
                        ;
                    \end{tikzpicture}
                \end{center}

                Remarque : la racine est comparée avec tous les éléments des deux sous-arbres et les éléments du sous-arbre gauche ne sont jamais comparés avec ceux du sous-arbre droit.

                \begin{indt}{Ainsi, si $1 \le i < j \le n$, il y a deux cas :}
                    $-$ $\exists k \in \nset 1 n\ |\ l_i < l_k < l_j$ et $l_k$ est choisi come pivot courant juste avant $l_i$ et $l_j$ : $l_i$ et $l_j$ se retrouvent alors dans des sous-arbres distincts et ne sont pas comparés.

                    $-$ $\forall k \in \nset 1 n\ |\ l_i < l_k < l_j$, $l_k$ est choisi comme pivot après $l_i$ ou $l_j$, donc soit $l_i$, soit $l_j$ est choisi comme pivot en premier parmi tous ces éléments donc soit $l_i$, soit $l_j$ est racine du sous-arbre associé et $l_i$ et $l_j$ sont comparés.
                \end{indt}

                En renumérotant les $l_i$ par ordre croissant, nommés $l_i'$, on sait que $l_i'$ et $l_j'$ sont comparés (avec $i < j$) si et seulement si $l_i'$ ou $l_j'$ est le premier élément choisi comme pivot parmi les $l_k',\ k \in \nset i j$.

                comme les tirages aléatoires sont uniformes, la probabilité que $l_k'$ soit tiré comme pivot en premier est la même quelle que soit la valeur de $k$.

                Donc la probabilité $p_{i, j}'$ que $l_i'$ et $l_j'$  soient comparés vaut $\dfrac{2}{j - i + 1}$

                Donc
                \[
                    \begin{array}{rcl}
                        E(C)
                        &=& \displaystyle
                        \sum_{i = 1}^n \sum_{j = i + 1}^n p_{i, j}
                        = \sum_{i = 1}^n \sum_{j = i + 1}^n p_{i, j}'
                        = \sum_{i = 1}^n \sum_{j = i + 1}^n \dfrac{2}{j - i + 1}
                        \\
                        &\le& \displaystyle
                        2 \sum_{i = 1}^n \underbrace{\sum_{k = 1}^n \dfrac 1 k}_{H_n}
                        \\
                        &\le& 2n H_n
                    \end{array}
                \]

                Or $H_n = \ln n + \gamma + o(1)$.

                Donc $E(C) = \mathcal O(n\log n)$.

                \vspace{12pt}
                
                Remarque : on peut démontrer qu'avec une forte probabilité la valeur de $C$ n'est pas très éloignée de son espérance, donc cet algorithme est très souvent optimal en terme d'ordre de grandeur du nombre de comparaisons.
            \end{indt}
        \end{indt}

        \vspace{12pt}
        
        \begin{indt}{\subsection{Las Vegas et Monte Carlo}}
            \begin{indt}{\subsubsection{Définition (\textit{algorithmes de type Las Vegas et Monte Carlo})}}
                \begin{indt}{On distingue deux familles d'algorithmes probabilistes :}
                    $-$ Les algorithmes de type \emph{Las Vegas}, qui renvoient toujours un résultat correct malgré les tirages aléatoires mais leur temps d'exécution peut dépendre du tirage aléatoire ;

                    $-$ Les algorithmes de type \emph{Monte Carlo}, dont la complexité ne dépend pas des tirages aléatoires, mais dont le résultat peut être incorrect selon le tirage.
                \end{indt}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple d'algorithmes de type Las Vegas}}
                $\bullet$ Le tri rapide randomisé vu en \ref{2.1.3} (page \pageref{2.1.3}) : le résultat est toujours trié, mais le nombre de comparaisons dépend des tirages du pivot.
                Cela peut aller de $\mathcal O(n\log n)$ dans le meilleur cas à $\mathcal O(n^2)$ dans le pire cas.
                Cependant, comme l'espérance du nombre de comparaison est en $\mathcal O(n\log n)$, on est plus souvent proche du meilleur cas que du pire cas.

                \vspace{12pt}
                
                $\bullet$ Pour le problème des $N$ reines : ce problème consiste à placer $N$ dames sur un échiquier $N \times N$ de telle sorte qu'aucune dame ne soit en prise.

                Il est possible de résoudre ce problème par un algorithme de recherche exhaustive avec un retour sur place (en plaçant les dames rangée par rangée) mais en pratique cela ne fonctionne que pour $N$ assez petit en raison du grand nombre de retours sur trace.

                Un algorithme probabiliste qui fonctionne mieux en pratique consiste à tirer aléatoirement les positions des dames rangée après rangée et à repartir de zéro en cas d'impossibilité à trouver une position valide sur une rangée (on appelle position valide sur la rangée $k$ une case de la rangée $k$ qui n'est pas en prise vis-à-vis des dames placées sur les $k$ premières rangées).

                \begin{indalgo}{Solution probabiliste au problème des $N$ reines}
                    \label{alg:6}

                    $k \gets 0$\;

                    \While{$k \neq N$}{
                        \If{il existe une position valide sur la rangée $k$}{
                            Placer une dame sur une position valide aléatoire uniforme sur la rangée $k$\;
                            $k \gets k + 1$\;
                        }
                        \Else{
                            Retirer les dames\;
                            $k \gets 0$\;
                        }
                    }
                \end{indalgo}

                Pour tirer aléatoirement uniformément une position valide en une ragée donnée, on utilise un algorithme \emph{en ligne}, \textit{i.e} un algorithme qui reçoit et traite des données sans connaître \textit{a priori} la quantité de données.
                Cela permet de d'éviter de construire une structure contenant les positions valides.
                Cet algorithme conserve une position candidate et pour position valide rencontrée remplace la position candidate par celle-ci avec une certaine probabilité.

                Il permet également de vérifier à la volée s'il existe une position valide.

                \begin{indalgo}{Tirage aléatoire uniforme de la position}
                    \label{alg:7}

                    \texttt{nbPersValides} $\gets 0$\;

                    \For{chaque position $i$}{
                        \If{$i$ est valide}{
                            \texttt{nbPersValides} $\gets$ \texttt{nbPersValides} $+ 1$\;
                            $r \gets$ entier aléatoire uniforme entre $0$ et \texttt{nbPersValides} $- 1$\;

                            \If{$r = 0$}{
                                \texttt{pos} $\gets i$\;
                            }
                        }
                    }

                    \If{\texttt{nbPersValides} $= 0$}{
                        Pas de position valide\;
                    }
                    \Else{
                        \texttt{pos} est la position sélectionnée\;
                    }
                \end{indalgo}

                \vspace{6pt}
                
                Proposition :
                \begin{emphBox}
                    L'algorithme \ref{alg:7} effectue un tirage aléatoire uniforme d'une position valide s'il en existe une.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On démontre l'invariant :
                    \[
                        \forall j \le i\ \text{valide},\
                        \mathbb P(\mathtt{pos} = j) = \dfrac{1}{\mathtt{nbPersValides}}
                    \]

                    $-$ Tant qu'aucun $i$ valide n'a été rencontré, il n'y a rien à démontrer.

                    $-$ Pour l'indice $i$ minimal tel que $i$ est valide, $r$ est tiré aléatoirement dans l'ensemble $\set 0$, donc $r = 0$ et $\mathbb P(\mathtt{pos} = i) = \dfrac{1}{\mathtt{nbPersValides}} = 1$

                    \begin{indt}{$-$ Si l'invariant est vrai au rang $i$ :}
                        $+$ Si $i + 1$ n'est pas valide, l'invariant ne change pas et reste vrai.

                        $+$ Si $i + 1$ est valide,
                        \[
                            \mathbb P(\mathtt{pos} = i + 1)
                            = \mathbb P(r = 0)
                            = \dfrac 1 {\mathtt{nbPersValides} + 1}
                        \]

                        $\forall j \le i$ valide :
                        \[
                            \begin{array}{rcl}
                                \mathbb P(\mathtt{pos} = j)
                                &=& \mathbb P(\mathtt{pos} = j\ \text{à la fin du tour}\ i) \mathbb{P}(r \neq 0)
                                \vspace{3pt}
                                \\
                                &=& \dfrac 1 {\mathtt{nbPersValides}} \cdot \dfrac{\mathtt{nbPersValides}}{\mathtt{nbPersValides} + 1}
                                \vspace{3pt}
                                \\
                                &=& \dfrac{1}{\mathtt{nbPersValides} + 1}
                            \end{array}
                        \]
                    \end{indt}
                \end{proof}
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple d'algorithme de type Monte Carlo}}
                Le test de primalité de \textsc{Fermat} repose sur le petit théorème de \textsc{Fermat} :
                \begin{emphBox}
                    Un entier $n \in \N^* \setminus \set 1$ est premier si et seulement si
                    \[
                        \forall a \in \nset 2 {n - 1},\ a^{n - 1} \equiv 1\ [n]
                    \]
                \end{emphBox}

                D'où l'algorithme suivant :
                \begin{indalgo}{Test de primalité de \textsc{Fermat}}
                    \If{$n = 2$}{
                        \Return \texttt{true}\;
                    }

                    \Else{
                        $a \gets$ entier aléatoire uniforme entre $2$ et $n - 1$\;

                        \Return $a^{n - 1} \equiv 1\ [n]$\;
                    }
                \end{indalgo}

                \vspace{6pt}
                
                Proposition :
                \begin{emphBox}
                    Si $n$ est premier, l'algorithme donne le bon résultat avec la probabilité 1.

                    Si $n$ n'est pas premier, et si ce n'est pas un nombre de \textsc{Carmichael} (un nombre de \textsc{Carmichael} est un entier $m$ tel que $\forall a \in \nset{2}{m - 1},\ a \wedge m = 1 \Rightarrow a^{m - 1} \equiv 1\ [m]$), alors la probabilité de succès est supérieure à $\dfrac 1 2$.
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    Si $n$ est premier, trivial.

                    Sinon, on note
                    \[
                        S = \set{a \in \nset 1 {n - 1}\ |\ a^{n - 1} \equiv 1\ [n]}
                    \]

                    La probabilité d'échec vaut $\dfrac{\abs S - 1}{n - 2}$.

                    On remarque que tout élément de $S$ est inversible et que $S$ forme un groupe multiplicatif donc $S$ forme un sous-groupe de l'ensemble des inversibles de $\Z/n\Z$.

                    Par le théorème de \textsc{Lagrange}, $\abs S$ divise l'ordre de ce groupe, \textit{i.e} $\abs{S} \mid \varphi(n)$, où $\varphi$ est l'indicatrice d'\textsc{Euler}.

                    Si $n$ n'est pas de \textsc{Carmichael}, $S$ est un sous-groupe de cet ensemble, donc $\abs S < \varphi(n)$, donc
                    \[
                        \abs S \le \dfrac{\varphi(n)} 2 \le \dfrac{n - 1} 2
                    \]

                    Donc
                    \[
                        \dfrac{\abs S - 1}{n - 2}
                        \le \dfrac{1}{n - 2} \cdot \dfrac{n - 3}{2}
                        \le \dfrac 1 2
                    \]

                    Remarque : si $n$ est de \textsc{Carmichael}, alors la probabilité d'échec est
                    \[
                        \dfrac{\varphi(n) - 1}{n - 2}
                    \]
                \end{proof}

                Remarque : on peut facilement augmenter la probabilité de succès en exécutant $k$ fois cet algorithme et en renvoyant la conjonction des $k$ résultats.

                Cet algorithme échoue sur un entier composé $n$ (qui n'est pas de \textsc{Carmichael}) si et seulement si les $k$ exécutions (indépendantes) échouent donc avec probabilité $\le \dfrac{1}{2^k}$.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Faux positif / faux négatifs}}
                \begin{indt}{$\bullet$ Définition : pour un algorithme de type Monte Carlo qui tente de résoudre un problème de décision, on appelle :}
                    $-$ \emph{faux positif} un résultat valant vrai alors que faux est attendu ;

                    $-$ \emph{faux négatif} un résultat valant faux alors que vrai est attendu.
                \end{indt}

                \vspace{12pt}
                
                $\bullet$ Remarque : il existe des algorithmes de type Monte Carlo dont les erreurs sont uniquement des faux positifs (comme le test de primalité de \textsc{Fermat}), uniquement des faux négatifs ou des deux types.
            \end{indt}

            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Transformation d'un algorithme de type Monte Carlo en algorithme de type Las Vegas}}
                Si l'on dispose d'un algorithme de type Monte Carlo $M$ et d'un vérificateur $V$ d'entrées une entrée pour $M$ et le résultat associé et qui renvoie vrai si et seulement si ce résultat est correct et faux sinon, alors on peut construire l'algorithme de type Las Vegas $L$ suivant :
                \begin{indalgo}{$L$, algorithme Las Vegas associé à l'algorithme Monte Carlo $M$}
                    \KwInput{$x$}

                    $y \gets M(x)$\;

                    \While{$\neg V(x, y)$}{
                        $y \gets M(x)$\;
                    }

                    \Return $y$\;
                \end{indalgo}

                \vspace{6pt}
                
                Proposition :
                \begin{emphBox}
                    Si pour toute entrée $x$ de taille $n$, $M$ est de complexité $\mathcal O(m(n))$, et $V$ de complexité $\mathcal O(v(n))$, et si de plus la probabilité d'échec de $M$ est $p < 1$, alors la complexité espérée de $L$ est
                    \[
                        \mathcal O\!\lr{\dfrac{m(n) + v(n)}{1 - p}}
                    \]
                \end{emphBox}

                \vspace{6pt}
                
                \begin{proof}
                    On note $X$ la variable aléatoire du nombre d'exécutions de $M$.

                    $\forall k \ge 1,\ \mathbb P(X = k) = p^{k - 1}(1 - p)$ ($k - 1$ échecs suivis d'un succès).

                    Donc
                    \[
                        E(X) = \sum_{k = 1}^{+\infty} k p^{k - 1} (1 - p)
                        = \dfrac{1 - p}{(1 - p)^2}
                        = \dfrac{1}{1 - p}
                    \]

                    Comme à chaque exécution de $M$ on exécute aussi $V$ pour vérifier le résultat, la complexité espérée est
                    \[
                        E(X) \cdot \mathcal O\!\lr{m(n) + v(n)}
                        = \mathcal O\!\lr{\dfrac{m(n) + v(n)}{1 - p}}
                    \]
                \end{proof}
            \end{indt}
        \end{indt}
    \end{indt}
    
\end{document}
%--------------------------------------------End
