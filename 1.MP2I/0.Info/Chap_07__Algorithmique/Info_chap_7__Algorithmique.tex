\documentclass[a4paper, 12pt, twoside]{article}


%------------------------------------------------------------------------
%
% Author                :   Lasercata
% Last modification     :   2023.04.14
%
%------------------------------------------------------------------------


%------ini
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
%\usepackage[english]{babel}


%------geometry
\usepackage[textheight=700pt, textwidth=500pt]{geometry}


%------color
\usepackage{xcolor}
\definecolor{ff4500}{HTML}{ff4500}
\definecolor{00f}{HTML}{0000ff}
\definecolor{0ff}{HTML}{00ffff}
\definecolor{656565}{HTML}{656565}

\renewcommand{\emph}{\textcolor{ff4500}}
\renewcommand{\em}{\color{ff4500}}

\newcommand{\strong}[1]{\textcolor{ff4500}{\bf #1}}
\newcommand{\st}{\color{ff4500}\bf}


%------Code highlighting
\usepackage{listings}

\definecolor{cbg}{HTML}{272822}
\definecolor{cfg}{HTML}{ececec}
\definecolor{ccomment}{HTML}{686c58}
\definecolor{ckw}{HTML}{f92672}
\definecolor{cstring}{HTML}{e6db72}
\definecolor{cstringlight}{HTML}{98980f}
\definecolor{lightwhite}{HTML}{fafafa}

\lstdefinestyle{DarkCodeStyle}{
    backgroundcolor=\color{cbg},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstring},
    basicstyle=\ttfamily\footnotesize\color{cfg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    xleftmargin=\leftskip
}

\lstdefinestyle{LightCodeStyle}{
    backgroundcolor=\color{lightwhite},
    commentstyle=\itshape\color{ccomment},
    keywordstyle=\color{ckw},
    numberstyle=\tiny\color{cbg},
    stringstyle=\color{cstringlight},
    basicstyle=\ttfamily\footnotesize\color{cbg},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=L,
    xleftmargin=\leftskip
}

%\lstset{style=DarkCodeStyle}
\lstset{style=LightCodeStyle}

%Usage : \begin{lstlisting}[language=Caml] ... \end{lstlisting}


%-------make the table of content clickable
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
%Uncomment this and comment above for dark mode
% \hypersetup{
%     colorlinks,
%     citecolor=white,
%     filecolor=white,
%     linkcolor=white,
%     urlcolor=white
% }


%------pictures
\usepackage{graphicx}
%\usepackage{wrapfig}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric}


%------tabular
\usepackage{array}
%\usepackage{color}
%\usepackage{colortbl}
%\usepackage{multirow}


%------Physics
%---Packages
%\usepackage[version=4]{mhchem} %$\ce{NO4^2-}$

%---Commands
\newcommand{\link}[2]{\mathrm{#1} \! - \! \mathrm{#2}}
\newcommand{\pt}[1]{\cdot 10^{#1}} % Power of ten
\newcommand{\dt}[2][t]{\dfrac{\mathrm d #2}{\mathcal d #1}} % Derivative


%------math
%---Packages
%\usepackage{textcomp}
%\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools} % For abs
\usepackage{stmaryrd} %for \llbracket and \rrbracket
\usepackage{mathrsfs} %for \mathscr{x} (different from \mathcal{x})

%---Commands
%-Sets
\newcommand{\N}{\mathbb{N}} %set N
\newcommand{\Z}{\mathbb{Z}} %set Z
\newcommand{\Q}{\mathbb{Q}} %set Q
\newcommand{\R}{\mathbb{R}} %set R
\newcommand{\C}{\mathbb{C}} %set C
\newcommand{\U}{\mathbb{U}} %set U
\newcommand{\seg}[2]{\left[ #1\ ;\ #2 \right]}
\newcommand{\nset}[2]{\left\llbracket #1\ ;\ #2 \right\rrbracket}

%-Exponantial / complexs
\newcommand{\e}{\mathrm{e}}
\newcommand{\cj}[1]{\overline{#1}} %overline for the conjugate.

%-Vectors
\newcommand{\vect}{\overrightarrow}
\newcommand{\veco}[3]{\displaystyle \vect{#1}\binom{#2}{#3}} %vector + coord

%-Limits
\newcommand{\lm}[2][{}]{\lim\limits_{\substack{#2 \\ #1}}} %$\lm{x \to a} f$ or $\lm[x < a]{x \to a} f$
\newcommand{\Lm}[3][{}]{\lm[#1]{#2} \left( #3 \right)} %$\Lm{x \to a}{f}$ or $\Lm[x < a]{x \to a}{f}$
\newcommand{\tendsto}[1]{\xrightarrow[#1]{}}

%-Integral
\newcommand{\dint}[4][x]{\displaystyle \int_{#2}^{#3} #4 \mathrm{d} #1} %$\dint{a}{b}{f(x)}$ or $\dint[t]{a}{b}{f(t)}$

%-left right
\newcommand{\lr}[1]{\left( #1 \right)}
\newcommand{\lrb}[1]{\left[ #1 \right]}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert} % abs{x} -> |x|
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\lrangle}[1]{\left\langle #1 \right\rangle}

%-Others
\newcommand{\para}{\ /\!/\ } %//
\newcommand{\ssi}{\ \Leftrightarrow \ }
\newcommand{\eqsys}[2]{\begin{cases} #1 \\ #2 \end{cases}}

\newcommand{\med}[2]{\mathrm{med} \left[ #1\ ;\ #2 \right]}  %$\med{A}{B} -> med[A ; B]$
\newcommand{\Circ}[2]{\mathscr{C}_{#1, #2}}

\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}


%------commands
%---to quote french text
\newcommand{\simplecit}[1]{\guillemotleft$\;$#1$\;$\guillemotright}
\newcommand{\cit}[1]{\simplecit{\textcolor{656565}{#1}}}
\newcommand{\quo}[1]{\cit{\it #1}}

%---to indent
\newcommand{\ind}[1][20pt]{\advance\leftskip + #1}
\newcommand{\deind}[1][20pt]{\advance\leftskip - #1}

%---to indent a text
\newcommand{\indented}[2][20pt]{\par \ind[#1] #2 \par \deind[#1]}
\newenvironment{indt}[2][20pt]{#2 \par \ind[#1]}{\par \deind} %Titled indented env


\usepackage[many]{tcolorbox}
\DeclareTColorBox{pseudocode}{O{black}O{lightwhite}}{
    breakable,
    outer arc=0pt,
    arc=0pt,
    top=0pt,
    toprule=-.5pt,
    right=0pt,
    rightrule=-.5pt,
    bottom=0pt,
    bottomrule=-.5pt,
    colframe=#1,
    colback=#2,
    enlarge left by=10pt,
    width=\linewidth-\leftskip-10pt,
}

%---title
\newcommand{\thetitle}[2]{\begin{center}\textbf{{\LARGE \underline{\emph{#1} :}} {\Large #2}}\end{center}}

%---parts
%-I
\newcommand{\mainpart}[2][$\!\!$]{\underline{\large \textbf{\emph{\textit{#1} #2}}}}
\newcommand{\bmainpart}[2][$\!\!$]{\underline{\large \textbf{\textit{#1} #2}}}
%-A
\newcommand{\subpart}[2][$\!\!$]{\underline{\bf \textit{#1} #2}}
%-1
\newcommand{\subsubpart}[2][$\!\!$]{\underline{\textsl{#1} #2}}
%-a
\newcommand{\subsubsubpart}[2][$\!\!$]{\underline{\it #1 #2}}


%------page style
\usepackage{fancyhdr}
\usepackage{lastpage}

\setlength{\headheight}{18pt}
\setlength{\footskip}{50pt}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE, RO]{\textit{\today}}
\fancyhead[RE, LO]{\large{\textsl{\emph{\texttt{\jobname}}}}}

\fancyfoot[RO, LE]{\textit{\texttt{\textcolor{black}{Page \thepage /}\pageref{LastPage}}}} %Change 'black' to 'white' for dark mode
\fancyfoot[LO, RE]{\includegraphics[scale=0.12]{/home/lasercata/Pictures/1.images_profil/logo/mieux/lasercata_logo_fly_fond_blanc.png}}

% For dark mode :
%/home/lasercata/Pictures/1.images_profil/logo/mieux/lasercata_logo_fly.png


%------init lengths
\setlength{\parindent}{0pt} %To avoid using \noindent everywhere.
\setlength{\parskip}{3pt}


%---------------------------------Begin Document
\begin{document}
    
    %For dark mode :
    % \pagecolor{black}
    % \color{white}
    
    \thetitle{Chapitre 7}{\'Eléments d'algorithmique}
    
    \tableofcontents
    \newpage
    
    
    \begin{indt}{\section{Introduction}}
        
        \`A la manière de la recherche d'un élément dans un ABR, on s'intéresse à la décomposition d'un problème en un ou plusieurs sous-problèmes. On distingue plusieurs méthodes selon le type de problème à résoudre, la manière dont on décompose le problème et la manière dont on reconstruit la solution au problème à partir de celles des sous-problèmes. Nous étudierons la méthode \textit{diviser pour régner}, la programmation dynamique, et les algorithmes gloutons.
        
    \end{indt}
    
    \vspace{12pt}
    
    \begin{indt}{\section{Diviser pour régner}}
        
        \begin{indt}{\subsection{Méthode et premiers exemples}}
            \begin{indt}{\subsubsection{Méthode}}
                \begin{indt}{La méthode \textit{diviser pour régner} se décompose en 3 étapes :}
                    (1) Diviser le problème en un ou plusieurs sous-problèmes \textbf{disjoints} ;
                    
                    (2) Résoudre les sous-problèmes, soit récursivement, soit directement si le sous-problème est assez simple ;
                    
                    (3) Reconstruire la solution au problème initial à partir de celles des sous-problèmes.
                \end{indt}
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarque}}
                On retrouve l'idée des principes d'induction, où l'on utilise l'hypothèse d'induction (la solution à un sous-problème) pour démontrer la propriété sur une instance plus grande (le problème initial).
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Complexité des algorithmes \textit{diviser pour régner}}}
                Cette méthode donne une relation de récurrence de la forme
                    \[ C(n) = C_{\text{décomposition}}(n) + \sum_{p\ \text{sous-problèmes}} C(\abs p) + C_{\text{reconstruction}}(n) \]
                
                (ici, $\abs p$ désigne la taille du problème $p$)
                
                L'enjeux est de trouver des manières peu coûteuses de décomposer / reconstruire et d'obtenir peu de sous-problèmes assez petits pour avoir une complexité meilleure que celle de la résolution naïve du problème.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple (Tri fusion)}}
                Tri fusion (\textit{cf} chap 1, 3.1.4)
                
                \vspace{6pt}
                
                Problème : on veut trier une liste
                
                \vspace{6pt}
                
                \begin{indt}{Algo :}
                    (1) Diviser : on décompose la liste en 2 listes de tailles égales à une unité près ;
                    
                    (2) Résoudre : on trie récursivement les 2 listes, sauf si elles sont de taille $\le 1$ (déjà triées)
                    
                    (3) Reconstruire : on fusionne les 2 listes triées en une liste.
                \end{indt}
                
                \vspace{6pt}
                
                Récurrence :
                    \[ C(n) = \mathcal O(n) + C\lr{\floor{\dfrac n 2}} + C\lr{\ceil{\dfrac n 2}} + \mathcal O(n) \]
                
                Donc $\mathcal O(n \log n)$
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple (exponentiation rapide)}}
                (\textit{cf} TD$_{04}$)
                
                Problème : calcul de $x^n$
                
                \begin{indt}{Algo :}
                    (1) Diviser : on veut calculer $x^{\floor{\frac n 2}}$ ;
                    
                    (2) Résoudre : appel récursif, ou si $\floor{\dfrac n 2} = 0$, on renvoie 1 ;
                    
                    (3) Reconstruire : si $n \equiv 0\ [2]$, on calcule $x^{\floor{\frac n 2}} \cdot x^{\floor{\frac n 2}}$, sinon on calcule $x \cdot x^{\floor{\frac n 2}} \cdot x^{\floor{\frac n 2}}$.
                \end{indt}
                
                \vspace{6pt}
                
                Récurrence :
                    \[ C(n) = \mathcal O(1) + C\lr{\floor{\dfrac n 2}} + \mathcal O(1) \]
                
                Donc $\mathcal O(\log n)$.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple (recherche dichotomique)}}
                (\textit{cf} TD$_{06}$)
                
                Problème : on veut déterminer si une valeur apparaît dans un tableau trié.
                
                \begin{indt}{Algo :}
                    (1) Diviser : on s'intéresse à la moitié gauche ou droite du tableau selon la comparaison de la valeur recherchée avec l'élément au milieu du tableau ;
                    
                    (2) Résoudre : appel récursif sauf si le sous-tableau est de taille $\le 1$ ou si l'élément au milieu est la valeur recherché ;
                    
                    (3) Reconstruire : la solution au sous-problème est la solution au problème initial.
                \end{indt}
                
                \vspace{6pt}
                
                Récurrence :
                    \[ C(n) = \mathcal O(1) + C\lr{\floor{\dfrac n 2}} \]
                
                Donc $\mathcal O(\log n)$.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple (Tri rapide)}}
                \label{2.1.7}
                
                (\textit{cf} colle 01)
                
                Problème : trier une liste.
                
                \begin{indt}{Algo :}
                    (1) Diviser : on choisit un élément ``pivot'' et on répartit la liste en 2 sous-listes (celle des éléments $<$ au pivot et celle des éléments $>$ au pivot) et on compte le nombre d'occurrences du pivot ;
                    
                    (2) Résoudre : appel récursif sauf si la liste est de taille $\le 1$ ;
                    
                    (3) Recomposition : on concatène les 2 listes triées (dans le bon ordre) en plaçant entre les 2 le bon nombre d'occurrences du pivot.
                \end{indt}
                
                \vspace{6pt}
                
                Récurrence :
                    \[ C(n) = \mathcal O(1) + C(q) + C(r) + \mathcal O(p + q) \]
                
                où
                $
                    \left|
                    \begin{array}{l}
                        p\ \text{est le  nombre d'occurrences du pivot}
                        \\
                        q\ \text{est le nombre d'éléments $<$ pivot}
                        \\
                        r\ \text{est le nombre d'éléments $>$ pivot}
                    \end{array}
                    \right.
                $
                
                Avec un choix déterministe en temps constant du pivot (ex : le premier élément), on a :
                
                $\mathcal O(n^2)$ dans le pire cas ;
                
                $\mathcal O(n\log n)$ dans le meilleur cas.
            \end{indt}
        \end{indt}
        
        \vspace{12pt}
        
        \begin{indt}{\subsection{Application : calcul de la médiane}}
            \begin{indt}{\subsubsection{Problème}}
                On veut calculer la médiane d'une liste de $n$ valeurs distinctes, c'est-à-dire l'élément de rang $\floor{\dfrac n 2}$ où le rang d'un élément est le nombre d'éléments qui lui sont strictement inférieurs.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Solution naïve}}
                On trie la liste et on renvoie l'élément au milieu
                
                Complexité : $\mathcal O(n\log n)$ (ex : avec le tri fusion)
                
                Objectif : $\mathcal O(n)$ (optimal puisqu'on doit bien lire tous les éléments)
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Généralisation et algorithme \textit{diviser pour régner}}}
                On s'intéresse au problème suivant : trouver l'élément de rang $i$, où $i$ est un paramètre.
                
                \begin{indt}{Algo :}
                    (1) Diviser : on choisit un pivot $p$ et on détermine les listes $l_<$ et $l_>$ des éléments strictement inférieurs / supérieurs à $p$ ;
                    
                    \begin{indt}{(2) Résoudre : cela dépend de la taille de $l_<$ :}
                        $-$ Si $\abs{l_<} > i$, alors on cherche l'élément de rang $i$ dans $l_<$ ;
                        
                        $-$ Si $\abs{l_<} = i$, alors $p$ est l'élément de rang $i$ ;
                        
                        $-$ Si $\abs{l_<} < i$, alors on cherche l'élément de rang $i - \abs{l_<} - 1$ dans $l_>$ ;
                    \end{indt}
                    
                    (3) Reconstruire : l'élément de rang $i$ est le résultat de la résolution du sous-problème.
                \end{indt}
                
                \vspace{6pt}
                
                Récurrence :
                    \[
                        C(n) =
                        \mathcal O(n)
                        + C_{\text{choix pivot}}(n)
                        +
                        \begin{cases}
                            C(q)
                            \\
                            \text{ou}
                            \\
                            \mathcal O(1)
                            \\
                            \text{ou}
                            \\
                            C(r)
                        \end{cases}
                        \quad \text{selon la taille de $l_<$}
                    \]
                avec les notations de \ref{2.1.7} (p.\pageref{2.1.7}).
                
                \vspace{12pt}
                
                Dans le pire cas, comme pour le tri rapide $\mathcal O(n^2)$ par exemple si on choisit le premier élément comme pivot et si le tableau est trié et $i = n - 1$.
                
                Dans le meilleur cas, le choix du pivot est déterministe. On peut se rapprocher du meilleur cas de tri rapide en choisissant un pivot proche de la médiane.
                
                On peut s'approcher de la médiane (que l'on cherche à calculer), à moindre coût à l'aide de l'algorithme de la médiane des médianes.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Algorithme de la médiane des médianes}}
                Idée : on regroupe les éléments de la liste en petits paquets (de taille constante) dont on peut calculer les médianes naïvement, et on choisit comme pivot la médiane de ces médianes, calculée récursivement.
                
                \begin{tabular}{ccccccccc}
                    \fbox{5}
                    &
                    & \fbox{5}
                    &
                    & $\cdots$
                    &
                    & \fbox{5}
                    \\
                    \hline
                    & \vline %\fbox{$\phantom 5$}
                    &
                    & \vline %\fbox{$\phantom 5$}
                    & $\cdots$
                    & \vline
                    \\
                    \hline
                    &&&& $\downarrow$
                    \\
                    &&&& $p$
                \end{tabular}
                
                Complexité :
                    \[ C(n) = \underbrace{\mathcal O(n) + C\lr{\dfrac n 5}}_{\text{calcul du pivot}} + \underbrace{\mathcal O(n)}_{\text{division}} + \underbrace{f(n)}_{\text{éventuel appel récursif}} \]
                
                On peut majorer la valeur de $f(n)$ en majorant les tailles des listes $l_<$ et $l_>$.
                
                On s'intéresse à $l_<$ (le cas de $l_>$ se traite de manière similaire).
                
                Observons les $\dfrac n 5$ médianes des paquets. Parmi celles-ci, il y en a la moitié qui sont inférieures à $p$. Dans le pire cas, tous les éléments des $\dfrac n {10}$ paquets de ces médianes sont inférieures à $p$. Dans les autres paquets, il y a au plus 2 éléments inférieurs à $p$ (car la médiane et les 2 éléments qui lui sont supérieurs sont supérieurs à $p$).
                
                Au total, il y a au plus $5 \cdot \dfrac n {10} + 2 \cdot \dfrac n {10} = \dfrac n 2 + \dfrac n 5 = 7\dfrac n {10}$ éléments dans $l_<$.
                
                Donc $C(n) \le C\lr{\dfrac n 5} + C\lr{\dfrac{7n}{10}} + \mathcal O(n)$
                
                En pratique, sous les approximations, on montre
                    \[ \exists c > 0\ |\ \forall n,\ C(n) \le cn + C\lr{\floor{\dfrac n 5}} + C\lr{7\floor{\dfrac n {10}} + 4} \]
                
                Montrons que $C(n) = \mathcal O(n)$, \textit{i.e}
                    \[ \exists c' > 0,\ \exists N \in \N\ |\ \forall n \ge N,\ C(n) \le c'n \]
                
                On procède par analyse-synthèse :
                
                \begin{indt}{$-$ Analyse :}
                    %\vspace{-24pt}
                    $
                        \begin{array}{rcll}
                            C(n) &\le& cn + C\lr{\floor{\dfrac n 5}} + C\lr{7\floor{\dfrac n {10}} + 4}
                            \\ \\
                            &\le& cn + c'\floor{\dfrac n 5} + c'\lr{7\floor{\dfrac n {10}} + 4}
                            \\ \\
                            &\le& cn + \dfrac{c'n}{5} + c'\lr{\dfrac{7n}{10} + 4}
                            \\ \\
                            &\le& cn + c'\lr{\dfrac{9n}{10} + 4}
                            \\ \\
                            &\le& c'n & \text{dès que}\ cn \le c'\lr{\dfrac n {10} - 4}
                        \end{array}
                    $
                    
                    \vspace{12pt}
                    
                    Or $cn \le c'\lr{\dfrac n {10} - 4} \ssi \dfrac{10cn}{n - 40} \le c'$ (si $n > 40$)
                    
                    et $\dfrac{n}{n - 40} \le 2$ si $n \ge 80$
                \end{indt}
                    
                \vspace{24pt}
                
                \begin{indt}{$-$ Synthèse :}
                    On choisit $N = 800$ et $c' = 20c$
                    
                    $\forall n \ge N,\ \dfrac{n}{n - 40} \le 2$ donc $\dfrac{10cn}{n - 40} \le 20c = c'$
                    
                    Donc une récurrence avec la même démonstration qu'en analyse conclut :
                        \[ C(n) \le c'n \]
                    
                    Donc l'algorithme de la médiane des médianes permet de déterminer la médiane en temps linéaire.
                \end{indt}
            \end{indt}
        \end{indt}
        
        \vspace{12pt}
        
        \begin{indt}{\subsection{Application : couverture de points par des segments de même longueur}}
            \begin{indt}{\subsubsection{Problème}}
                \'Etant donné $n$ points $x_1, \ldots, x_n$ sur la droite réelle, et $k \in \N^*$, on veut déterminer la longueur $l$ minimale telle qu'il existe une couverture de $x_1, \ldots, x_n$ par $k$ segments de longueur $l$.
                
                $k$ segments $[l_1, r_1], \ldots, [l_k, r_k]$ forment une couverture de $x_1, \ldots, x_n$ si, et seulement si :
                    \[ \forall i \in \nset 1 n,\ \exists j \in \nset 1 k\ |\ x_i \in [l_j, r_j] \]
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarque}}
                Ceci est un problème d'optimisation : on cherche à minimiser / maximiser une certaine quantité en respectant des contraintes.
                
                On peut en général associer à un problème d'optimisation un problème de décision, c'est-à-dire un problème où l'on se pose la question de l'existence d'un objet vérifiant des contraintes.
                
                Ici : étant donné $x_1, \ldots, x_n$, $k$ et $l$, existe-il une couverture de $x_1, \ldots, x_n$ par $k$ segments de longueur $l$ ?
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Résolution par dichotomie du problème d'optimisation}}
                Idée : on suppose que l'on sait résoudre le problème de décision et on procède à une recherche dichotomique du $l$ minimal pour lequel l'algorithme de résolution du problème de décision répond \textit{oui}.
                
                On sait que $L = \dfrac{\max\limits_{i, j}\abs{x_i - x_j}}{k}$ permet de construire une couverture, donc on cherche dans $[0, L]$.
                
                On procède à une recherche dichotomique dans un tableau virtuel (on ne le construit pas) qui à chaque $i \in \nset 0 L$ associe la réponse au problème de décision pour $l = i$.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Résolution du problème de décision}}
                \label{2.3.4}
                
                Idée : on s'intéresse au problème de décision dual suivant : étant donné $n$ segments $[l_1, r_1], \ldots, [l_n, r_n]$ et $k \in \N^*$, peut-on choisir $k$ points $x_1, \ldots, x_k$ tels que
                    \[ \forall i \in \nset 1 n,\ \exists j \in \nset 1 k\ |\ x_j \in [l_i, r_i] \]
                
                Cela permet de résoudre le problème initial : si $\exists ([l_i, r_i])_{i \in \nset 1 k}$ couverture de $(x_j)_{j \in \nset 1 n}$ par des segments de longueur $l$, alors on a une solution au problème dual pour les intervalles $[x_1 - l, x_1], \ldots, [x_n - l, x_n]$ ($x_i \in [l_i, r_i] \ssi l_j \in [x_i - l, x_i]$)
                
                \vspace{6pt}
                
                Résolution du problème dual : soit $([l_i, r_i])_{i \in \nset 1 n}$ $n$ segments et $k \in \N^*$.
                
                    \[ [\ \left[\ \vphantom{\dfrac a a} ]\ [\ ]\ \right]\ [\ ] \]
                
                On traite les segments par extrémités droite croissante :
                
                Algorithme :
                \begin{pseudocode}
                    $P \leftarrow \varnothing$
                    
                    Trier et renuméroter les segments par $r_i$ croissant
                    
                    $p \leftarrow r_1$
                    
                    $P \leftarrow P \cup \set p$
                    
                    \begin{indt}{Pour $i$ de 2 à $n$ :}
                        \begin{indt}{Si $l_i > p$}
                            $p \leftarrow r_i$
                            
                            $P \leftarrow P \cup \set p$
                        \end{indt}
                    \end{indt}
                    
                    Si $\abs P \le k$ succès, sinon échec.
                \end{pseudocode}
                
                Correction : tous les arguments sont rencontrés par un point de $P$.
                
                Invariant de boucle : au début du tour d'indice $i$, tous les segments jusqu'à l'indice $i - 1$ sont rencontrés et $p = \max P = r_j$ pour un certain $j < i$
                
                \vspace{12pt}
                
                $\bullet$ Initialement : $P = \set{r_1} = \set p$ donc $p = \max P = r_1$ avec $1 < 2$, et $r_1 \in [l_1, r_1]$ donc $[l_1, r_1]$ est rencontré par un élément de $P$
                
                \vspace{6pt}
                
                $\bullet$ Conservation de l'invariant : on suppose l'invariant vrai au début du tour $i$, en particulier $p = \max P = r_j$ pour $j < i$
                
                Si $l_i \le p$, comme $j < i$, $r_j \le r_i$, donc $l_i \le p \le r_i$ et il est inutile de rajouter un point pour $[l_i, r_i]$. De plus, $p = \max P = r_j$ avec $j < i + 1$
                
                Si $l_i > p$, il faut un point supplémentaire.
                
                $P \cup \set{r_i}$ rencontre bien $[l_i, r_i]$ et $p = r_i = \max P \cup \set{r_i}$ avec $i < i + 1$
                
                \vspace{12pt}
                
                Optimalité : en notant $P = \set{p_i\ |\ i \in \nset{1}{\abs f}}$, on montre par récurrence finie que $\forall i \in \nset{1}{\abs P},\ \exists P_{\rm opt}$ optimal rencontrant tous les segments et contenant $\set{p_j\ |\ j \in \nset 1 i}$
                
                \vspace{6pt}
                
                Initialisation : $i = 1$
                
                On considère $P_{\rm opt}$ une solution optimale.
                
                Si $p_1 \not \in P_{\rm opt}$, on s'intéresse à $\min P_{\rm opt}$, qui est nécessaire pour rencontrer les segments $[l_1, r_1], \ldots, [l_j, r_j]$ pour un certain $j$.
                
                $\forall k \le j$, on remarque que $l_k \le \min P_{\rm opt} \le r_1 = p_1 \le r_k$ donc $P_{\rm opt} \setminus \set{\min P_{\rm opt}} \cup \set{r_1}$ convient.
                
                \vspace{6pt}
                
                Héréditée : par hypothèse de récurrence, $\exists P_{\rm opt}$ qui rencontre $[l_1, r_1], \ldots, [l_n, r_n]$ et qui contient $\set{p_s\ |\ s \in \nset 1 i}$.
                
                On numérote les éléments de $P_{\rm opt}$ : $popt_1, \ldots, popt_{\abs{P_{\rm opt}}}$, si $popt_{i + 1} \neq p_{i + 1}$, on utilise le même raisonnement que ci-dessus pour montrer que tous les intervalles pour lesquels $popt_{i + 1}$ est nécessaire sont rencontrés par $p_{i + 1}$.
                
                Pour la même raison, on peut remplacer $popt_{i + 1}$ par $p_{i + 1}$.
                
                \vspace{12pt}
                
                Remarque : on utilise un algorithme glouton pour résoudre le problème dual (\textit{cf} partie \ref{4}, p.\pageref{4})
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Analyse de complexité}}
                $-$ Résolution du problème dual : $\mathcal O(n\log n)$ à cause du tri ;
                
                $-$ Conversion du problème de décision en son dual : $\mathcal O(n)$ pour la construction des $[x_i - l, x_i]$ ;
                
                $-$ Recherche par dichotomie : calcul du diamètre $D = \max\limits_{i, j}\abs{x_i - x_j}$ en $\mathcal O(n)$, puis $\mathcal O\lr{\log \dfrac D k}$ itérations ;
                
                $-$ Au total : $\mathcal O\lr{n\log(n) \log \dfrac D k}$
            \end{indt}
        \end{indt}
        
    \end{indt}
    
    \vspace{12pt}
    
    \begin{indt}{\section{Programmation dynamique}}
        
        \begin{indt}{\subsection{Méthode}}
            \begin{indt}{\subsubsection{Introduction}}
                Si les sous-problèmes ne sont pas disjoints, la méthode \textit{diviser pour régner} donne des algorithmes dont la complexité est mauvaise, parce que l'on est amené à résoudre plusieurs fois le même sous-problème.
                
                \vspace{6pt}
                
                Exemple : calcul des coefficients binomiaux avec le triangle de Pascal :
                    \[ \binom n p = \binom{n - 1}{p} + \binom{n - 1}{p - 1} \]
                
                L'implémentation naïve de cette formule donne une complexité
                    \[
                        \begin{array}{rcl}
                            C(n, p) &=& C(n - 1, p) + C(n - 1, p - 1)
                            \\
                            &=& \cdots
                            \\
                            &=& \displaystyle \mathcal O\lr{\binom n p}
                        \end{array}
                    \]
                exponentiel dans le pire cas $\lr{p = \dfrac n 2}$
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Approche descendante de la programmation dynamique}}
                On stocke les valeurs calculées pour retrouver en temps constant si on en a encore besoin. On parle de \textit{mémoïsation}.
                
                Exemple :
                
                \newpage
                
                \begin{lstlisting}[language=Caml, xleftmargin=80pt]
let binom_memoised (n : int) (p : int) : int =
    let t = Array.make_matrix (n + 1) (p + 1) (-1) in
    let rec aux (i : int) (j : int) : int =
        if t.(i).(j) <> -1 then t.(i).(j)
        else
            if j = 0 || i = j then begin
                t.(i).(j) <- 1;
                1
            end else begin
                t.(i).(j) <- aux (i - 1) j + aux (i - 1) (j - 1);
                t.(i).(j)
            end
    in aux n p;;\end{lstlisting}
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Approche ascendante}}
                On adopte un point de vue impératif en inversant l'ordre dans lequel les sous-problèmes sont considérés. On commence par les cas de base et on construit progressivement la solution au problème global.
                
                \vspace{12pt}
                
                Remarque : on doit faire attention aux dépendances entre les valeurs calculées car elles ne sont plus gérées par la pile d'exécution.
                
                Exemple : triangle de Pascal :
                    \[
                        \begin{array}{cc}
                            & j
                            \\
                            & \downarrow
                            \\
                            i \rightarrow
                            &
                            \begin{pmatrix} 
                                \phantom a & \phantom a & \phantom a
                                \\
                                \phantom a & \nwarrow \!\! \uparrow \!\!\!\!\!\! x & \phantom a
                                \\
                                \phantom a & \phantom a & \phantom a
                            \end{pmatrix}
                        \end{array}
                    \]
                
                $\displaystyle \binom i j = \binom {i - 1} j + \binom{i - 1}{j - 1} \rightarrow$ on remplit la matrice ligne par ligne
                
                \begin{lstlisting}[language=Caml, xleftmargin=80pt]
let binom_ascending (n : int) (p : int) : int =
    let t = Array.make_matrix (n + 1) (p + 1) 1 in
    for i = 2 to n do
        for j = 1 to min (n - 1) p do
            t.(i).(j) <- t.(i - 1).(j) + t.(i - 1).(j - 1)
        done
    done
    t.(n).(p)\end{lstlisting}
                
                Complexité spatiale / temporelle : $\mathcal O(np)$
                
                On peut faire mieux en complexité spatiale : $\mathcal O(p)$ : un seul tableau de taille $p$ suffit
                
                \begin{center}
                    \begin{tabular}{cc}
                        $i - 1$
                        & \begin{tabular}{|c|c|c|}
                            \hline
                            & $\phantom{\rm X}$ &
                            \\
                            \hline
                        \end{tabular}
                        \\
                        & $\nwarrow \!\! \uparrow$
                        \\
                        $i$
                        & \begin{tabular}{|c|c|c|}
                            \hline
                            &X&
                            \\
                            \hline
                        \end{tabular}
                        \\
                        & $\uparrow$
                        \\
                        & $j$
                    \end{tabular}
                \end{center}
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                \label{3.1.4}
                
                Nombres de Delannoy : nombre de chemins distincts que peut prendre une dame sur un échiquier pour aller du coin inférieur gauche au coin supérieur droit en utilisant que les déplacements par rangée / colonne croissante, en fonction des dimensions de l'échiquier.
                
                \begin{center}
                    \begin{tikzpicture}[scale=.5]
                        \draw[step=1cm] (0, 0) grid (4, 4);
                        \draw[fill] (.5, .5) circle (.2cm);
                    \end{tikzpicture}
                \end{center}
                
                \[ D(i, j) = \underbrace{D(i, j - 1)}_{\rightarrow} + \underbrace{D(i - 1, j)}_{\uparrow} + \underbrace{D(i - 1, j - 1)}_{\nearrow} \]
                
                Si $i \neq 0$ et $j \neq 0$
                
                $D(i, j) = 1$ si $i = 0$ ou $j = 0$
                
                par programmation dynamique, on calcule $D(n, p)$ en $\mathcal O(np)$ en temps et $\mathcal O(p)$ en espace (2 rangées suffisent).
            \end{indt}
        \end{indt}
        
        \vspace{12pt}
        
        \begin{indt}{\subsection{Application aux problèmes d'optimisation}}
            \begin{indt}{\subsubsection{Introduction}}
                Un problème d'optimisation est un problème dans lequel on veut maximiser ou minimiser une valeur sous certaines contraintes.
                L'objectif est de déterminer les paramètres permettant d'atteindre l'optimum. La méthode de la programmation dynamique s'applique à de nombreux problèmes d'optimisation.
                Pour appliquer cette méthode, le problème doit avoir une certaine propriété, appelée \textit{propriété de sous-structure optimale} : si l'on dispose d'une solution optimale au problème, alors cette solution induit une solution optimale à un ou plusieurs sous-problèmes.
                
                Par exemple, si un plus court chemin de $A$ vers $B$ passe par $C$, alors le sous-chemin de $A$ vers $C$ est bien un plus court chemin de $A$ à $C$.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple}}
                Chemin de poids maximum sur un échiquier.
                
                On reprend le problème vu en \ref{3.1.4} (p. \pageref{3.1.4}), mais on ne cherche pas à déterminer le nombre de chemins, mais plutôt on suppose que les cases de l'échiquier contiennent des poids et on veut trouver un chemin qui maximise la somme des poids des cases parcourues.
                
                On modélise le problème à l'aide d'une matrice de poids :
                    \[
                        \begin{pmatrix}
                            31 & 21 & 12 & 26 & 34
                            \\
                            37 & 21 & 34 & 26 & 10
                            \\
                            2 & 39 & 12 & 49 & 47
                        \end{pmatrix}
                    \]
                
                Une recherche exhaustive n'est pas raisonnable : le nombre de chemins à tester pour une matrice $n \times p$ est le nombre de Delannoy $D(n, p)$. Or $\displaystyle D(n, p) \ge \binom{n + p}{p}$ (en interdisant les pas diagonaux, il faut $n$ pas verticaux, et $p$ pas horizontaux pour passer d'un coin à l'autre. Un chemin est donc caractérisé par l'emplacement de ses pas horizontaux).
                
                \vspace{12pt}
                
                Vérifions que ce problème satisfait la propriété de sous-problème optimale : si un chemin de poids maximal de $(0, 0)$ à $(n - 1, p - 1)$ passe par $(i, j)$, alors le sous-chemin de $(0, 0)$ à $(i, j)$ est de poids maximal. Sinon, on complète un chemin de poids maximal de $(0, 0)$ à $(i, j)$ par le sous-chemin de $(i, j)$ à $(n - 1, p - 1)$ et on obtient un chemin valide de $(0, 0)$ à $(n - 1, p - 1)$ qui contredit l'optimalité du chemin initial.
                
                La propriété de sous-structure optimale correspond en fait à une récurrence sur le poids maximal $w_{i, j}$ d'un chemin de $(0, 0)$ à $(i ,j)$ : on note $M = (m_{i, j})_{i, j}$ la matrice définissant le problème.
                
                Alors :
                    \[
                        w_{i, j} = m_{i, j} +
                        \left\{
                        \begin{array}{ll}
                            0
                            & \text{si $i = j = 0$}
                            \\
                            w_{0, j - 1}
                            & \text{si $i = 0$ et $j \neq 0$}
                            \\
                            w_{i - 1, 0}
                            & \text{si $i \neq 0$ et $j = 0$}
                            \\
                            \max(w_{i, j - 1}, w_{i - 1, j}, w_{i - 1, j - 1})
                            & \text{si $i \neq 0$ et $j \neq 0$}
                        \end{array}
                        \right.
                    \]
                
%                     \[
%                         \begin{array}{cc}
%                             \begin{array}{cc}
%                                 n - 1
%                                 & \rightarrow
%                                 \\
%                                 \\
%                                 0 & \rightarrow
%                             \end{array}
%                             &
%                             \begin{pmatrix}
%                                 * & \phantom * & \phantom *
%                                 \\
%                                 \\
%                                 *
%                             \end{pmatrix}
%                             \\
%                             &
%                             \begin{array}{ccc}
%                                 \uparrow & & \uparrow
%                                 \\
%                                 0 & & p - 1
%                             \end{array}
%                         \end{array}
%                     \]
                On peut calculer $w_{n - 1, p - 1}$ par programmation dynamique.
                
                On adopte l'approche ascendante de la programmation dynamique : on remplit la matrice $W = (w_{i, j})_{i, j}$ soit par lignes croissantes (et colonnes croissantes), soit par colonnes croissantes (et lignes croissantes), soit par anti-diagonales, \textit{i.e} par $i + j$ croissant.
                
                \boxed{\rm Exo} : code
                
                \vspace{12pt}
                
                Complexité : $\mathcal O(np)$, en espace aussi, mais on peut optimiser l'espace requis car il suffit de connaître une ligne ou une colonne pour remplir la suivante : $\mathcal O(\min(n, p))$ en espace.
                
                \vspace{12pt}
                
                Remarque : on n'a calculé que le poids maximal, pas un chemin réalisant ce poids.
                
                \begin{indt}{Deux possibilités :}
                    (1) Lors du remplissage de la matrice, on détermine en plus de $w_{i, j}$ un prédécesseur de $(i, j)$ sur un chemin de poids maximal de $(0, 0)$ à $(i ,j)$. On détermine un tel prédécesseur en gardant un couple qui réalise le maximum dans la formule de récurrence.
                    
                    (2) On détermine la matrice $W$ puis on retrouve un chemin de poids maximal en partant de la case $(n - 1, p - 1)$, en déterminant un prédécesseur à l'aide des valeurs de $N$ puis en calculant récursivement un chemin vers ce prédécesseur.
                \end{indt}
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple : Produit de matrices}}
                On cherche à minimiser le nombre de multiplications nécessaires au calcul du produit de $n$ matrices.
                
                Si $A \in \mathcal M_{n, p}(K),\ B \in \mathcal M_{p, q} (K)$, on peut calculer $AB$ à l'aide de $npq$ multiplications grâce à la définition :
                    \[ (AB)_{i, j} = \sum_{k = 1}^p a_{i,k} b_{k, j} \]
                
                \begin{indt}{Si $A, B, C$ sont de tailles respectives $2 \times 5,\ 5 \times 8,\ 8 \times 6$, on peut calculer $ABC$ de deux façons :}
                    $-$ $(AB)C$ : 176 multiplications ;
                    
                    $-$ $A(BC)$ : 300 multiplications.
                \end{indt}
                
                On veut donc trouver un parenthésage optimal du produit
                    \[ \prod_{k = 0}^{n - 1} A_k \]
                où $\forall k \in \nset{0}{n - 1},\ A_k \in \mathcal M_{t_k, t_{k + 1}} (K)$.
                
                \vspace{12pt}
                
                Ce problème vérifie la propriété de sous-problème optimal : si
                    \[ \lr{ \prod_{k = 0}^k A_k } \lr{ \prod_{k = i + 1}^{n - 1} A_k } \]
                est un parenthésage optimal, on a bien des parenthésages optimaux des produits
                    \[ \prod_{k = 0}^i A_k \quad \text{et} \quad \prod_{k = i + 1}^{n - 1} A_k \]
                
                On observe qu'il faudra résoudre les sous-problèmes suivants : optimisation du parenthésage du produit
                    \[ P_{i, j} = \prod_{k = i}^j A_k \]
                $\forall (i, j) \in \nset 0 {n - 1}^2$. On note $m_{i, j}$ le nombre minimal de multiplications nécessaires au calcul de $P_{i, j}$.
                
                Si $\underbrace{P_{i, k}}_{\in \mathcal M_{t_i, t_{k + 1} }(K)} \times \underbrace{P_{k + 1, j}}_{\in \mathcal M_{t_{k + 1}, t_{j + 1}}(K)}$ est un parenthésage optimal, alors
                    \[ m_{i, j} = m_{i, k} + m_{k + 1, j} + t_i t_{k + 1} t_{j + 1} \]
                
                On trouve alors la relation de récurrence
                    \[
                        m_{i, j} =
                        \left\{
                        \begin{array}{ll}
                            0
                            & \text{si $i = j$}
                            \\
                            \min\limits_{k \in \nset{i}{j - 1}} \lr{ m_{i, k} + m_{k + 1, j} + t_i t_{k + 1} t_{j + 1} }
                            & \text{si $i < j$}
                        \end{array}
                        \right.
                    \]
                
                Comment remplir la matrice $M = (m_{i,j})_{i,j}$ ?
                    \[
                        \begin{pmatrix}
                            0
                            \\
                            & \ddots & \leftarrow & \bullet
                            \\
                            & & \ddots & \downarrow
                            \\
                            & & & \ddots
                            \\
                            & & & & 0
                        \end{pmatrix}
                    \]
                
                On peut remplir la matrice par diagonales, \textit{i.e} par $j - i$ croissant.
                
                \boxed{\rm Exo} code.
                
                \vspace{12pt}
                
                Complexité : $\mathcal O(n^3)$ en temps, et $\mathcal O(n^2)$ en espace.
                
                \vspace{12pt}
                
                Comment représenter un parenthésage optimal $\Pi_{i, j}$ de $P_{i, j}$ ?
                
                Si le parenthésage s'écrit $P_{i, k} P_{k + 1, j}$, il suffit de retenir $k$ et les parenthésages $\Pi_{i, k}$ et $\Pi_{k + 1, j}$
                
                \begin{center}
                    \begin{tikzpicture}
                        \node [circle, draw] {$k$}
                            child {node [rectangle, draw] {$\Pi_{i, k}$}}
                            child {node [rectangle, draw] {$\Pi_{k + 1, j}$}}
                        ;
                    \end{tikzpicture}
                \end{center}
            \end{indt}
        \end{indt}
        
    \end{indt}
    
    \vspace{12pt}
    
    \begin{indt}{\section{Algorithmes gloutons}}
        
        \label{4}
        
        \begin{indt}{\subsection{Méthode}}
            \begin{indt}{\subsubsection{Introduction}}
                La programmation dynamique est une méthode utile pour résoudre de nombreux problèmes, mais les implémentations nécessitent en général d'allouer une grande quantité de mémoire.
                
                L'approche des \textit{algorithmes gloutons} est similaire à celle de la programmation dynamique dans sa version descendante.
                La différence principale réside dans le fait qu'on ne résout pas plusieurs sous-problèmes pour garder ensuite la meilleure solution : on choisit à l'avance le sous-problème à résoudre.
                Le choix du sous-problème se fait à l'aide d'une \textit{heuristique}, \textit{i.e} d'une stratégie de décision locale permettant de faire un choix rapide.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Exemple : Problème du rendu de monnaie}}
                On doit rendre une somme $n$ à l'aide de pièces / billets dont la valeur appartient à un ensemble $S$.
                
                Objectif : minimiser le nombre de pièces / billets utilisés.
                
                \vspace{12pt}
                
                $\bullet$ Résolution par programmation dynamique : on note $f(n, S)$ le nombre minimal de pièces / billets nécessaire pour atteindre $n$ avec des pièces / billets de valeur prise dans $S$, et on note $S = \set{c_1, \ldots, c_k}$.
                
                On remarque que $\forall i \in \nset 1 k,\ f(n, S) \le \min( \underbrace{f(n, S \setminus \set{c_i})}_{\text{on n'utilise pas $c_i$}},\ 1 + f(n - c_i, S) )$
                
                On peut remplir la matrice $F = (f_{i, j})_{i, j}$ où $\forall (i, j),\ f_{i, j} = f(i, \set{c_l\ |\ l \in \nset 1 j})$
                
                Algo en $\mathcal O(nk)$ donc complexité exponentielle !
                
                Explication :
                
                $-$ paramètres du problème : $n$ et $S = \set{c_l\ |\ l \in \nset 1 k}$
                
                $-$ taille des paramètres : $\mathcal O(\log n)$ et $k$
                
                $\mathcal O(nk) = \mathcal O(2^{\log_2 n} k)$ exponentiel en $\log n$
                
                \vspace{12pt}
                
                $\bullet$ Algo glouton :
                
                Idée : on utilise autant que possible la plus grande pièce $\le n$ dans l'idée qu'il faudrait utiliser plusieurs petites pièces pour compenser l'absence d'une telle pièce.
                
                \vspace{6pt}
                
                Algorithme :
                \begin{pseudocode}
                    Trier $S$ par ordre décroissant et renuméroter en $c_1', \ldots, c_k'$
                    
                    \begin{indt}{Pour $i$ de 1 à $k$}
                        Utiliser $c_i'$ autant que possible, \textit{i.e} $\floor{\dfrac{n}{c_i'}}$ fois :
                        
                        $n \leftarrow n - c_i' \cdot \floor{\dfrac n {c_i'}}$
                    \end{indt}
                    
                    Si $n \neq 0$, échec
                \end{pseudocode}
                
                \vspace{12pt}
                
                $\bullet$ Exemple : $n = 48$
                
                $-$ Dans le système européen, $S = \set{1, 2, 5, 10, 20}$.
                Solution : $2 \cdot 20 + 5 + 2 + 1$
                
                On peut démontrer que l'algo est optimal et qu'il réussit toujours dans ce système.
                
                $-$ Dans le système britannique antérieur à 1971 : $S = \set{1, 3, 6, 12, 24, 30}$.
                Solution : $30 + 12 + 6$
                
                Optimum : $2 \cdot 24$
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Remarque}}
                Cet exemple démontre qu'un algorithme glouton ne fournit pas forcément toujours une solution optimale. Il faut donc pouvoir démontrer l'optimalité d'un algorithme glouton lorsque l'on souhaite l'utiliser. Il existe un cadre théorique donnant une condition suffisante d'optimalité, appelée \textit{théorie des matroïdes} (H.P).
                
                Nous avons vu en \ref{2.3.4} (p.\pageref{2.3.4}) une autre méthode de démonstration : on montre par récurrence qu'il existe une solution optimale qui fait les mêmes choix que l'algorithme glouton et on montre aussi que l'algorithme glouton fournit une solution, ce qui implique que toute solution optimale faisant les mêmes choix donne la même solution.
            \end{indt}
        \end{indt}
        
        \vspace{12pt}
        
        \begin{indt}{\subsection{Application : Ordonnancement de tâches unitaires avec pénalité de retard}}
            \begin{indt}{\subsubsection{Description du problème}}
                On considère un ensemble de $n$ tâches $T = \set{t_k\ |\ k \in \nset{0}{n - 1}}$ qui nécessitent chacune une unité de temps pour être réalisées. \`A chaque tâche $t \in T$ est associée une date limite $f(t) \in \nset{0}{n - 1}$ et une pénalité $p(t) \in \N$ si la date limite n'est pas respectée.
                
                On appelle ordonnancement des tâches une fonction
                    \[
                        \begin{array}{rcccc}
                            d & : & T & \longrightarrow & \nset{0}{n - 1}
                            \\
                            && t & \longmapsto & d(t)
                        \end{array}
                    \]
                qui à chaque $t \in T$ associe la date $d(t)$ à laquelle la tâche est réalisée. $d$ doit être une bijection (on n'effectue qu'une tâche à chaque instant et toutes les tâches sont traitées).
                
                \vspace{6pt}
                
                \begin{indt}{Un ordonnancement $d$ définit 2 ensembles :}
                    $-$ $T^+ = \set{t \in T\ |\ d(t) \le f(t)}$ ensemble des tâches réalisées à temps ;
                    
                    $-$ $T^- = \set{t \in T\ |\ d(t) > f(t)}$ ensemble des tâches réalisées en retard.
                \end{indt}
                
                \vspace{6pt}
                
                La pénalité associée à $d$ est $\displaystyle P(d) = \sum_{ t \in T^-} p(t)$
                
                Objectif : déterminer $d$ minimisant $P(d)$.
                
                \vspace{12pt}
                
                Exemple :
                
                \begin{center}
                    \begin{tabular}{|c!{\vline width 2pt}c|c|c|c|c|c|c|c|}
                        \hline
                        & 0 & 1 & 2 & 3 & 4 & 5 & 6
                        \\
                        \noalign{\hrule height 2pt}
                        $f$ & 0 & 1 & 2 & 3 & 3 & 3 & 5
                        \\
                        \hline
                        $p$ & 3 & 6 & 4 & 2 & 5 & 7 & 1
                        \\
                        \hline
                        $d$ & 6 & 0 & 1 & 4 & 3 & 2 & 5
                        \\
                        \hline
                    \end{tabular}
                \end{center}
                
                Donne $P(d) = p(t_0) + p(t_3) = 3 + 2 = 5$
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Algorithme glouton}}
                On remarque que les pénalités de retard ne dépendent pas de la durée du retard.
                
                Ce qui compte est donc l'ordonnancement des tâches de $T^+$. On cherche donc à construire $T^+ \subseteq T$ tel que l'on puisse trouver un ordonnancement qui traite dans les délais les éléments de $T^+$ et tel que $P(T^+) = \displaystyle \sum_{t \in T^+} p(t)$ est maximal.
                
                Heuristique : on sélectionne en priorité les tâches de pénalité maximale que l'on planifie le plus tard possible.
                
                \vspace{12pt}
                
                Algorithme :
                
                \begin{pseudocode}
                    Trier les tâches par pénalité décroissante et les renommer $t_0, \ldots, t_{n - 1}$
                    
                    $T^+ \leftarrow \varnothing$
                    
                    Marquer toutes les dates de $\nset{0}{n - 1}$ comme disponibles
                    
                    \begin{indt}{Pour $i$ de 0 à $n - 1$ :}
                        \begin{indt}{S'il existe une date $j \le f(t_i)$ disponible :}
                            Ajouter $t_i$ à $T^+$
                            
                            Marquer le plus grand $j \le f(t_i)$ disponible comme occupé par $t_i$
                        \end{indt}
                    \end{indt}
                    
                    \vspace{6pt}
                    
                    \`A la fin, placer les tâches non sélectionnées aux dates restantes.
                \end{pseudocode}
                
                \vspace{12pt}
                
                Complexité :
                
                $-$ le tri de fait en $\mathcal O(n\log n)$
                
                $-$ la recherche d'un temps $j \le f(t_i)$ disponible dépend de la structure de données utilisée pour représenter l'ensemble des dates disponibles.
                
                Avec un tableau de booléens, on parcourt les dates jusqu'à en trouver une disponible ou jusqu'à exhaustion des dates : recherche linéaire donc $\mathcal O(n^2)$ au total.
                
                Avec un ABR équilibré contenant les dates disponibles :
                
                \boxed{\rm Exo} étant donné une date $d$, on peut trouver le plus grand élément de l'ABR inférieur ou égal à $d$ et l'extraire en temps $\mathcal O(\log n)$
                
                Donc $\mathcal O(n \log n)$ au total.
            \end{indt}
            
            \vspace{12pt}
            
            \begin{indt}{\subsubsection{Preuve d'optimalité}}
                \textbf{Lemme 1 :}
                \begin{pseudocode}
                    Soit $T$ un ensemble de tâches et $t \in T$ de pénalité maximale.
                    
                    Alors $\exists T^+ \subseteq T$ dont on peut planifier les éléments dans les délais, tel que $P(T^+)$ est maximal et tel que $t \in T^+$
                \end{pseudocode}
                
                \vspace{12pt}
                
                \begin{indt}{$\square$ Démonstration :}
                    On considère $T^+$ optimal.
                    
                    Si $t \not \in T^+$, s'il reste une date $d \le f(t)$ non occupée par des tâches de $T^+$, alors $T^+ \cup \set t$ contredit l'optimalité de $T^+$.
                    
                    Donc $\exists t' \in T^+$ planifiée à une date $d \le f(t)$.
                    
                    Or $p(t') \le p(t)$ donc $T' = (T^+ \setminus \set{t'}) \cup \set t$ est aussi une solution optimale.
                    
                    En effet, on peut traiter toutes les tâches de $T$ dans les délais et $P(T') = P(T^+) - p(t') + p(t) \ge P(T^+)$
                    $\blacksquare$
                \end{indt}
                
                \vspace{12pt}
                
                \textbf{Lemme 2 :}
                \begin{pseudocode}
                    On considère $T^+ \subseteq T$ optimal contenant une tâche et de pénalité maximale associée à la date $j$.
                    
                    On construit le sous-problème sans $t$ ni la date $j$ ainsi :
                        \[ T' = T \setminus \set t \]
                        \[
                            \forall t' \in T',\ f'(t') =
                            \left\{\!\!
                            \begin{array}{ll}
                                f(t') & \text{si $f(t') < j$}
                                \\
                                f(t') - 1 & \text{sinon}
                            \end{array}
                            \right.
                        \]
                    
                    Alors $T^+ \setminus \set t$ est optimal pour $T'$
                \end{pseudocode}
                
                \vspace{12pt}
                
                \begin{indt}{$\square$ Démonstration}
                    Tout ordonnancement de $T'$ correspond à ordonnancement de $T$ et réciproquement en décalant les temps postérieurs à $j$ pour insérer la tâche $t$. Si on dispose de $d'$ pour $T'$, alors $d$, définie par
                        \[
                            d(t') =
                            \left\{\!\!
                            \begin{array}{ll}
                                d'(t') & \text{si $d'(t') < j$}
                                \\
                                d'(t') + 1 & \text{sinon}
                            \end{array}
                            \right.
                        \]
                    donne un ordonnancement de même pénalité.
                    
                    Si $\exists T^{++}$ optimal pour $T'$ avec $P(T^{++}) > P(T^+ \setminus \set t) = P(T^+) - p(t)$
                    alors $T^{++} \cup \set t$ donneront une solution optimale pour $T$ qui contredit l'optimalité de $T^+$
                    $\blacksquare$
                \end{indt}
                
                \vspace{12pt}

                \textbf{Théorème}
                \begin{pseudocode}
                    L'algorithme glouton donne une solution optimale.
                \end{pseudocode}
                
                \vspace{12pt}
                
                \begin{indt}{$\square$ Démonstration :}
                    Par récurrence sur $n$ :
                    
                    $-$ Initialisation : $n = 1$, trivial.
                    
                    $-$ Héréditée : on note $T = \set{t_k\ |\ k \in \nset 0 n}$ ordonné par pénalité décroissante
                    
                    Par le lemme 1, il existe une solution optimale $T^+$ contenant $t_0$
                    
                    Par le lemme 2, $T^+ \setminus \set{t_0}$ est optimal pour $T \setminus \set{t_0}$ avec les délais décalés
                    
                    Par hypothèse de récurrence, l'algorithme glouton est optimal pour $T \setminus \set{t_0}$ avec les délais décalés : ensemble $T'$
                    
                    On peut décaler l'ordonnancement pour insérer $t_0$ et obtenir une solution pour $T$
                    
                    $P(T' \cup \set{t_0}) = P(T') + p(t_0) = P(T^+ \setminus \set{t_0}) + p(t_0) = P(T^+)$ optimal
                    $\blacksquare$
                \end{indt}
            \end{indt}
        \end{indt}
        
    \end{indt}
    
    
    
\end{document}
%--------------------------------------------End
